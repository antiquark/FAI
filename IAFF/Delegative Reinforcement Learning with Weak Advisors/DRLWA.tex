%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{dom}

% autosize deliminaters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}

% operators that require brackets
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\Var}{\operatorname{Var}}

% operators that require parentheses
\newcommand{\Ent}{\operatorname{H}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Sym}{\operatorname{Sym}}

\newcommand{\Prj}{\operatorname{pr}}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}\left(#1 \| #2\right)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\text{tv}}\AP{#1}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\In}{\mathcal{I}}
\newcommand{\FH}{(\A \times \Ob)^*}
\newcommand{\IH}{(\A \times \Ob)^\omega}
\newcommand{\Ado}{\bar{\Ob}}
\newcommand{\Ada}{\bar{\A}}
\newcommand{\Adi}{{\bar{\In}}}
\newcommand{\Adao}{\overline{\A \times \Ob}}
\newcommand{\Adfh}{\Adao^*}
\newcommand{\Adih}{\Adao^\omega}
\DeclareMathOperator{\HD}{hdom}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\Co}{\mathcal{C}}
\newcommand{\UC}{\mathcal{U}}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\V}{\operatorname{V}}
\newcommand{\Q}{\operatorname{Q}}
\newcommand{\EU}{\operatorname{EU}}

\newcommand{\D}{\mathcal{D}}
\newcommand{\Do}{\mathfrak{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Z}{Z}
\newcommand{\J}{J}

\begin{document}

[Previously](https://agentfoundations.org/item?id=1550), we defined a setting called "Delegative Inverse Reinforcement Learning" (DIRL) in which the agent can delegate actions to an "advisor" and the reward is only visible to the advisor as well. We proved a sublinear regret bound (converted to traditional normalization in online learning, the bound is $O(n^{2/3})$) for one-shot DIRL (as opposed to standard regret bounds in RL which are only applicable in the *episodic* setting). However, this required a rather strong assumption about the advisor: in particular, the advisor had to choose the optimal action with maximal likelihood. Here, we consider "Delegative Reinforcement Learning" (DRL), i.e. a similar setting in which the reward is directly observable by the agent. We also restrict our attention to finite MDP environments (we believe these results can be generalized to a much bigger class of environments, but not to arbitrary environments). On the other hand, the assumption about the advisor is much weaker: the advisor is only required to avoid *catastrophic* actions (i.e. actions that lose value to zeroth order in inverse time discount) and assign some positive probability to a nearly optimal action. As before, we prove a one-shot regret bound (in traditional normalization, $O(n^{3/4})$). Analogously to [before](https://agentfoundations.org/item?id=1587), we allow for "corrupt" states in which both the advisor and the reward signal stop being reliable.

***

Appendix A contains the proofs...

\section{Notation}

%Given a set $X$, the notation $X^*$ means $\bigsqcup_{n \in \Nats} X^n$ and the notation $X^+$ means $X^* \setminus \Estr$, where $\Estr \in X^0$.

The notation $K: X \M Y$ means $K$ is Markov kernel from $X$ to $Y$. When $Y$ is a finite set, this is the same as a measurable function $K: X \rightarrow \Delta Y$, and we use these notations interchangeably. Given $K: X \M Y$ and $A \subseteq Y$ (corr. $y \in Y$), we will use the notation $K(A \mid x)$ (corr $K(y \mid x)$) to stand for $\Pr_{y' \sim K(x)}\left[y' \in A\right]$ (corr. $\Pr_{y' \sim K(x)}\left[y' = y\right]$). Given $Y$ is a finite set, $\mu \in \Delta Y^\omega$, $h \in Y^*$ and $y \in Y$, the notation $\mu(y \mid h)$ means $\Pr_{x \sim \mu}\left[x_{\Abs{h}} = y \mid x_{:\Abs{h}}=h\right]$.

Given $\Omega$ a measurable space, $\mu \in \Delta \Omega$, $n,m \in \Nats$, $\{A_k\}_{k \in [n]}$, $\{B_j\}_{j \in [m]}$ finite sets and $\{X_k: \Omega \rightarrow A_k\}_{k \in [n]}$, $\{Y_j: \Omega \rightarrow B_j\}_{j \in [m]}$ random variables (measurable mappings), the mutual information between the joint distribution of the $X_k$ and the joint distribution of the $Y_j$ will be denoted

$$\I{\omega \sim \mu}\left[X_0, X_1 \ldots X_{n-1}; Y_0, Y_1 \ldots Y_{m-1}\right]$$

We will parameterize our geometric time discount by $\gamma=e^{-1/t}$, thus all functions that were previously defined to depend on $t$ are now considered functions of $\gamma$.

\section{Results}

We start by explaining the relation between the formalism of general environments we used before and the formalism of finite MDPs.

\#Definition 1

A *finite Markov Decision Process* (MDP) is a tuple 

$$M=\left(\St_M,\ \A_M,\ \T_M: \St_M \times \A_M \M \St_M,\ \R_M: \St_M \rightarrow [0,1]\right)$$

Here, $\St_M$ is a finite set (the set of states), $\A_M$ is a finite set (the set of actions), $\T_M$ is the transition kernel and $\R_M$ is the reward function.

A *stationary policy* for $M$ is any $\pi: \St_M \M \A_M$. The space of stationary policies is denoted $\Pi_M$. Given $\pi \in \Pi_M$, we define $\T_{M\pi}: \St_M \M \St_M$ by

$$\T_{M\pi}(t \mid s) := \T_M\left(t \mid s, \pi(s)\right)$$

%$$\pi^M_0(s) = \delta_s$$
%
%$$\pi^M_{n+1}\left(s \mid s_0\right) = \E{s_n \sim \pi^M_n(s_0)}\left[\T_M\left(s \mid s_n, \pi\left(s_n\right)\right)\right]$$
%
%we define $\pi^M: \St_M \M \St_M^\omega$ by requiring that for any $s_0 \in \St_M$, $s \in \St_M$, $n \in \Nats$ and $h \in \St_M^{n+1}$
%
%$$\Supp{\pi^M\left(s_{0}\right)} \subseteq s_0 \St_M^\omega$$
%
%$$\pi^M\left(s_{0}\right)(s \mid h):=\T_M\left(s \mid h_n, \pi\left(h_n\right)\right)$$

We define $\V_M : \St_M \times (0,1) \rightarrow [0,1]$ and $\Q_M: \St_M \times \A_M \times (0,1) \rightarrow [0,1]$ by

$$\V_M(s,\gamma):=(1-\gamma)\max_{\pi \in \Pi_M} \sum_{n=0}^\infty \gamma^n \E{\T_{M\pi}^n(s)}\left[\R_M\right]$$

$$\Q_M(s,a,\gamma):=(1-\gamma)\R_M(s)+\gamma\E{t \sim \T_M(s,a)}\left[\V_M(t,\gamma)\right]$$

Here, $\T_{M\pi}^n: \St_M \M \St_M$ is just the $n$-th power of $\T_{M\pi}$ in the sense of Markov kernel composition.

As well known, $\V_M$ and $\Q_M$ are rational functions in $\gamma$ for $1-\gamma \ll 1$, therefore in this limit we have the Taylor expansions

$$\V_M(s,\gamma)=\sum_{k=0}^\infty {\frac{1}{k!} \V_M^k(s)\cdot(1-\gamma)^k}$$

$$\Q_M(s,a,\gamma)=\sum_{k=0}^\infty {\frac{1}{k!} \Q_M^k(s,a)\cdot(1-\gamma)^k}$$

Given any $s \in \St_M$, we define $\Sq{\A_M^k(s) \subseteq \A_M}{k}$ recursively by

$$\A^0_M(s) := \Argmax{a \in \A_M} \Q_M^0(s,a)$$

$$\A^{k+1}_M(s) := \Argmax{a \in \A_M^k(s)} \Q_M^{k+1}(s,a)$$

***

All MDPs will be assumed to be finite, so we drop the adjective "finite" from now on.

\#Definition 2

Let $\In=(\A,\Ob)$ be an interface. An $\In$-universe $\upsilon=(\mu,r)$ is said to be *an $\Ob$-realization of MDP $M$ with state function $S: \HD{\mu} \rightarrow \St_M$* when $\A_M=\A$ and for any $h \in \HD{\mu}$, $a \in \A$ and $o \in \Ob$

$$\T_M\left(s \mid S(h),a\right) =\Pr_{o \sim \mu(ha)}\left[S(hao)=s\right]$$

$$r(h)=\R_M\left(S(h)\right)$$

***

Now now define the relevant notion of a "good advisor."

\#Definition 3

Let $\upsilon = (\mu,r)$ be a universe and $\epsilon > 0$. A policy $\pi$ for is said to be *$\epsilon$-sane for $\upsilon$* when there are $M$, $S$ s.t. $\upsilon$ is an $\Ob$-realization of $M$ with state function $S$ and for any $h \in \HD{\mu}$

i. $\Supp{\pi(h)} \subseteq \A_M^0\left(S(h)\right)$

ii. $\exists a \in \A_M^1\left(S(h)\right): \pi(a \mid h) > \epsilon$

***


We can now formulate the regret bound.

\#Theorem 1

Fix an interface $\In$ and $\epsilon > 0$. Consider $\Hy = \{\upsilon^k = (\mu^k,r) \in \Upsilon_{\In}\}_{k \in [N]}$ for some $N \in \Nats$ (note that $r$ doesn't depend on $k$). Assume that for each $k \in [N]$, $\sigma^k$ is a $\epsilon$-sane policy for $\upsilon^k$. Then, there is an $\bar{\In}$-metapolicy $\pi^*$ s.t. for any $k \in [N]$

$$\EU_{\upsilon^k}^*(\gamma) - \EU_{\bar{\upsilon}^k\left[\sigma^k\right]}^{\pi^*}(\gamma) = O\left((1-\gamma)^{1/4}\right)$$

\#Corollary 1

Fix an interface $\In$ and $\epsilon > 0$. Consider $\Hy = \{\upsilon^k = (\mu^k,r) \in \Upsilon_{\In}\}_{k \in \Nats}$. Assume that for each $k \in \Nats$, $\sigma^k$ is a $\epsilon$-sane policy for $\upsilon^k$. Define $\bar{\Hy}:=\Sqn{\bar{\upsilon}^k\left[\sigma^k\right]}$. Then, $\bar{\Hy}$ is learnable.

***

Now, we deal with corrupt states.

\#Definition 4

Let $\upsilon = (\mu,r)$ be a universe and $\epsilon > 0$. A policy $\pi$ for is said to be *locally $\epsilon$-sane for $\upsilon$* when there are $M$, $S$ and $\UC \subseteq \St_M$ (the set of uncorrupt states) s.t. $\upsilon$ is an $\Ob$-realization of $M$ with state function $S$, $S(\Estr) \in \UC$ and for any $h \in \HD{\mu}$, if $S(h) \in \UC$ then

i. If $a \in \Supp{\pi(h)}$ and $o \in \Supp{\mu(ha)}$ then $S\left(hao\right) \in \UC$.

ii. $\Supp{\pi(h)} \subseteq \A_M^0\left(S(h)\right)$

iii. $\exists a \in \A_M^1\left(S(h)\right): \pi(a \mid h) > \epsilon$

***

Of course, this requirement is still unrealistic for humans in the real world. In particular, it makes the formalism unsuitable for modeling the use of AI for catastrophe mitigation (which is ultimately what we are interested in!) since it assumes the advisor is already capable of avoiding any catastrophe. In following work, we plan to relax the assumptions further.

\#Corollary 2

Fix an interface $\In$ and $\epsilon > 0$. Consider $\Hy = \{\upsilon^k = (\mu^k,r^k) \in \Upsilon_{\In}\}_{k \in [N]}$ for some $N \in \Nats$. Assume that for each $k \in [N]$, $\sigma^k$ is locally $\epsilon$-sane for $\upsilon^k$. For each $k \in [N]$, let $\UC^k \subseteq \St_{M^k}$ be the corresponding set of uncorrupt states. Assume further that for any $k,j \in \Nats$ and $h \in \HD{\mu^k} \cap \HD{\mu^j}$, if $S^k(h) \in \UC^k$ and $S^j(h) \in \UC^j$, then $r^k(h)=r^j(h)$. Then, there is an $\bar{\In}$-policy $\pi^*$ s.t. for any $k \in [N]$

$$\EU_{\upsilon^k}^*(\gamma) - \EU_{\bar{\upsilon}^k\left[\hat{\sigma}^k\right]}^{\pi^*}(\gamma) = O\left((1-\gamma)^{1/4}\right)$$

\#Corollary 3

Assume the same conditions as in Corollary 2, except that $\Hy$ may be countable infinite. Then, $\bar{\Hy}$ is learnable.

\section{Appendix A}

First, we prove an information theoretic bound that shows that for Thompson sampling, the expected information gain is bounded below by a function of the loss.

\#Proposition A.N1

Consider a probability space $(\Omega, P \in \Delta\Omega)$, $N \in \Nats$, $R \subseteq [0,1]$ a finite set and random variables $U: \Omega \rightarrow R$, $K: \Omega \rightarrow [N]$ and $\J: \Omega \rightarrow [N]$. Assume that $K_*P = J_*P = \zeta \in \Delta[N]$ and $\I{}[K;J] = 0$. Then

$$\I{}\left[K;J,U\right] \geq 2 \left(\min_{i \in [N]} {\zeta(i)}\right) \left(\E{}\left[U \mid J = K\right]-\E{}\left[U\right]\right)^2$$

\#Proof of Proposition A.N1

We have

$$\I{}\left[K;J,U\right] = \I{}\big[K;J\big] + \I{}\left[K;U \mid J\right] = \I{}\left[K;U \mid J\right] = \E{}\left[\KL{U_*\left(P \mid K,J\right)}{\ U_*\left(P \mid J\right)}\right]$$

Using Pinsker's inequality, we get

$$\Ia{}{K;J,U} \geq 2\Ea{}{\Dtva{U_*\left(P \mid K,J\right),U_*\left(P \mid J\right)}^2} \geq 2\Ea{}{\AP{\Ea{}{U \mid K,J}-\Ea{}{U \mid J}}^2}$$

Denote $U_{kj} := \Ea{}{U \mid K = k, J = j}$. We get

$$\Ia{}{K;J,U} \geq 2\Ea{(k,j)\sim\zeta\times\zeta}{\AP{U_{kj}-\Ea{k'\sim\zeta}{U_{k'j}}}^2} \geq 2\Ea{(k,j)\sim\zeta\times\zeta}{[[k=j]]\AP{U_{kj}-\Ea{k'\sim\zeta}{U_{k'j}}}^2}$$

$$\Ia{}{K;J,U} \geq 2\Ea{j \sim \zeta}{\zeta(j)\AP{U_{jj}-\Ea{k\sim\zeta}{U_{kj}}}^2} \geq 2 \AP{\min_{j \in [N]} \zeta(j)}\Ea{j \sim \zeta}{\AP{U_{jj}-\Ea{k\sim\zeta}{U_{kj}}}^2}$$

$$\Ia{}{K;J,U}\geq 2 \AP{\min_{j \in [N]} \zeta(j)} \AP{\Ea{j \sim \zeta}{U_{jj}}-\Ea{(k,j)\sim\zeta\times\zeta}{U_{kj}}}^2=2 \left(\min_{i \in [N]} {\zeta(i)}\right) \left(\E{}\left[U \mid J = K\right]-\E{}\left[U\right]\right)^2$$

***

Now, we describe a "delegation routine" $\D$ that can transform any "proto-policy" $\pi$ that recommends some *set* of actions from $\A$ into an actual $\Adi$-policy s.t (i)  with high probability, on each round, either a "safe" recommended action is taken, or all recommended actions are "unsafe" or delegation is performed and (ii) the expected number of delegations is small. For technical reasons, we also need to the modified routines $\D^{!k}$ which behave the same way as $\D$ except for some low probability cases.

\#Proposition A.N2

Fix $\epsilon,\delta \in (0,1)$, $N \in \Nats$ and an interface $\In=(\A,\Ob)$. Consider some $\{\sigma^k: \FH \M \A\}_{k \in [N]}$. Then, there exist $\D: \Adfh \times 2^\A \rightarrow \Ada$ and $\{\D^{!k}: \Adfh \times 2^\A \rightarrow \Ada\}_{k \in [N]}$ with the following properties. Given $x \in \left(2^\A \times \Adao\right)^*$, we denote $\underline{x}$ its projection to $\Adfh$. Thus, $\underline{\underline{x}}\in\FH$.
Given  $\mu$ an $\In$-environment, $\pi: \HD{\mu} \M 2^\A$, $\D': \Adfh \times 2^\A \rightarrow \Ada$ and $k \in [N]$, we can define $\Xi\left[\mu,\sigma^k,\D',\pi\right]\in \Delta\left(2^\A \times \Adao\right)^\omega$ as follows
 
$$\Xi\left[\mu,\sigma^k,\D',\pi\right]\left(\B,a,o \mid x\right):=\pi\left(\B \mid \underline{\underline{x}}\right)\D'\left(a \mid \underline{x},\B\right) \bar{\mu}[\sigma^k]\left(o \mid \underline{x}a\right)$$

We require that for every $\pi$, $\mu$ and $k$ as above, the following conditions hold

i. Both for $\D'=\D$ and for $\D'=\D^{!k}$

$$\E{x \sim\Xi\left[\mu,\sigma^k,\D',\pi\right]}\left[\Abs{\{n \in \Nats \mid x_n \in 2^\A \times \bot \times \bar{\Ob}\}}\right] \leq \frac{\ln N}{\delta \ln\left(1 + \epsilon(1-\epsilon)^{\frac{1-\epsilon}{\epsilon}}\right)}=O\left(\frac{\ln N}{\delta \epsilon}\right)$$

ii. $\Dtv\left(\Xi\left[\mu,\sigma^k,\D,\pi\right],\Xi\left[\mu,\sigma^k,\D^{!k},\pi\right]\right) \leq N\delta$

iii. For all $x \in \HD{\bar{\mu}[\sigma^k]}$, if $\D^{!k}\left(x,\pi\left(\underline{x}\right)\right) \ne \bot$ then $\sigma^k\left(\D^{!k}\left(x,\pi\left(\underline{x}\right)\right) \mid \underline{x}\right) > 0$

iv. For all $x \in \HD{\bar{\mu}[\sigma^k]}$, if $\D^{!k}\left(x,\pi\left(\underline{x}\right)\right) \not\in \pi\left(\underline{x}\right) \cup \{\bot\}$ then $\forall a \in \pi\left(\underline{x}\right): \sigma^k\left(a \mid \underline{x}\right) \leq \epsilon$

***

In order to prove Proposition A.N2, we need another mutual information bound.

\#Proposition A.N5

Consider $\epsilon,\delta \in (0,1)$, $\A$ a finite set, $\B \subseteq \A$, $N \in \Nats$, $\zeta \in \Delta[N]$ and $\sigma: [N] \M \A$. Suppose that for every $a \in \A$

$$\Pa{k \sim \zeta}{\sigma(a \mid k) > 0 \land \AP{a \in \B \lor \forall b \in \B: \sigma(b \mid k) \leq \epsilon}} \leq 1 - \delta$$

Then

$$\Ia{(k,a)\sim\zeta\ltimes\sigma}{k;a} \geq \delta \ln\left(1 + \epsilon(1-\epsilon)^{\frac{1-\epsilon}{\epsilon}}\right)$$

\#Proof of Proposition A.N5

We define $\Sq{\A_k: \Adfh \times 2^\A \rightarrow 2^A}{k}$, $\zeta: \Adfh \M [N]$, $\Sq{\zeta^{!k}: \Adfh \M [N]}{k}$, $\D$ and $\Sq{\D^{!k}}{k}$ by simultaneous recursion. For each $h \in \Adfh$, $a \in \Ada$, $o \in \Ado$, $\B \subseteq \A$ and $k \in [N]$, we require

$$\A_k(h, \B):=\{a \in \A \mid \sigma^k\AP{a \mid \underline{h}} > 0 \land \AP{a \in \B \lor \forall b \in \B: \sigma^k\AP{b \mid \underline{h}} \leq \epsilon}\}$$
%
$$\zeta(k \mid \Estr):=\frac{1}{N}$$
%
$$\zeta(k \mid hao):=\begin{cases} \zeta(k \mid h)[[a \in \A_k(h,\B?)]]??? \text{ if } a\ne\bot \\ \AP{\sum_{j=0}^{N-1} \zeta(j \mid h)\cdot \sigma^j\AP{b \mid \underline{h}}}^{-1} \zeta(k \mid h)\cdot \sigma^k\AP{b \mid \underline{h}} \text{ if } a = \bot,\ o \in b\Ob \end{cases}$$
%
$$\zeta^{!k}...\ ?$$
%
$$\D(h,\B) := \begin{cases} \text{any element of } \Argmax{a \in \A} \Pa{k\sim\zeta(h)}{a \in \A_k(h,\B)} \text{ if } \max_{a \in \A} \Pa{k\sim\zeta(h)}{a \in \A_k(h,\B)} \geq 1-\delta \\ \bot \text{ otherwise} \end{cases}$$
%
$$\D^{!k}(h,\B):=\begin{cases} \D(h,\B) \text{ if } \D(h,\B) \in \A_k(h,\B) \\ \text{any element of } \A_k(h,\B) \text{ otherwise} \end{cases}???$$

% Define \zeta so that it discards any hypothesis with prob. < \delta
% This ensures that delegation occurs iff there is no action which is good with probability 1
% Alternative would be discarding hypothesis according to which prev. action was bad
% That would complicate notation since \D has to depend on the history of recommendation sets either
% \D^{!k} should be defined using \zeta^{!k}, not through \D

TBD

\#Proof of Proposition A.N2

Define...

TBD

\#Definition A.N1

Consider $k \in \Nats$ and a universe $\upsilon=(\mu,r)$ that is an $\Ob$-realization of $M$ with state function $S$. A policy $\pi$ is called *$k$-optimal for $\upsilon$* when for any $h \in \HD{\mu}$

$$\Supp \pi(h) \subseteq \A_M^k\left(S(h)\right)$$

$\pi$ is called *Blackwell optimal for $\upsilon$* when it is $k$-optimal for any $k \in \Nats$. By standard results in MDP theory, a stationary Blackwell optimal policy always exists, and any Blackwell optimal policy has optimal expected utility for any $\gamma$ sufficiently close to 1.

***

The following proposition relates optimality in terms of expected utility to expected *truncated* utility, where truncated utility is defined by only summing rewards within a time duration $T$.

\#Proposition A.N3

Fix an MDP $M$. Then, for any $\gamma\in(0,1)$, $T \in \Nats$, universe $\upsilon=(\mu,r)$ that is an $\Ob$-realization of $M$ with state function $S$, $\pi^1$ a $1$-optimal policy for $\upsilon$ and $\pi^*$ a Blackwell optimal policy for $\upsilon$, we have

$$\sum_{n=0}^{T-1} \gamma^n \left(\E{x \sim \mu\bowtie\pi^*}\left[r\left(x_{:n}\right)\right]-\E{x \sim \mu\bowtie\pi^1}\left[r\left(x_{:n}\right)\right]\right) \leq O\left(1+T\left(1-\gamma\right)\right)$$

\#Proof of Proposition A.N3

TBD

***

The following shows that for any policy that doesn't make "irreversible errors," regret can be approximated by "episodic regret" for sufficiently large episode duration.

\#Proposition A.N4

Consider a universe $\upsilon=(\mu,r)$ that is an $\Ob$-realization of $M$ with state function $S$. Suppose that $\pi^*$ is a Blackwell optimal policy for $\upsilon$ and $\pi^0$ is a $0$-optimal policy for $\upsilon$. For any $n \in \Nats$, let $\pi^*_n$ be a policy s.t. for any $h \in \HD{\mu}$

$$\pi^*_n(h):=\begin{cases} \pi^0(h) \text{ if } \Abs{h} < nT \\ \pi^*(h) \text{ otherwise} \end{cases}$$

Then, for any $\gamma\in(0,1)$ and $T \in \Nats^+$

$$\EU^*_\upsilon(\gamma)-\EU^{\pi^0}_\upsilon(\gamma) \leq (1-\gamma)\sum_{n=0}^\infty \sum_{m=0}^{T-1} \gamma^{nT+m}\left(\E{x\sim\mu\bowtie\pi^*_n}\left[r\left(x_{:nT+m}\right)\right]-\E{x\sim\mu\bowtie\pi^0}\left[r\left(x_{:nT+m}\right)\right]\right) + O\left(\frac{1}{T}\right)$$

\#Proof of Proposition A.N4

TBD

\#Proof of Theorem 1

Fix $\gamma \in (0,1)$, $\delta\in\left(0,N^{-1}\right)$ and $T \in \Nats^+$. For each $k \in [N]$, suppose $\upsilon^k$ is an $\Ob$-realization of $M^k$ with state function $S^k$ and denote $\nu^k:=\bar{\mu}^k\left[\sigma^k\right]$. To avoid cumbersome notation, whenever $M^k$ should appear a subscript, we will replace it by $k$. Let $(\Omega,P \in \Delta\Omega)$ be a probability space\Comment{ and $\{\F_n \subseteq 2^\Omega\}_{n \in \Nats \sqcup \{-1\}}$ a filtration of $\Omega$}. Let $K: \Omega \rightarrow [N]$ be \Comment{measurable w.r.t. $\F_{-1}$}a random variable and the following be stochastic processes\Comment{ adapted to $\F$}

$$\Z_n,\tilde{\Z}_n: \Omega \rightarrow \Delta[N]$$
%
$$\J_n: \Omega \rightarrow [N]$$
%
$$\Psi_n: \Omega \rightarrow 2^\A$$
%
$$A_n: \Omega \rightarrow \Ada$$
%
$$\Theta_n: \Omega \rightarrow \Ado$$

We also define $A\Theta_{:n}: \Omega \rightarrow \Adfh$ by

$$A\Theta_{:n}:= A_0\Theta_0A_1\Theta_1 \ldots A_{n-1}\Theta_{n-1}$$

(The following conditions on $A$ and $\Theta$ imply that the range of the above is indeed in $\Adfh$.) Let $\D$ and $\D^{!k}$ be as in Proposition A.N2. We construct $\Omega$\Comment{, $\F$}, $K$, $\Z$, $\tilde{\Z}$, $\J$, $\Psi$, $A$ and $\Theta$ s.t $K$ is uniformly distributed and for any $k \in [N]$, $l \in \Nats$, $m \in [T]$ and $o \in \Ob$, denoting $n = lT+m$

$$\tilde{\Z}_0(k)\equiv\frac{1}{N}$$
%
$$\Z_{n}(k) = \frac{\tilde{\Z}_{n}(k)[[\tilde{\Z}_{n}(k) \geq \delta]] }{\sum_{j = 0}^{N-1}\tilde{\Z}_{n}(j)[[\tilde{\Z}_{n}(j) \geq \delta]]}$$
%
$$\Pr\left[\J_{l} = k \mid Z_{lT}\right] = \Z_{lT}\left(k\right)$$
%
$$\Psi_{n} = \A^1_{\J_l}\left(S^{\J_l}(A\Theta_{:n})\right)$$
%
$$\Pr\left[\Theta_{n} = o \mid A\Theta_{:n}\right] = \nu^K\left(o \mid A\Theta_{:n}\right)$$
%
$$A_n = \D\left(A\Theta_{:n}, \Psi_n\right)$$
%
$$\tilde{\Z}_{n+1}(k)\sum_{j = 0}^{N-1} \Z_n(j) [[A_n = \D^{!j}\left(A\Theta_{:n}, \Psi_n\right)]] \nu^j(\Theta_n \mid A\Theta_{:n}A_n)=\Z_{n}(k) [[A_n = \D^{!k}\left(A\Theta_{:n}, \Psi_n\right)]] \nu^k\left(\Theta_{n} \mid A\Theta_{:n}A_{n}\right)$$

Note that the last equation has the form of a Bayesian update which is allowed to be arbitrary when update is on "impossible" information.

We now construct the $\Adi$-policy $\pi^*$ s.t. for any $n \in \Nats$, $h \in \Adfh$ s.t. $\Pr\left[A\Theta_{:n}=h\right] > 0$ and $a \in \Ada$

$$\pi^*(a \mid h):=\Pr\left[A_n = a \mid A\Theta_{:n} = h\right]$$

That is, we perform Thompson sampling at time intervals of size $T$, moderated by the delegation routine $\D$, and discard from our belief state hypotheses whose probability is below $\delta$ and hypotheses sampling which resulted in recommending "unsafe" actions i.e. actions that $\D$ refused to perform.

In order to prove $\pi^*$ has the desired property, we will define the stochastic processes $\Z^!$, $\tilde{\Z}^!$, $\J^!$, $\Psi^!$, $A^!$ and $\Theta^!$, each process of the same type as its shriekless counterpart (thus $\Omega$ is constructed to accommodate them). These processes are required to satisfy the following:

$$\tilde{\Z}^!_0(k)\equiv\frac{1}{N}$$
%
$$\Z_{n}^!(k) = \frac{\tilde{\Z}^!_{n}(k)[[\tilde{\Z}^!_{n}(k) \geq \delta]] }{\sum_{j = 0}^{N-1}\tilde{\Z}^!_{n}(j)[[\tilde{\Z}^!_{n}(j) \geq \delta]]}[[\tilde{\Z}^!_{n}(K) \geq \delta]] + [[K = k]]\cdot [[\tilde{\Z}^!_{n}(K) < \delta]]$$
%
$$\Pr\left[\J^!_{l} = k \mid Z^!_{lT}\right] = \Z^!_{lT}\left(k\right)$$
%
$$\Psi^!_{n} = \A^1_{\J^!_l}\left(S^{\J^!_l}(A\Theta^!_{:n})\right)$$
%
$$\Pr\left[\Theta^!_{n} = o \mid A\Theta^!_{:n}\right] = \nu^K\left(o \mid A\Theta^!_{:n}\right)$$
%
$$A^!_n = \D^{!K}\left(A\Theta^!_{:n}, \Psi^!_n\right)$$

$$\tilde{\Z}^!_{n+1}(k)=\frac{\Z^!_{n}(k) [[A^!_n = \D^{!k}\left(A\Theta^!_{:n}, \Psi^!_n\right)]] \nu^k\left(\Theta^!_{n} \mid A\Theta^!_{:n}A^!_{n}\right)}{\sum_{j = 0}^{N-1} \Z^!_n(j) [[A^!_n = \D^{!j}\left(A\Theta^!_{:n}, \Psi^!_n\right)]] \nu^j(\Theta^!_n \mid A\Theta^!_{:n}A^!_n)}$$

For any $k \in [N]$, we construct the $\Adi$-policy $\pi^{?k}$ s.t. for any $n \in \Nats$, $h \in \Adfh$ s.t. $\Pr\left[A\Theta^!_{:n}=h,\ K = k\right] > 0$ and $a \in \Ada$

$$\pi^{?k}(a \mid h):=\Pr\left[A^!_n = a \mid A\Theta^!_{:n} = h,\ K = k\right]$$

Given any $\Adi$-policy $\pi$ and $\In$-policy $\sigma$ we define $\alpha_{\sigma\pi}: \FH \M \Adfh$ by

$$\alpha_{\sigma\pi} (g \mid h) := [[h = \underline{g}]]C_h\prod_{n = 0}^{\Abs{h}-1} \sum_{a \in \A}\left([[g_n \in \bot a\Ob]] \pi\left(\bot \mid g_{:n}\right)\sigma\left(a \mid h_{:n}\right)+[[g_n \in a\bot\Ob]]\pi\left(a \mid g_{:n}\right)\right)$$

Here, $C_h \in \Reals$ is a constant defined s.t. the probabilities sum to 1. We define the $\In$-policy $\left[\sigma\right]\underline{\pi}$ by

$$\left[\sigma\right]\underline{\pi}(a \mid h):=\Pr_{g \sim \alpha_{\sigma\pi}(h)}\left[\pi\left(g\right)=a \lor \left(\pi\left(g\right)=\bot \land \sigma(h)=a\right)\right]$$

Condition iii of Proposition A.N2 and condition i of $\epsilon$-sanity for $\sigma^k$ imply that for any $h \in \HD{\mu^k}$

$$\Supp{\left[\sigma^k\right]\underline{\pi}^{?k}(h)} \subseteq \A^0_k\left(S^k\left(h\right)\right)$$

This means we can apply Proposition A.N4 and get

$$\EU^*_{\upsilon^k}(\gamma)-\EU^{\pi^{?k}}_{\bar{\upsilon}^k[\sigma^k]}(\gamma) \leq (1-\gamma)\sum_{n=0}^\infty \sum_{m=0}^{T-1} \gamma^{nT+m}\left(\E{x\sim\mu^k\bowtie\pi^{*k}_n}\left[r\left(x_{:nT+m}\right)\right]-\E{x\sim\nu^k\bowtie\pi^{?k}}\left[r\left(\underline{x}_{:nT+m}\right)\right]\right) + O\left(\frac{1}{T}\right)$$

Here, the $\In$-policy $\pi^{*k}_n$ is defined as $\pi^*_n$ in Proposition A.N4. We also define the $\Adi$-policies $\pi^{!k}_n$ and $\pi^{!!k}_n$ by

$$\pi^{!k}_n(a \mid h):=\begin{cases} \pi^{?k}(a \mid h) \text{ if } \Abs{h} < nT \\ \Pr\left[A^!_{\Abs{h}} = a \mid A\Theta^!_{:{\Abs{h}}} = h,\ K = k,\ \J^!_n = k\right] \text{ otherwise} \end{cases}$$

$$\pi^{!!k}_n(a \mid h):=\begin{cases} \pi^{?k}(a \mid h) \text{ if } \Abs{h} < nT \\ \pi^{!k}_n(a \mid h) + \pi^{!k}_n(\bot \mid h) \cdot \pi^{*k}_n\left(a \mid \underline{h}\right) \text{ if } \Abs{h} \geq nT \text{ and } a \ne \bot \\ 0 \text{ if } \Abs{h} \geq nT \text{ and } a = \bot \end{cases}$$

Denote $\rho^{*k}_n:=\mu^k\bowtie\pi^{*k}_n$, $\rho^{!!k}_n:=\nu^k\bowtie\pi^{!!k}_n$, $\rho^{!k}_n:=\nu^k\bowtie\pi^{!k}_n$, $\rho^{?k}:=\nu^k\bowtie\pi^{?k}$, $R^{?k}=\EU^*_{\upsilon^k}(\gamma)-\EU^{\pi^{?k}}_{\bar{\upsilon}^k[\sigma^k]}(\gamma)$. For each $n \in \Nats$, denote

$$\EU_n^{*k}(\gamma):=\frac{1-\gamma}{1-\gamma^T}\sum_{m=0}^{T-1} \gamma^{m}\E{x\sim\rho^{*k}_n}\left[r\left(x_{:nT+m}\right)\right]$$

$$\EU_n^{!!k}(\gamma):=\frac{1-\gamma}{1-\gamma^T}\sum_{m=0}^{T-1} \gamma^{m}\E{x\sim\rho^{!!k}_n}\left[r\left(\underline{x}_{:nT+m}\right)\right]$$

$$\EU_n^{!k}(\gamma):=\frac{1-\gamma}{1-\gamma^T}\sum_{m=0}^{T-1} \gamma^{m}\E{x\sim\rho^{!k}_n}\left[r\left(\underline{x}_{:nT+m}\right)\right]$$

$$\EU_n^{?k}(\gamma):=\frac{1-\gamma}{1-\gamma^T}\sum_{m=0}^{T-1} \gamma^{m}\E{x\sim\rho^{?k}}\left[r\left(\underline{x}_{:nT+m}\right)\right]$$

We have

$$R^{?k} \leq (1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \left(\EU^{*k}_n(\gamma)-\EU^{?k}_n(\gamma)\right) + O\left(\frac{1}{T}\right)$$

$$R^{?k} \leq (1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \left(\EU^{*k}_n(\gamma)-\EU^{!!k}_n(\gamma)+\EU^{!!k}_n(\gamma)-\EU^{!k}_n(\gamma)+\EU^{!k}_n(\gamma)-\EU^{?k}_n(\gamma)\right) + O\left(\frac{1}{T}\right)$$

Condition iv of Proposition A.N2 and condition ii of $\epsilon$-sanity for $\sigma^k$ imply that, given $h \in \HD{\nu^k}$ s.t. $\Abs{h} \geq nT$

$$\Supp{\pi^{!k}_n(h)} \subseteq \A^1_k\left(S^k\left(\underline{h}\right)\right) \cup \{\bot\}$$
%
$$\Supp{\pi^{!!k}_n(h)} \subseteq \A^1_k\left(S^k\left(\underline{h}\right)\right)$$

Therefore, we can apply Proposition A.N3 to the terms $\EU^{*k}_n(\gamma)-\EU^{!!k}_n(\gamma)$ and get

$$R^{?k} \leq (1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \left(\EU^{!!k}_n(\gamma)-\EU^{!k}_n(\gamma)+\EU^{!k}_n(\gamma)-\EU^{?k}_n(\gamma)\right) + O\left(\frac{1}{T}+\frac{1-\gamma}{1-\gamma^T}+T\frac{(1-\gamma)^2}{1-\gamma^T}\right)$$

We have

$$\EU^{!!k}_n(\gamma)-\EU^{!k}_n(\gamma) \leq \Pr_{x\sim\rho^{!k}_n}\left[\exists m \in [T]: x_{nT+m} \in \bot\Ado\right]$$

Thus, using condition i of Proposition A.N2, we can bound the contribution of the $\EU^{!!k}_n(\gamma)-\EU^{!k}_n(\gamma)$ terms and get

$$R^{?k} \leq (1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \left(\EU^{!k}_n(\gamma)-\EU^{?k}_n(\gamma)\right) + O\left(\frac{1-\gamma^T}{\delta}+\frac{1}{T}+\frac{1-\gamma}{1-\gamma^T}+T\frac{(1-\gamma)^2}{1-\gamma^T}\right)$$

We denote

$$\xi(\gamma,T,\delta,\epsilon):=\frac{1-\gamma^T}{\delta\epsilon}+\frac{1}{T}+\frac{1-\gamma}{1-\gamma^T}+T\frac{(1-\gamma)^2}{1-\gamma^T}$$

Define the random variables $\Sqn{U^!_n : \Omega \rightarrow [0,1]}$ by 

$$U^!_n:=\frac{1-\gamma}{1-\gamma^T}\sum_{m=0}^{T-1} \gamma^{m} r\left(A\Theta^!_{:nT+m}\right)$$

Averaging the previous inequality over $k$, we get

$$\frac{1}{N}\sum_{k=0}^{N-1}R^{?k} \leq (1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \left(\E{}\left[U^!_n \mid \J^!_n = K\right]-\E{}\left[U^!_n\right]\right) + O\left(\xi(\gamma,T,\delta,\epsilon)\right)$$

$$\frac{1}{N}\sum_{k=0}^{N-1}R^{?k} \leq (1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \E{}\left[\E{}\left[U^!_n \mid \J^!_n = K,\ Z^!_{nT}\right]-\E{}\left[U^!_n \mid Z^!_{nT}\right]\right] + O\left(\xi(\gamma,T,\delta,\epsilon)\right)$$

$$\frac{1}{N}\sum_{k=0}^{N-1}R^{?k} = \sqrt{(1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \E{}\left[\left(\E{}\left[U^!_n \mid \J^!_n = K,\ Z^!_{nT}\right]-\E{}\left[U^!_n \mid Z^!_{nT}\right]\right)^2\right]} + O\left(\xi(\gamma,T,\delta,\epsilon)\right)$$

We apply Proposition A.N1 to each term in the sum over $n$.

$$\frac{1}{N}\sum_{k=0}^{N-1}R^{?k} = \sqrt{(1-\gamma^T)\sum_{n=0}^\infty \gamma^{nT} \E{}\left[\frac{1}{2\delta}\I{}\left[K;\J^!_n,U^!_n \mid Z^!_{nT}\right]\right]} + O\left(\xi(\gamma,T,\delta,\epsilon)\right)$$

$$\frac{1}{N}\sum_{k=0}^{N-1}R^{?k} = \sqrt{\frac{1-\gamma^T}{2\delta}\sum_{n=0}^\infty \gamma^{nT} \E{}\left[\Ent\Big(Z^!_{nT}\Big)-\Ent\left(Z^!_{(n+1)T}\right)\right]} + O\left(\xi(\gamma,T,\delta,\epsilon)\right)$$

$$\frac{1}{N}\sum_{k=0}^{N-1}R^{?k} = \sqrt{\frac{1-\gamma^T}{2\delta}\ln N} + O\left(\frac{1-\gamma^T}{\delta}+\frac{1}{T}+\frac{1-\gamma}{1-\gamma^T}+T\frac{(1-\gamma)^2}{1-\gamma^T}\right)$$

$$\frac{1}{N}\sum_{k=0}^{N-1}R^{?k} = O\left(\sqrt{\frac{1-\gamma^T}{\delta}} + \frac{1}{T}+\frac{1-\gamma}{1-\gamma^T}+T\frac{(1-\gamma)^2}{1-\gamma^T}\right)$$

Condition ii of Proposition A.N2 implies that

$$\Dtv\left(\bar{\nu}^k\left[\sigma^k\right]\bowtie\pi^*,\ \bar{\nu}^k\left[\sigma^k\right]\bowtie\pi^{?k}\right) \leq 2N\delta$$

Here, the factor of 2 comes from the difference between the equations for $Z_n$ and $Z^!_n$ (we can construct and intermediate policy between $\pi^*$ and $\pi^{?k}$ and use the triangle inequality for $\Dtv$). We conclude

$$\EU^*_{\upsilon^k}(\gamma)-\EU^{\pi^{*}}_{\bar{\upsilon}^k[\sigma^k]}(\gamma) = O\left(\delta+\sqrt{\frac{1-\gamma^T}{\delta}} +\frac{1}{T}+\frac{1-\gamma}{1-\gamma^T}+T\frac{(1-\gamma)^2}{1-\gamma^T}\right)$$

Now we set $\delta:=\left(1-\gamma\right)^{1/4}$,  $T:=\Floor{\left(1-\gamma\right)^{-1/4}}$. Since $\gamma^T \rightarrow 1$ as $\gamma \rightarrow 1$, we can use the approximation $1-\gamma^T \approx T(1-\gamma) \approx (1-\gamma)^{3/4}$. This immediately gives the desired result.

TBD

\end{document}



