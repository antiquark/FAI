%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{dom}

% operators that require brackets
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\Var}{\operatorname{Var}}

% operators that require parentheses
\newcommand{\Ent}{\operatorname{H}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Sym}{\operatorname{Sym}}

\newcommand{\Prj}{\operatorname{pr}}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\In}{\mathcal{I}}
\newcommand{\FH}{(\A \times \Ob)^*}
\newcommand{\IH}{(\A \times \Ob)^\omega}
\newcommand{\Ado}{\bar{\Ob}}
\newcommand{\Ada}{\bar{\A}}
\newcommand{\Adi}{{\bar{\I}}}
\newcommand{\Adao}{\overline{\A \times \Ob}}
\newcommand{\Adfh}{\Adao^*}
\newcommand{\Adih}{\Adao^\omega}
\DeclareMathOperator{\HD}{hdom}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\Co}{\mathcal{C}}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\V}{\operatorname{V}}
\newcommand{\Q}{\operatorname{Q}}
\newcommand{\EU}{\operatorname{EU}}

\begin{document}

[Previously](https://agentfoundations.org/item?id=1550), we defined a setting called "Delegative Inverse Reinforcement Learning" (DIRL) in which the agent can delegate actions to an "advisor" and the reward is only visible to the advisor as well. We proved a sublinear regret bound (converted to traditional normalization in online learning, the bound is $O(n^{2/3})$) for one-shot DIRL (as opposed to standard regret bounds in RL which are only applicable in the *episodic* setting). However, this required a rather strong assumption about the advisor: in particular, the advisor had to choose the optimal action with maximal likelihood. Here, we consider "Delegative Reinforcement Learning" (DRL), i.e. a similar setting in which the reward is directly observable by the agent. We also restrict our attention to finite MDP environments (we believe these results can be generalized to a much bigger class of environments, but not to arbitrary environments). On the other hand, the assumption about the advisor is much weaker: the advisor is only required to avoid *catastrophic* actions (i.e. actions that lose value to zeroth order in inverse time discount) and assign some positive probability to a nearly optimal action. As before, we prove a one-shot regret bound (in traditional normalization, $O(n^{3/4})$). Analogously to [before](https://agentfoundations.org/item?id=1587), we allow for "corrupt" states in which both the advisor and the reward signal stop being reliable.

***

Appendix A contains the proofs...

\section{Notation}

%Given a set $X$, the notation $X^*$ means $\bigsqcup_{n \in \Nats} X^n$ and the notation $X^+$ means $X^* \setminus \Estr$, where $\Estr \in X^0$.

The notation $K: X \M Y$ means $K$ is Markov kernel from $X$ to $Y$. When $Y$ is a finite set, this is the same as a measurable function $K: X \rightarrow \Delta Y$, and we use these notations interchangeably. Given $K: X \M Y$ and $A \subseteq Y$ (corr. $y \in Y$), we will use the notation $K(A \mid x)$ (corr $K(y \mid x)$) to stand for $\Pr_{y' \sim K(x)}\left[y' \in A\right]$ (corr. $\Pr_{y' \sim K(x)}\left[y' = y\right]$). Given $Y$ is a finite set, $\mu \in \Delta Y^\omega$, $h \in Y^*$ and $y \in Y$, the notation $\mu(y \mid h)$ means $\Pr_{x \sim \mu}\left[x_{\Abs{h}} = y \mid x_{:\Abs{h}}=h\right]$.

Given $\Omega$ a measurable space, $\mu \in \Delta \Omega$, $n,m \in \Nats$, $\{A_k\}_{k \in [n]}$, $\{B_j\}_{j \in [m]}$ finite sets and $\{X_k: \Omega \rightarrow A_k\}_{k \in [n]}$, $\{Y_j: \Omega \rightarrow B_j\}_{j \in [m]}$ random variables (measurable mappings), the mutual information between the joint distribution of the $X_k$ and the joint distribution of the $Y_j$ will be denoted

$$\I{\omega \sim \mu}\left[X_0, X_1 \ldots X_{n-1}; Y_0, Y_1 \ldots Y_{m-1}\right]$$

We will parameterize our geometric time discount by $\gamma=e^{-1/t}$, thus all functions that were previously defined to depend on $t$ are now considered functions of $\gamma$.

\section{Results}

We start by explaining the relation between the formalism of general environments we used before and the formalism of finite MDPs.

\#Definition 1

A *finite Markov Decision Process* (MDP) is a tuple 

$$M=\left(\St_M,\ \A_M,\ \T_M: \St_M \times \A_M \M \St_M,\ \R_M: \St_M \rightarrow [0,1]\right)$$

Here, $\St_M$ is a finite set (the set of states), $\A_M$ is a finite set (the set of actions), $\T_M$ is the transition kernel and $\R_M$ is the reward function.

A *stationary policy* for $M$ is any $\pi: \St_M \M \A_M$. The space of stationary policies is denoted $\Pi_M$. Given $\pi \in \Pi_M$, we define $\T_{M\pi}: \St_M \M \St_M$ by

$$\T_{M\pi}(t \mid s) := \T_M\left(t \mid s, \pi(s)\right)$$

%$$\pi^M_0(s) = \delta_s$$
%
%$$\pi^M_{n+1}\left(s \mid s_0\right) = \E{s_n \sim \pi^M_n(s_0)}\left[\T_M\left(s \mid s_n, \pi\left(s_n\right)\right)\right]$$
%
%we define $\pi^M: \St_M \M \St_M^\omega$ by requiring that for any $s_0 \in \St_M$, $s \in \St_M$, $n \in \Nats$ and $h \in \St_M^{n+1}$
%
%$$\Supp{\pi^M\left(s_{0}\right)} \subseteq s_0 \St_M^\omega$$
%
%$$\pi^M\left(s_{0}\right)(s \mid h):=\T_M\left(s \mid h_n, \pi\left(h_n\right)\right)$$

We define $\V_M : \St_M \times (0,1) \rightarrow [0,1]$ and $\Q_M: \St_M \times \A_M \times (0,1) \rightarrow [0,1]$ by

$$\V_M(s,\gamma):=(1-\gamma)\max_{\pi \in \Pi_M} \sum_{n=0}^\infty \gamma^n \E{\T_{M\pi}^n(s)}\left[\R_M\right]$$

$$\Q_M(s,a,\gamma):=(1-\gamma)\R_M(s)+\gamma\E{t \sim \T_M(s,a)}\left[\V_M(t,\gamma)\right]$$

Here, $\T_{M\pi}^n: \St_M \M \St_M$ is just the $n$-th power of $\T_{M\pi}$ in the sense of Markov kernel composition.

As well known, $\V_M$ and $\Q_M$ are rational functions in $\gamma$ for $1-\gamma \ll 1$, therefore in this limit we have the Taylor expansions

$$\V_M(s,\gamma)=\sum_{k=0}^\infty {\frac{1}{k!} \V_M^k(s)\cdot(1-\gamma)^k}$$

$$\Q_M(s,a,\gamma)=\sum_{k=0}^\infty {\frac{1}{k!} \Q_M^k(s,a)\cdot(1-\gamma)^k}$$

Given any $s \in \St_M$, we define $\Sq{\A_M^k(s) \subseteq \A_M}{k}$ recursively by

$$\A^0_M(s) := \Argmax{a \in \A_M} \Q_M^0(s,a)$$

$$\A^{k+1}_M(s) := \Argmax{a \in \A_M^k(s)} \Q_M^{k+1}(s,a)$$

***

All MDPs will be assumed to be finite, so we drop the adjective "finite" from now on.

\#Definition 2

Let $\I=(\A,\Ob)$ be an interface. An $\In$-universe $\upsilon=(\mu,r)$ is said to be *an $\Ob$-realization of MDP $M$ with state function $S: \HD{\mu} \rightarrow \St_M$* when $\A_M=\A$ and for any $h \in \HD{\mu}$, $a \in \A$ and $o \in \Ob$

$$\T_M\left(s \mid S(h),a\right) =\Pr_{o \sim \mu(ha)}\left[S(hao)=s\right]$$

$$r(h)=\R_M\left(S(h)\right)$$

***

Now now define the relevant notion of a "good advisor."

\#Definition 3

Given an MDP $M$ and $\epsilon > 0$, a stationary policy $\pi$ for $M$ is said to be *$\epsilon$-sane* when for any $s \in \St_M$

$$\Supp{\pi(s)} \subseteq \A_M^0(s)$$

$$\exists a \in \A_M^1(s): \pi(a \mid s) \geq \epsilon$$

***


We can now formulate the regret bound.

\#Theorem 1

Fix an interface $\In$ and $\epsilon > 0$. Consider $\Hy = \{\upsilon^k = (\mu^k,r) \in \Upsilon_{\In}\}_{k \in [N]}$ for some $N \in \Nats$ (note that $r$ doesn't depend on $k$). Assume that for each $k \in [N]$, $\upsilon^k$ is an $\Ob$-realization of $M^k$ with state function $S^k$, $\sigma^k$ is a $\epsilon$-sane stationary policy for $M^k$, and $\hat{\sigma}^k$ is an $\In$-policy s.t. for each $h \in \HD{\mu^k}$, $\hat{\sigma}^k(h)=\sigma^k\left(S^k(h)\right)$. Then, there is an $\bar{\In}$-policy $\pi^*$ s.t. for any $k \in [N]$

$$\EU_{\upsilon^k}^*(\gamma) - \EU_{\bar{\upsilon}^k\left[\hat{\sigma}^k\right]}^{\pi^*}(\gamma) = O\left((1-\gamma)^{1/4}\right)?$$

\#Corollary 1

Fix an interface $\In$ and $\epsilon > 0$. Consider $\Hy = \{\upsilon^k = (\mu^k,r) \in \Upsilon_{\In}\}_{k \in \Nats}$. Assume that for each $k \in \Nats$, $\upsilon^k$ is an $\Ob$-realization of $M^k$ with state function $S^k$, $\sigma^k$ is a $\epsilon$-sane stationary policy for $M^k$, and $\hat{\sigma}^k$ is an $\In$-policy s.t. for each $h \in \HD{\mu^k}$, $\hat{\sigma}^k(h)=\sigma^k\left(S^k(h)\right)$. Define $\bar{\Hy}:=\Sqn{\bar{\upsilon^k}\left[\hat{\sigma}^k\right]}$. Then, $\bar{\Hy}$ is learnable.

***

Now, we deal with corrupt states.

\#Definition 4

Given an MDP $M$, $s_0 \in \St_M$ and $\epsilon > 0$, a stationary policy $\pi$ for $M$ is said to be *locally $\epsilon$-sane at $s_0$* when there is $\Co \subseteq \St_M$ (the set of corrupt states) s.t. for any $n \in \Nats$, $\Supp{\T_{M\pi}^n\left(s_0\right)} \subseteq \St_M \setminus \Co$ and for any $s \in \St_M \setminus \Co$

$$\Supp{\pi(s)} \subseteq \A_M^0(s)$$

$$\exists a \in \A_M^1(s): \pi(a \mid s) \geq \epsilon$$

***

Of course, this requirement is still unrealistic for humans in the real world. In particular, it makes the formalism unsuitable for modeling the use of AI for catastrophe mitigation (which is ultimately what we are interested in!) since it assumes the advisor is already capable of avoiding any catastrophe. In following work, we plan to relax the assumptions further.

\#Corollary 2

Fix an interface $\In$ and $\epsilon > 0$. Consider $\Hy = \{\upsilon^k = (\mu^k,r^k) \in \Upsilon_{\In}\}_{k \in [N]}$ for some $N \in \Nats$. Assume that for each $k \in [N]$, $\upsilon^k$ is an $\Ob$-realization of $M^k$ with state function $S^k$, $\sigma^k$ is a locally $\epsilon$-sane at $S^k(\Estr)$ stationary policy for $M^k$, and $\hat{\sigma}^k$ is an $\In$-policy s.t. for each $h \in \HD{\mu^k}$, $\hat{\sigma}^k(h)=\sigma^k\left(S^k(h)\right)$. For each $k \in [N]$, let $\Co^k \subseteq \St_{M^k}$ be the corresponding set of corrupt states. Assume further that for any $k,j \in \Nats$ and $h \in \HD{\mu^k} \cap \HD{\mu^j}$, if $S^k(h) \not\in \Co^k$ and $S^j(h) \not\in \Co^j$, then $r^k(h)=r^j(h)$. Then, there is an $\bar{\In}$-policy $\pi^*$ s.t. for any $k \in [N]$

$$\EU_{\upsilon^k}^*(\gamma) - \EU_{\bar{\upsilon}^k\left[\hat{\sigma}^k\right]}^{\pi^*}(\gamma) = O\left((1-\gamma)^{1/4}\right)?$$

\#Corollary 3

Assume the same conditions as in Corollary 2, except that $\Hy$ may be countable infinite. Then, $\bar{\Hy}$ is learnable.

\section{Appendix A}

\#Proposition A.N1

Consider a probability space $(\Omega, \mu \in \Delta\Omega)$, $N \in \Nats$, $R \subseteq [0,1]$ a finite set and random variables $\{X_{ij}: \Omega \rightarrow R\}_{i,j \in [N]}$. Assume that for any $i,j \in [N]$ 

$$\E{\mu}\left[X_{ii}\right] \geq \E{\mu}\left[X_{ij}\right]$$ 

Fix $\zeta \in \Delta[N]$. Then

$$\II{(i,j)\sim\zeta\times\zeta}{\omega\sim\mu}\left[i;j,X_{ij}(\omega)\right] \geq 2 \left(\min_{i \in \Nats} {\zeta(i)}\right) \left(\EE{i \in \zeta}{\omega \in \mu}\left[X_{ii}(\omega)\right]-\EE{(i,j) \in \zeta \times \zeta}{\omega \in \mu}\left[X_{ij}(\omega)\right]\right)^2$$

\#Proof of Proposition A.N1

Mwhahaha

\end{document}



