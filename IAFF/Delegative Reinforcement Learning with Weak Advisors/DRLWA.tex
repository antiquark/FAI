%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{dom}

% operators that require brackets
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\Var}{\operatorname{Var}}

% operators that require parentheses
\newcommand{\Ent}{\operatorname{H}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Sym}{\operatorname{Sym}}

\newcommand{\Prj}{\operatorname{pr}}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\FH}{(\A \times \Ob)^*}
\newcommand{\IH}{(\A \times \Ob)^\omega}
\newcommand{\Ado}{\bar{\Ob}}
\newcommand{\Ada}{\bar{\A}}
\newcommand{\Adi}{{\bar{\I}}}
\newcommand{\Adao}{\overline{\A \times \Ob}}
\newcommand{\Adfh}{\Adao^*}
\newcommand{\Adih}{\Adao^\omega}
\DeclareMathOperator{\HD}{hdom}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\Co}{\mathcal{C}}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\V}{\operatorname{V}}
\newcommand{\Q}{\operatorname{Q}}
\newcommand{\EU}{\operatorname{EU}}

\begin{document}

[Previously](https://agentfoundations.org/item?id=1550), we defined a setting called "Delegative Inverse Reinforcement Learning" (DIRL) in which the agent can delegate actions to an "advisor" and the reward is only visible to the advisor as well. We proved a sublinear regret bound (converted to traditional normalization in online learning, the bound is $O(n^{2/3})$) for one-shot DIRL (as opposed to standard regret bounds in RL which are only applicable in the *episodic* setting). However, this required a rather strong assumption about the advisor: in particular, the advisor had to choose the optimal action with maximal likelihood. Here, we consider "Delegative Reinforcement Learning" (DRL), i.e. a similar setting in which the reward is directly observable by the agent. We also restrict our attention to finite MDP environments (we believe these results can be generalized to a much bigger class of environments, but not to arbitrary environments). On the other hand, the assumption about the advisor is much weaker: the advisor is only required to avoid *catastrophic* actions (i.e. actions that lose value to zeroth order in inverse time discount) and assign some positive probability to the optimal action. As before, we prove a one-shot regret bound (in traditional normalization, $O(n^{3/4})$). Analogously to [before](https://agentfoundations.org/item?id=1587), we allow for "corrupt" states in which both the advisor and the reward signal stop being reliable.

***

Appendix A contains the proofs...

\section{Notation}

%Given a set $X$, the notation $X^*$ means $\bigsqcup_{n \in \Nats} X^n$ and the notation $X^+$ means $X^* \setminus \Estr$, where $\Estr \in X^0$.

The notation $K: X \M Y$ means $K$ is Markov kernel from $X$ to $Y$. When $Y$ is a finite set, this is the same as a measurable function $K: X \rightarrow \Delta Y$, and we use these notations interchangeably. Given $K: X \M Y$ and $A \subseteq Y$ (corr. $y \in Y$), we will use the notation $K(A \mid x)$ (corr $K(y \mid x)$) to stand for $\Pr_{y' \sim K(x)}\left[y' \in A\right]$ (corr. $\Pr_{y' \sim K(x)}\left[y' = y\right]$). Given $Y$ is a finite set, $\mu \in \Delta Y^\omega$, $h \in Y^*$ and $y \in Y$, the notation $\mu(y \mid h)$ means $\Pr_{x \sim \mu}\left[x_{\Abs{h}} = y \mid x_{:\Abs{h}}=h\right]$.

\section{Results}

We start by explaining the relation between the formalism of general environments we used before and the formalism of finite MDPs.

\#Definition 1

A *finite Markov Decision Process* (MDP) is a tuple 

$$M=\left(\St_M,\ \A_M,\ \T_M: \St_M \times \A_M \M \St_M,\ \R_M: \St_M \rightarrow [0,1]\right)$$

Here, $\St_M$ is a finite set (the set of states), $\A_M$ is a finite set (the set of actions), $\T_M$ is the transition kernel and $\R_M$ is the reward function.

A *stationary policy* for $M$ is any $\pi: \St_M \M \A_M$. The space of stationary policies is denoted $\Pi_M$. Given $\pi \in \Pi_M$, we define $\T_{M\pi}: \St_M \M \St_M$ by

$$\T_{M\pi}(t \mid s) := \T_M\left(t \mid s, \pi(s)\right)$$

%$$\pi^M_0(s) = \delta_s$$
%
%$$\pi^M_{n+1}\left(s \mid s_0\right) = \E{s_n \sim \pi^M_n(s_0)}\left[\T_M\left(s \mid s_n, \pi\left(s_n\right)\right)\right]$$
%
%we define $\pi^M: \St_M \M \St_M^\omega$ by requiring that for any $s_0 \in \St_M$, $s \in \St_M$, $n \in \Nats$ and $h \in \St_M^{n+1}$
%
%$$\Supp{\pi^M\left(s_{0}\right)} \subseteq s_0 \St_M^\omega$$
%
%$$\pi^M\left(s_{0}\right)(s \mid h):=\T_M\left(s \mid h_n, \pi\left(h_n\right)\right)$$

We define $\V_M : \St_M \times (0,1) \rightarrow [0,1]$ and $\Q_M: \St_M \times \A_M \times (0,1) \rightarrow [0,1]$ by

$$\V_M(s,\gamma):=(1-\gamma)\max_{\pi \in \Pi_M} \sum_{n=0}^\infty \gamma^n \E{\T_{M\pi}^n(s)}\left[\R_M\right]$$

$$\Q_M(s,a,\gamma):=(1-\gamma)\R_M(s)+\gamma\E{t \sim \T_M(s,a)}\left[\V_M(t)\right]$$

Here, $\T_{M\pi}^n: \St_M \M \St_M$ is just the $n$-th power of $\T_{M\pi}$ in the sense of Markov kernel composition.

As well known, $\V_M$ and $\Q_M$ are rational functions in $\gamma$ for $1-\gamma \ll 1$, therefore in this limit we have the Taylor expansions

$$\V_M(s,\gamma)=\sum_{k=0}^\infty {\frac{1}{k!} \V_M^k(s)\cdot(1-\gamma)^k}$$

$$\Q_M(s,a,\gamma)=\sum_{k=0}^\infty {\frac{1}{k!} \Q_M^k(s,a)\cdot(1-\gamma)^k}$$

Given any $s \in \St_M$, we define $\Sq{\A_M^k(s) \subseteq \A_M}{k}$ recursively by

$$\A^0_M(s) := \Argmax{a \in \A_M} \Q_M^0(s,a)$$

$$\A^{k+1}_M(s) := \Argmax{a \in \A_M^k(s)} \Q_M^{k+1}(s,a)$$

***

All MDPs will be assumed to be finite, so we drop the adjective "finite" from now on.

\#Definition 2

Let $\I=(\A,\Ob)$ be an interface. An $\I$-universe $\upsilon=(\mu,r)$ is said to be *of MDP type* when there is an MDP $M$ and $S: \HD{\mu} \rightarrow \St_M$ s.t. $\A_M=\A$ and for any $h \in \HD{\mu}$, $a \in \A$ and $o \in \Ob$

$$\T_M\left(s \mid S(h),a\right) =\Pr_{o \sim \mu(ha)}\left[S(hao)=s\right]$$

$$r(h)=\R_M\left(S(h)\right)$$

Given $\pi$ a stationary policy for $M$, we define the $\I$-policy $\pi^S$ by 

$$\pi^S(h) := \pi\left(S(h)\right)$$

***

Now now the relevant notion of a "good advisor."

\#Definition 3

Given an MDP $M$ and $\epsilon > 0$, a stationary policy $\pi$ for $M$ is said to be *strictly $\epsilon$-sane* when for any $s \in \St_M$

$$\Supp{\pi(s)} \subseteq \A_M^0(s)$$

$$\exists a \in \A_M^1(s): \pi(a \mid s) \geq \epsilon$$

TBD

\section{Appendix A}

\#Proposition A.1

Hahaha

\#Proof of Proposition A.1

Mwhahaha

\end{document}



