%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Dom}{dom}

% autosize delimiters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}
\newcommand{\AC}[1]{\left\{#1\right\}}
\newcommand{\APM}[2]{\left(#1\;\middle\vert\;#2\right)}
\newcommand{\ABM}[2]{\left[#1\;\middle\vert\;#2\right]}
\newcommand{\ACM}[2]{\left\{#1\;\middle\vert\;#2\right\}}

% operators that require brackets
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\CP}[3]{\underset{#1}{\operatorname{Pr}}\ABM{#2}{#3}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\CE}[3]{\underset{#1}{\operatorname{E}}\ABM{#2}{#3}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\CI}[3]{\underset{#1}{\operatorname{I}}\ABM{#2}{#3}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}

% operators that require parentheses
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
\newcommand{\PS}[1]{\mathcal{P}\AP{#1}}

\newcommand{\D}{\mathrm{d}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}\AP{#1\middle\vert\middle\vert#2}}
\newcommand{\RD}[3]{\operatorname{D}_{#1}\AP{#2\middle\vert\middle\vert#3}}
\newcommand{\Dtv}{\operatorname{d}_{\textnormal{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\textnormal{tv}}\AP{#1}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\Floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\Chev}[1]{\left\langle #1 \right\rangle}
\newcommand{\Quote}[1]{\left\ulcorner #1 \right\urcorner}

\newcommand{\K}{\xrightarrow{\textnormal{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}

% Paper specific

\newcommand{\A}{\mathcal{A}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Pe}{P}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\Co}{C}
\newcommand{\V}{\operatorname{V}}
\newcommand{\QV}{\operatorname{QV}}
\DeclareMathOperator{\Hi}{H}
\DeclareMathOperator{\Z}{Z}

\begin{document}

We introduce a variant of the concept of a "[quantilizer](https://intelligence.org/files/QuantilizersSaferAlternative.pdf)" for the setting of choosing a policy for a finite Markov decision process (MDP), where the generic unknown cost is replaced by an unknown penalty term in the reward function. This is essentially a generalization of quantilization in repeated games with a cost independence assumption. We show that the "quantilal" policy shares some properties with the ordinary optimal policy, namely that (i) it can always be chosen to be Markov (ii) it can be chosen to be stationary (but not deterministic of course) when time discount is geometric (iii) the "quantilum" value of an MDP with geometric time discount is a semialgebraic function of the parameters, and in particular it converges when the discount parameter $\lambda$ approaches 1. Finally, we demonstrate a polynomial-time algorithm for computing the quantilal policy, showing that quantilization is not qualitatively harder than ordinary optimization.

***

\section{Background}

Quantilization (introduced in [Taylor 2015](https://intelligence.org/files/QuantilizersSaferAlternative.pdf)) is a method of dealing with "[Extremal Goodhart's Law](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy)". According to Extremal Goodhart, when we attempt to optimize a utility function $\Ut^*: \A \rightarrow \Reals$ by aggressively optimizing a *proxy* $\Ut: \A \rightarrow \Reals$, we are likely to land outside of the domain where the proxy is useful. Quantilization addresses this by assuming an unknown cost function $\Co: \A \rightarrow [0,\infty)$ whose expectation $\Ea{\zeta}{\Co}$ w.r.t. some reference probability measure $\zeta \in \Delta\A$ is bounded by 1. $\zeta$ can be thought of as defining the "domain" within which $\Ut$ is well-behaved (for example it can be the probability measure of choices made by *humans*). We can then seek to maximize $\Ea{}{\Ut}$ while constraining $\Ea{}{\Co}$ by a fixed bound $C_{\max}$:

$$\tilde{\xi}^* :\in \Argmax{\xi \in \Delta\A}\ACM{\Ea{\xi}{\Ut}}{\forall \Co: \A \rightarrow [0,\infty): \Ea{\zeta}{\Co} \leq 1 \implies \Ea{\xi}{\Co} \leq C_{\max}}$$

Alternatively, we can choose some parameter $\eta\in(0,\infty)$ and maximize the minimal guaranteed expectation of $\Ut-\eta \Co$:

$$\xi^* :\in \Argmax{\xi \in \Delta\A}\inf_{\Co:\A\rightarrow[0,\infty)}\ACM{\Ea{\xi}{\Ut-\eta \Co}}{\Ea{\zeta}{\Co} \leq 1}$$

These two formulations are almost equivalent since both amount to finding a strategy that is Pareto efficient w.r.t. to the two objectives $\Ea{}{\Ut}$ and $-\Ea{}{\Co}$. For $\tilde{\xi}^*$ the tradeoff is governed by the parameter $C_{\max}$ and for $\xi^*$ the tradeoff is governed by the parameter $\eta$. Indeed, it is easy to see that any $\xi^*$ is also optimal for the first criterion if we take $C_{\max}=\Ea{\xi^*}{\Co}$, and any $\tilde{\xi}^*$ is also optimal for the latter criterion for an appropriate choice of $\eta$ (it needs to be a subderivative of the Pareto frontier).

In the following, we will use as our starting point the *second* formulation, which can be thought of as a zero-sum game in which $\Ut-\eta \Co$ is the utility function of the agent whose strategy set is $\A$, and $\Co$ is chosen by the adversary. The quantilal strategy $\xi^*$ is the Nash equilibrium of the game.

This formulation seems natural if we take $\eta:=\Ea{\zeta}{\max\AP{\Ut-\Ut^*,0}}$ (a measure of how "optimistic" is $\Ut$ in the "domain" $\zeta$) and $\Co:=\eta^{-1}\max\AP{\Ut-\Ut^*,0}$. In particular, the quantilum value (the value of the game) is a lower bound on the expectation of $\Ut^*$. Note that all the Propositions we prove except Proposition 5 work for the first formulation as well, and modifying the proofs is straightforward.

In principle, this formalism can be applied to sequential interaction between an agent and an environment, if we replace $\A$ by the set of *policies*. However, if it is possible to make structural assumptions about $\Ut$ and $C$, we can do better. Taylor explores one such structural assumption, namely a sequence of independent games in which both $\Ut$ and $\Co$ are additive across the games. We consider a more general setting, namely that of a finite Markov decision process (MDP).

\section{Results}

A finite MDP is defined by a finite set of states $\St$, a finite set of actions $\A$, a transition kernel $\T: \St \times \A \K \St$ and a reward function $\R: \St \rightarrow \Reals$. To specify the utility function, we also need to fix a time discount function $\gamma \in \Delta\Nats$. This allows defining $\Ut: \St^\omega \rightarrow \Reals$ by

$$\Ut(x):=\Ea{n\sim\gamma}{\R\AP{x_n}}$$

We also fix an initial distribution over states $\zeta_0 \in \Delta \St$. In "classical" MDP theory, it is sufficient to consider a deterministic initial state $s_0 \in \St$, since the optimal policy doesn't depend on the initial state anyway. However, quantilization is different since the worst-case cost function depends on the initial conditions.

We now assume that the cost function $\Co: \St^\omega \rightarrow \Reals$ (or, the true utility function $\Ut^*$) has the same form. That is, there is some penalty function $\Pe: \St \rightarrow [0,\infty)$ s.t.

$$\Co(x) = \Ea{n\sim\gamma}{\Pe\AP{x_n}}$$

Denote $\St^*:=\bigsqcup_{n\in\Nats}\St^n$. Given a policy $\pi: \St^*\times\St \K \A$ (where the $\St^*$ factor represents the past history the factor $\St$ represents the current state), we define $\Hi{\pi} \in \Delta\St^\omega$ in the usual way (the distribution over histories resulting from $\pi$). Finally, we fix $\eta \in (0,\infty)$. We are now ready to define quantilization in this setting

\#Definition 1

$\pi^* : \St^*\times\St \K \A$ is said to be *quantilal relatively to reference policy $\sigma: \St^*\times\St \K A$* when

$$\pi^* :\in \Argmax{\pi: \St^*\times\St \K \A}\inf_{\Pe:\St\rightarrow[0,\infty)}\ACM{\Ea{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{\R\AP{x_n}-\eta\Pe\AP{x_n}}}{\Ea{\substack{x\sim\Hi{\sigma}\\n\sim\gamma}}{\Pe\AP{x_n}}\leq1}$$

We also define the *quantilum value* $\QV \in \Reals$ by

$$\QV := \sup_{\pi: \St^*\times\St \K \A}\inf_{\Pe:\St\rightarrow[0,\infty)}\ACM{\Ea{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{\R\AP{x_n}-\eta\Pe\AP{x_n}}}{\Ea{\substack{x\sim\Hi{\sigma}\\n\sim\gamma}}{\Pe\AP{x_n}}\leq1}$$

($\QV$ cannot be $-\infty$ since taking $\pi=\sigma$ yields a lower bound of $\Ea{\substack{x\sim\Hi{\sigma}\\n\sim\gamma}}{\R\AP{x_n}}-\eta$.)

***

In the original quantilization formalism, the quantilal strategy can be described more explicitly, as sampling according to the reference measure $\zeta$ from some top fraction of actions ranked by expected utility. Here, we don't have an analogous description, but we can in some sense evaluate the infimum over $\Pe$.

For any $\pi: \St^*\times\St \K \A$, define $\Z{\pi}\in\Delta\St$ by

$$\Z{\pi}(s):=\Pa{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{x_n=s}$$

For any $\mu,\nu\in\Delta\St$, the notation $\RD{\infty}{\mu}{\nu}$ signifies the Renyi divergence of order $\infty$:

$$\RD{\infty}{\mu}{\nu} := \ln \max_{\substack{s\in\St \\ \mu(s)> 0}}\frac{\mu(s)}{\nu(s)}$$

In general, $\RD{\infty}{\mu}{\nu} \in [0,\infty]$. 

\#Proposition 1

$\pi^* : \St^*\times\St \K \A$ is quantilal relatively to reference policy $\sigma: \St^*\times\St \K A$ if and only if

$$\pi^* \in \Argmax{\pi: \St^*\times\St \K \A}\AP{\Ea{\Z{\pi}}{\R}-\eta \exp{\RD{\infty}{\Z{\pi}}{\Z{\sigma}}}}$$

Also, we have

$$\QV = \sup_{\pi: \St^*\times\St \K \A}\AP{\Ea{\Z{\pi}}{\R}-\eta \exp{\RD{\infty}{\Z{\pi}}{\Z{\sigma}}}}$$

***

If the maximization in Proposition 1 was over arbitrary $\xi \in \Delta\St$ rather than $\xi$ of the form $\Z{\pi}$, we would get ordinary quantilization and sampling $\xi$ would be equivalent to sampling some top fraction of $\zeta:=\Z{\sigma}$. In general, the image of the $\Z$ operator is some closed convex set which is *not* the entire $\Delta\St$.  \Comment{Also, Proposition 1 shows that quantilality only depends on $\sigma$ via $\zeta$. In the following, we will say that $\pi$ is quantilal relatively to $\zeta\in\Delta\St$ when it satisfies the property given in Proposition 1 with $\Z{\sigma}$ replaced by $\zeta$.}

So far we considered arbitrary (non-stationary) policies. From classical MDP theory, we know that an *optimal* policy can always be chosen to be Markov:

\#Definition 2

$\pi: \St^* \times \St \K \A$ is said to be a *Markov* policy, when there is some $\pi_\text{M}: \Nats \times S \K \A$ s.t. $\pi(h,s)=\pi_\text{M}\AP{\Abs{h},s}$.

***

Note that a priori it might be unclear whether even a non-stationary quantilal policy. However, we have

\#Proposition 2

For any $\sigma: \St^* \times \St \K \A$, there exists a Markov policy which is quantilal relatively to $\sigma$.

***

Now assume that our time discount function is geometric, i.e. there exists $\lambda\in[0,1)$ s.t. $\gamma(n)=(1-\lambda)\lambda^n$. Then it is known than an *optimal* policy can be chosen to be stationary:

\#Definition 3

$\pi: \St^* \times \St \K \A$ is said to be a *stationary* policy, when there is some $\pi_\text{T}: S \K \A$ s.t. $\pi(h,s)=\pi_\text{T}\AP{s}$.

***

Once again, the situation for quantilal policies is analogous:

\#Proposition 3

If $\gamma$ is geometric, then for any $\sigma: \St^* \times \St \K \A$, , there exists a stationary policy which is quantilal relatively to $\sigma$.

***

What is *not* analogous is that an optimal policy can be chosen to be *deterministic* whereas, of course, this is not the case for quantilal policies.

It is known that the value of an optimal policy depends on the parameters as a piecewise rational function, and in particular it converges as $\lambda \rightarrow 1$ and has a Taylor expansion at $\lambda=1$. The quantilum value has the same property.

\#Proposition 4

$\QV$ is a piecewise rational continuous function of $\lambda$, $\eta$, the matrix elements of $\T$, the values of $\R$, the values of $\zeta_0$ and the values of $\Z\sigma$, with a final number of "pieces".

\#Corollary 1

$\QV$ converges as $\lambda \rightarrow 1$, holding all other parameters fixed. It is analytic in $\lambda$ for some interval $\AB{\lambda_0,1}$ and therefore can be described by a Taylor expansion around $\lambda=1$ inside this interval.

***

Note that for optimal policies, Proposition 4 holds for a simpler reason. Specifically, the optimal policy is piecewise constant (since it's deterministic) and there is a Blackwell policy i.e. a fixed policy which is optimal for any $\lambda$ sufficiently close to 1. And, it is easy to see that for a constant policy, the value is a rational function of $\lambda$. On the other hand, the quantilal policy is not guaranteed to be locally constant anywhere. Nevertheless the quantilum value is still piecewise rational.

Finally, we address the question of the computational complexity of quantilization. We prove the following

\#Proposition 5

Assume geometric time discount. Assume further that $\R(s)$, $\T\APM{t}{s,a}$, $\zeta_0(s)$, $\lambda$, $\Z{\sigma}(s)$ and $\eta$ are rational numbers. Then:

a. There is an algorithm for computing $\QV$ which runs in time polynomial in the size of the input $\R$, $\T$, $\zeta_0$, $\lambda$, $\Z\sigma$ and $\eta$. Also, if $\sigma$ is stationary and $\sigma\APM{t}{s,a}$ are rational, then $\Z{\sigma}(s)$ are also rational and can be computed from $\sigma$, $\T$, $\zeta_0$ and $\lambda$ in polynomial time.

b. Given an additional rational input parameter $\epsilon\in(0,1)$, there is an algorithm for computing a stationary policy which is an $\epsilon$-equilibrium in the zero-sum game associated with quantilization, which runs in time polynomial in the size of the input and $\ln\frac{1}{\epsilon}$.

***

The algorithm of Proposition 5b computes an $\epsilon$-equilibrium rather than an exact Nash equilibrium (= quantilal policy), which is (probably) inevitable since the exact quantilal policy is (probably) irrational.

\section{Future Directions}

To tease a little, here are some developments of this work that I'm planning:

* Apply quantilization to *reinforcement learning*, i.e. when we don't know the MDP in advance. In particular, I believe that the principles of quantilization can be used not only to deal with misspecified rewards, but also to deal with traps to some extent (assuming it a priori known that the reference policy has at most a small probability of falling into a trap). This has some philosophical implications on how *humans* avoid traps.

* Unify that formalism with [DRL](https://agentfoundations.org/item?id=1656). The role of the reference policy will be played by the advisor (thus the reference policy is not known in advance but is learned online). This means we can drop the sanity condition for the advisor, at the price of (i) having a regret bound defined w.r.t. some kind of *quantilum* value rather than optimal value (ii) having a term in the regret bound proportional to the (presumably small) rate of falling into traps when following the reference (advisor) policy. It should be possible to further develop that by unifying it with the ideas of [catastrophe DRL](https://agentfoundations.org/item?id=1715).

* Deal with more general environments, e.g. POMDPs and continuous state spaces.

\section{Proofs}

\#Proposition A.N1

$$\inf_{\Pe:\St\rightarrow[0,\infty)}\ACM{\Ea{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{\R\AP{x_n}-\eta\Pe\AP{x_n}}}{\Ea{\substack{x\sim\Hi{\sigma}\\n\sim\gamma}}{\Pe\AP{x_n}}\leq1}=\Ea{\Z{\pi}}{\R}-\eta \exp{\RD{\infty}{\Z{\pi}}{\Z{\sigma}}}$$

\#Proof of Proposition A.N1

The definition of $\Z$ implies that

$$\Ea{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{\R\AP{x_n}-\eta\Pe\AP{x_n}}=\Ea{\Z{\pi}}{\R-\eta P}$$

Also

$$\Ea{\substack{x\sim\Hi{\sigma}\\n\sim\gamma}}{\Pe\AP{x_n}}=\Ea{\Z{\sigma}}{P}$$

Observe that

$$\Ea{\Z{\pi}}{P} = \sum_{s\in\St}\Z{\pi}(s)P(s) \leq \max_{\substack{s \in \St \\ \Z{\pi}(s) > 0}}\frac{\Z{\pi}(s)}{\Z{\sigma}(s)}\cdot\sum_{s\in\St}\Z{\sigma}(s)P(s) = \exp{\RD{\infty}{Z(\pi)}{Z(\sigma)}} \cdot \Ea{\Z{\sigma}}{P}$$

It follows that for any $P$ that satisfies the constraint $\Ea{\substack{x\sim\Hi{\sigma}\\n\sim\gamma}}{\Pe\AP{x_n}} \leq 1$, we have $\Ea{\Z{\sigma}}{\Pe} \leq 1$ and therefore

$$\Ea{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{\R\AP{x_n}-\eta\Pe\AP{x_n}} = \Ea{\Z{\pi}}{\R} - \eta \Ea{\Z{\pi}}{\Pe} \geq \Ea{\Z{\pi}}{\R} - \eta\exp{\RD{\infty}{Z(\pi)}{Z(\sigma)}}$$

To show that the inequality can be arbitrarily close to equality, choose $s^* \in \St$ s.t. $\Z{\pi}(s^*) > 0$ and $\frac{\Z{\pi}\AP{s^*}}{\Z{\sigma}\AP{s^*}} = \exp{\RD{\infty}{Z(\pi)}{Z(\sigma)}}$. If $\Z{\sigma}\AP{s^*} > 0$, we can define $\Pe^*$ by

$$\Pe^*(s):=\begin{cases} \Z{\sigma}(s^*)^{-1} \text{ if } s=s^* \\ 0 \text{ otherwise} \end{cases}$$

Clearly $\Ea{\Z{\sigma}}{\Pe^*} = 1$ and $\Ea{\Z{\pi}}{P^*} = \exp{\RD{\infty}{Z(\pi)}{Z(\sigma)}}$. We get

$$\Ea{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{\R\AP{x_n}-\eta\Pe^*\AP{x_n}} = \Ea{\Z{\pi}}{\R} - \eta \Ea{\Z{\pi}}{\Pe^*} = \Ea{\Z{\pi}}{\R} - \eta\exp{\RD{\infty}{Z(\pi)}{Z(\sigma)}}$$

In the case $\Z{\sigma}\AP{s^*} > 0$, we can take any $M > 0$ and define $\Pe^M$ by

$$\Pe^M(s):=\begin{cases} M \text{ if } s=s^* \\ 0 \text{ otherwise} \end{cases}$$

Clearly $\Ea{\Z{\sigma}}{\Pe^M} = 0 \leq 1$ and $\Ea{\Z{\pi}}{P^M} = M \cdot \Z{\pi}\AP{s^*}$. We get

$$\Ea{\substack{x\sim\Hi{\pi}\\n\sim\gamma}}{\R\AP{x_n}-\eta\Pe^M\AP{x_n}} = \Ea{\Z{\pi}}{\R} - \eta \Ea{\Z{\pi}}{\Pe^M} = \Ea{\Z{\pi}}{\R} - M \cdot \Z{\pi}\AP{s^*}$$

Since $\Z{\pi}\AP{s^*} > 0$, we can make this expression arbitrarily low and therefore the infimum is $-\infty$ which is what we need since in this case $\RD{\infty}{\Z{\pi}}{\Z{\sigma}} = \infty$.

***

Proposition 1 now follows immediately from Proposition A.N1.

We will use the notation $\Pi:=\AC{\St^* \times \St \K \A}$ (the space of all policies). We also have $\Pi_{\text{M}}:=\AC{\Nats \times \St \K \A}$ (the space of Markov policies) and $\Pi_{\text{T}}:=\AC{ \St \K \A}$ (the space of stationary policies). Mildly abusing notation, we will view $\Pi_{\text{M}}$ as a subspace of $\Pi$ and $\Pi_{\text{T}}$ as a subspace of $\Pi_{\text{M}}$.

\#Proposition A.N2

For any $\sigma: \St^* \times \St \K \A$, there exists *some* policy which is quantilal relatively to $\sigma$.

\#Proof of Proposition A.N2

$\Pi$ is the product of a countable number of copies of $\Delta\A$ (indexed by $\St^* \times \St$). $\Delta\A$ is naturally a topological space (a simplex), and we can thereby equip $\Pi$ by the product topology. By Tychonoff's theorem, $\Pi$ is compact. Moreover, it is easy to see that $\Z: \Pi \rightarrow \Delta\St$ is a continuous mapping. Finally, observe that $\RD{\infty}{\xi}{\Z{\sigma}}$ is lower semicontinuous in $\xi$ (since it is the maximum of a number of continuous functions) and therefore $\Ea{\xi}{\R} - \eta\exp{\RD{\infty}{\xi}{\Z{\sigma}}}$ is upper semicontinuous in $\xi$. By the extreme value theorem, it follows that a quantilal policy exists.

***

For any $n\in\Nats$, we define $\Z_n: \Pi \K \St$ by 

$$\Z_n{\pi}(s) := \Pa{x\sim\Hi{\pi}}{x_n=s}$$

\#Proof of Proposition 2

By Proposition A.N2, there is a quantilal policy $\pi^*: \St^* \times \St \K \A$. We define $\pi^\dagger: \Nats \times \St \K \A$ by

$$\pi^\dagger(n,s):=\CE{x\sim\Hi{\pi^*}}{\pi^*(x_{:n},s)}{x_n=s}$$

We now prove by induction that for any $n\in\Nats$, $\Z_n{\pi^*} = \Z_n{\pi^\dagger}$.

For $n=0$, we have $\Z_0{\pi^*} = \Z_0{\pi^\dagger} = \zeta_0$.

For any $n \in \Nats$ and any $\pi: \St^* \times \St \K \A$, we have

$$\Z_{n+1}{\pi}(t) = \Pa{x\sim\Hi{\pi}}{x_{n+1}=t} = \Ea{s\sim\Z_n{\pi}}{\CP{x\sim\Hi{\pi}}{x_{n+1} = t}{x_n=s}} = \Ea{s\sim\Z_n{\pi}}{\CE{\substack{x\sim\Hi{\pi} \\ a\sim\pi\AP{x_{:n},s}}}{\T\APM{t}{s,a}}{x_n=s}}$$

To complete the induction step, observe that, by definition of $\pi^\dagger$


$$\CE{\substack{x\sim\Hi{\pi} \\ a\sim\pi^*\AP{x_{:n},s}}}{\T\APM{t}{s,a}}{x_n=s} = \Ea{a\sim\pi^\dagger\AP{n,s}}{\T\APM{t}{s,a}}=\CE{\substack{x\sim\Hi{\pi} \\ a\sim\pi^\dagger\AP{n,s}}}{\T\APM{t}{s,a}}{x_n=s}$$

Now, for any $\pi$, $\Z{\pi}=\Ea{n\sim\gamma}{\Z_n{\pi}}$ and therefore $\Z{\pi^\dagger} = \Z{\pi^*}$. We conclude that $\pi^\dagger$ is also quantilal.

\#Proof of Proposition 3

By Proposition 2, there is $\pi^*: \Nats \times \St \K \A$ which is a Markov quantilal policy. We have

$$\Z_{n+1}{\pi^*}(t) = \Pa{x\sim\Hi{\pi^*}}{x_{n+1}=t} = \Ea{s\sim\Z_n{\pi^*}}{\CP{x\sim\Hi{\pi^*}}{x_{n+1} = t}{x_n=s}} = \Ea{\substack{s\sim\Z_n{\pi^*}\\a\sim\pi^*\AP{n,s}}}{\T\APM{t}{s,a}}$$

Taking the expected value of this equation w.r.t. $n\sim\gamma$ we get

$$\Ea{n\sim\gamma}{\Z_{n+1}{\pi^*}} = \Ea{\substack{n\sim\gamma\\s\sim\Z_n{\pi^*}\\a\sim\pi^*\AP{n,s}}}{\T(s,a)}$$

Also, we have

$$\Z\pi^* = \Ea{n\sim\gamma}{\Z_n \pi^*} = (1-\lambda)\sum_{n=0}^\infty \lambda^n \Z_n \pi^* = (1-\lambda)\AP{\zeta_0 + \lambda \sum_{n=0}^\infty \lambda^n \Z_{n+1}\pi^*}=(1-\lambda)\zeta_0 + \lambda \Ea{n\sim\gamma}{\Z_{n+1}{\pi^*}}$$

It follows that

$$\Z\pi^* = (1-\lambda)\zeta_0 + \lambda \Ea{\substack{n\sim\gamma\\s\sim\Z_n{\pi^*}\\a\sim\pi^*\AP{n,s}}}{\T(s,a)}$$

$$\Z\pi^* = (1-\lambda)\zeta_0 + \lambda \Ea{s\sim\Z\pi^*}{\CE{\substack{n\sim\gamma\\t\sim\Z_n\pi^*\\a\sim\pi^*(n,s)}}{\T(s,a)}{t=s}}$$

Define $\pi^\dagger: \St \K \A$ by

$$\pi^\dagger(s) := \CE{\substack{n\sim\gamma\\t\sim\Z_n\pi^*}}{\pi^*(n,s)}{t=s}$$

We get

$$\Z\pi^* = (1-\lambda)\zeta_0 + \lambda \Ea{s\sim\Z\pi^*}{\Ea{a\sim\pi^\dagger(s)}{\T(s,a)}}$$

Define the linear operator $T^\dagger: \Reals^\St \rightarrow \Reals^\St$ by the matrix 

$$T^\dagger_{ts} := \Ea{a\sim\pi^\dagger(s)}{\T\APM{t}{s,a}}$$

Viewing $\Delta\St$ as a subset of $\Reals^\St$, we get

$$\Z\pi^* = (1-\lambda)\zeta_0 + \lambda T^\dagger\Z\pi^*$$

$$\Z\pi^* = (1-\lambda)\AP{\boldsymbol{1}-\lambda T^\dagger}^{-1}\zeta_0$$

On the other hand, we have

$$\Z\pi^\dagger = (1-\lambda)\sum_{n=0}^\infty \lambda^n T^{\dagger n} \zeta_0 = (1-\lambda)\AP{\boldsymbol{1}-\lambda T^\dagger}^{-1}\zeta_0$$

Therefore, $\Z\pi^\dagger = \Z\pi^*$ and $\pi^\dagger$ is also quantilal.

\#Proposition A.N3

Assume geometric time discount. Define the linear operators $I: \Reals^\St \rightarrow \Reals^{\St\times\A}$ and $T: \Reals^\St \rightarrow \Reals^{\St\times\A}$ by the matrices

$$I_{t,sa}:=[[t=s]]$$

$$T_{t,sa}:=\T\APM{t}{s,a}$$

Define the linear operator $A: \Reals^\St \oplus \Reals^\St \rightarrow \Reals^{\St\times\A}\oplus\Reals^{\St}\oplus\Reals$ by

$$A(V,P):=\AP{\AP{I-\lambda T}V + \eta(1-\lambda)IP,\ P,\ -\Ea{\Z\sigma}{P}}$$

Define $B\in \Reals^{\St\times\A}\oplus\Reals^{\St}\oplus\Reals$ by

$$B:=\AP{(1-\lambda)I\R,\ \boldsymbol{0},\ -1}$$

Define $D \subseteq \Reals^\St \oplus \Reals^\St$ as follows

$$D:=\ACM{X \in \Reals^\St \oplus \Reals^\St}{AX \geq B}$$

Here, vector inequalities are understood to be componentwise.

Then,

$$\QV = \min_{(V,P)\in D}{\Ea{\zeta_0}{V}}$$

\#Proof of Proposition A.N3

Denote $\Pi_{\det}:=\AC{\St^* \times \St \rightarrow \A}$. As usual in extensive-form games, behavioral strategies are equivalent to mixed strategies and therefore the image of the pushforward operator $\Z_*: \Delta\Pi_{\det} \rightarrow \Delta\St$ is the same as the image of $\Z: \Pi \rightarrow \Delta\St$. It follows that

$$\QV = \sup_{\mu \in \Delta\Pi_{\det}}\inf_{\Pe:\St\rightarrow[0,\infty)}\ACM{\Ea{\Z_*\mu}{\R-\eta\Pe}}{\Ea{\Z\sigma}{\Pe}\leq1}$$

$\Pi_{\det}$ is a compact Polish space (in the sense of the product topology) and therefore $\Delta\Pi_{\det}$ is compact in the weak topology. By Sion's minimax theorem

$$\QV = \inf_{\Pe:\St\rightarrow[0,\infty)}\max_{\pi \in \Pi_{\det}}\ACM{\Ea{\Z\pi}{\R-\eta\Pe}}{\Ea{\Z\sigma}{\Pe}\leq1}$$

Now consider any $X=(V,P) \in D$. $AX \geq B$ implies (looking at the $\Reals^{\St\times\A}$ component) that

$$\AP{I-\lambda T}V + \eta(1-\lambda)IP \geq (1-\lambda)I\R$$

$$\AP{I-\lambda T}V\geq (1-\lambda)I(\R-\eta P)$$

$$V(t) - \lambda \sum_{s \in \St} \T\APM{t}{s,a} V(s) \geq (1-\lambda)\AP{\R(t)-\eta P(t)}$$

$$V(t) \geq (1-\lambda)\AP{\R(t)-\eta P(t)} + \lambda \max_{a \in \A } \sum_{s \in \St} \T\APM{t}{s,a} V(s)$$

Therefore, there is some $\R' \geq \R$ s.t.

$$V(t) = (1-\lambda)\AP{\R'(t)-\eta P(t)} + \lambda \max_{a \in \A } \sum_{s \in \St} \T\APM{t}{s,a} V(s)$$

The above is the Bellman equation for a modified MDP with reward function $\R'-\eta P$. Denote $\Z_t$ the version of the $\Z$ operator for the initial state $t$ (instead of $\zeta_0$). We get

$$V(t) = \max_{\pi\in\Pi_{\det}}{\Ea{\Z_t\pi}{\R'-\eta P}} \geq \max_{\pi\in\Pi_{\det}}{\Ea{\Z_t\pi}{\R-\eta P}}$$

Observing that $\Ea{t\sim\zeta_0}{\Z_t\pi} = \Z\pi$, we get

$$\Ea{\zeta_0}{V} \geq \Ea{t\sim\zeta_0}{\max_{\pi\in\Pi_{\det}}{\Ea{\Z_t\pi}{\R-\eta P}}} \geq \max_{\pi\in\Pi_{\det}} \Ea{\Z\pi}{\R - \eta P}$$

On the other hand, for any $P\in\Reals^\St$ s.t. $P\geq\boldsymbol{0}$ and $\Ea{\Z\sigma}{P} \leq 1$ (these inequalities correspond to the $\Reals^\St\oplus\Reals$ component of $AX \geq B$), there is $(V,P)\in D$ s.t.

$$\Ea{\zeta_0}{V} = \max_{\pi\in\Pi_{\det}} \Ea{\Z\pi}{\R - \eta P}$$

Namely, this $V$ is the solution of the Bellman equation for the reward function $\R-\eta P$. Therefore, we have

$$\max_{\pi\in\Pi_{\det}} \Ea{\Z\pi}{\R - \eta P} = \min_{V\in\Reals^\St}\ACM{\Ea{\zeta_0}{V}}{(V,P)\in D}$$

Minimizing both sides over $P$ inside the domain $\AC{P\geq\boldsymbol{0},\ \Ea{\Z\sigma}{P} \leq 1}$ we get

$$\QV = \min_{\Pe:\St\rightarrow[0,\infty)}\max_{\pi \in \Pi_{\det}}\ACM{\Ea{\Z\pi}{\R-\eta\Pe}}{\Ea{\Z\sigma}{\Pe}\leq1} = \min_{(V,P) \in D}{\Ea{\zeta_0}{V}}$$

\#Proof of Proposition 4

Using elementary linear algebra, it is easy to see that the minimum of a linear function inside a domain defined by a finite system of linear inequalities is a continuous piecewise rational function of the coefficients of the linear function and the linear inequalities, with a finite number of pieces (within the domain in which the minimum exists; using the fact that the minimum is always attained on a linear submanifold defined by taking a subset of the inequalities and making them into equalities). By Proposition A.N3, $\QV$ is exactly of this form, with coefficients that are polynomials in the parameters we consider.

\#Proof of Proposition 5

The algorithm for $\QV$ is obtained from Proposition A.N3 using linear programming. 

To see the claim regarding $\Z\sigma$ for stationary $\sigma$, define the linear operator $T^\sigma: \Reals^\St \rightarrow \Reals^\St$ by the matrix

$$T^\sigma_{ts} = \Ea{a\sim\sigma(s)}{\T\APM{t}{s,a}}$$

We then have

$$\Z\sigma = (1-\lambda)\sum_{n=0}^\infty \lambda^n T^{\sigma n} \zeta_0 = (1-\lambda)\AP{\boldsymbol{1}-\lambda T^\sigma}^{-1}\zeta_0$$

Mwhahaha

\end{document}



