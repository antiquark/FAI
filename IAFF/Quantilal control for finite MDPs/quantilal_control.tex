%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Dom}{dom}

% autosize delimiters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}
\newcommand{\AC}[1]{\left\{#1\right\}}
\newcommand{\APM}[2]{\left(#1\;\middle\vert\;#2\right)}
\newcommand{\ABM}[2]{\left[#1\;\middle\vert\;#2\right]}
\newcommand{\ACM}[2]{\left\{#1\;\middle\vert\;#2\right\}}

% operators that require brackets
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\CP}[3]{\underset{#1}{\operatorname{Pr}}\ABM{#2}{#3}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\CE}[3]{\underset{#1}{\operatorname{E}}\ABM{#2}{#3}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\CI}[3]{\underset{#1}{\operatorname{I}}\ABM{#2}{#3}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}

% operators that require parentheses
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
\newcommand{\PS}[1]{\mathcal{P}\AP{#1}}

\newcommand{\D}{\mathrm{d}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\textnormal{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\textnormal{tv}}\AP{#1}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\Floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\Chev}[1]{\left\langle #1 \right\rangle}
\newcommand{\Quote}[1]{\left\ulcorner #1 \right\urcorner}

\newcommand{\K}{\xrightarrow{\textnormal{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}

% Paper specific

\newcommand{\A}{\mathcal{A}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Pe}{P}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\Co}{C}
\newcommand{\V}{\operatorname{V}}

\begin{document}

We introduce a variant of the concept of a "[quantilizer](https://intelligence.org/files/QuantilizersSaferAlternative.pdf)" for the setting of choosing a policy for a finite Markov decision process (MDP), where the generic unknown cost is replaced by an unknown penalty term in the reward function. This is essentially a generalization of quantilization in repeated games with a cost independence assumption. We show that the "quantilal" policy shares some properties with the ordinary optimal policy, namely that (i) it can always be chosen to be Markov (ii) it can be chosen to be stationary (but not deterministic of course) when time discount is geometric (iii) the "quantilum" value of an MDP with geometric time discount is a semialgebraic function of the parameters, and in particular it converges when the discount parameter $\lambda$ approaches 1. Finally, we demonstrate a polynomial-time algorithm for computing the quantilal policy, showing that quantilization is not qualitatively harder than ordinary optimization.

***

\section{Results}

Quantilization (introduced in [Taylor 2015](https://intelligence.org/files/QuantilizersSaferAlternative.pdf)) is a method of dealing with "[Extremal Goodhart's Law](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy)". That is, when we attempt to optimize a utility function $\Ut^*: \A \rightarrow \Reals$ by aggressively optimizing a *proxy* $\Ut: \A \rightarrow \Reals$, we are likely to land outside of the domain where the proxy is useful. Quantilization addresses this by assuming an unknown cost function $\Co: \A \rightarrow [0,\infty)$ whose expectation $\Ea{\zeta}{\Co}$ w.r.t. some reference probability measure $\zeta \in \Delta\A$ is bounded by 1. $\zeta$ can be thought of as defining the "domain" within which $\Ut$ is well-behaved (for example it can be the probability measure of choices made by *humans*). We can then seek to maximize $\Ea{}{\Ut}$ while constraining $\Ea{}{\Co}$ by a fixed bound $C_{\max}$:

$$\tilde{\xi}^* := \Argmax{\xi \in \Delta\A}\ACM{\Ea{\xi}{\Ut}}{\forall \Co: \A \rightarrow [0,\infty): \Ea{\zeta}{\Co} \leq 1 \implies \Ea{\xi}{\Co} \leq C_{\max}}$$

Alternatively, we can choose some parameter $\eta\in(0,\infty)$ and maximize the minimal guaranteed expectation of $\Ut-\eta \Co$:

$$\xi^* := \Argmax{\xi \in \Delta\A}\min_{\Co:\A\rightarrow[0,\infty)}\ACM{\Ea{\xi}{\Ut-\eta \Co}}{\Ea{\zeta}{\Co} \leq 1}$$

These two formulations are almost equivalent since both amount to finding a strategy that is Pareto efficient w.r.t. to the two objectives $\Ea{}{\Ut}$ and $-\Ea{}{\Co}$. For $\tilde{\xi}^*$ the tradeoff is governed by the parameter $C_{\max}$ and for $\xi^*$ the tradeoff is governed by the parameter $\eta$. Indeed it is easy to see that any $\xi^*$ is also optimal for the first criterion if we take $C_{\max}=\Ea{\xi^*}{\Co}$, and any $\tilde{\xi}^*$ is also optimal for the latter criterion for an appropriate choice of $\eta$ (it needs to be a subderivative of the Pareto frontier).

In the following, we will stick to the second formulation, which can be thought of as a zero-sum game in which $\Ut-\eta \Co$ is the utility function of the agent whose strategy set is $\A$, and $\Co$ is chosen by the adversary. The quantilal strategy $\xi^*$ is the Nash equilibrium of the game.

This formulation seems natural if we take $\eta:=\Ea{\zeta}{\max\AP{\Ut-\Ut^*,0}}$ and $\Co:=\eta^{-1}\max\AP{\Ut-\Ut^*,0}$. In particular, the quantilum value (the value of the game) is a lower bound on the expectation of $\Ut^*$. Note that all the Propositions we prove except TBD work for the first formulation as well, and modifying the proofs is straightforward.

In principle, this formalism can be applied to sequential interaction between an agent and an environment, if we replace $\A$ by the set of *policies*. However, if it is possible to make structural assumptions about $\Ut$ and $C$, we can do better. Taylor explores one such structural assumption, namely a sequence of independent games in which both $\Ut$ and $\Co$ are additive across the games. We consider a more general setting, namely that of a finite Markov decision process (MDP).

A finite MDP is defined by a finite set of states $\St$, a finite set of actions $\A$, a transition kernel $\T: \St \times \A \K \St$ and a reward function $\R: \St \rightarrow \Reals$. To specify the utility function, we also need to fix a time discount function $\gamma \in \Delta\Nats$. This allows defining $\Ut: \St^\omega \rightarrow \Reals$ by

$$\Ut(x):=\Ea{n\sim\gamma}{\R\AP{x_n}}$$

We also fix an initial distribution over states $\zeta_0 \in \Delta \St$. In "classical" control theory, it is sufficient to consider a deterministic initial state $s_0 \in \St$, since the optimal policy doesn't depend on the initial state anyway. However, quantilization is different since the worst-case cost function depends on the initial conditions.

We now assume that the cost function $\Co: \St^\omega \rightarrow \Reals$ (or, the true utility function $\Ut^*$) has the same form. That is, there is some penalty function $\Pe: \St \rightarrow [0,\infty)$ s.t.

$$\Co(x) = \Ea{n\sim\gamma}{\Pe\AP{x_n}}$$

\#Definition 1

Bar

\#Lemma 1

Foo

***

Bar

\section{Proofs}

\#Proposition A.1

Hahaha

\#Proof of Proposition A.1

Mwhahaha

\end{document}



