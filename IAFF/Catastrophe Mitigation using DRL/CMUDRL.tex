%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{dom}

% autosize deliminaters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}

% operators that require brackets
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\Var}{\operatorname{Var}}

% operators that require parentheses
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Sym}{\operatorname{Sym}}

\newcommand{\Prj}{\operatorname{pr}}

\newcommand{\D}{\mathrm{d}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\text{tv}}\AP{#1}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\In}{\mathcal{I}}
\newcommand{\FH}{(\A \times \Ob)^*}
\newcommand{\IH}{(\A \times \Ob)^\omega}
\newcommand{\Ado}{\bar{\Ob}}
\newcommand{\Ada}{\bar{\A}}
\newcommand{\Adi}{{\bar{\In}}}
\newcommand{\Adao}{\overline{\A \times \Ob}}
\newcommand{\Adfh}{\Adao^*}
\newcommand{\Adih}{\Adao^\omega}
\DeclareMathOperator{\HD}{hdom}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\UC}{\mathcal{U}}
\newcommand{\SF}{\St^{\text{F}}}
\newcommand{\SD}{\St^{\text{D}}}
\newcommand{\SC}{\St^{\text{C}}}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\V}{\operatorname{V}}
\newcommand{\Q}{\operatorname{Q}}
\newcommand{\EU}{\operatorname{EU}}

\newcommand{\Dl}{\mathcal{D}}
\newcommand{\Do}{\mathfrak{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Z}{Z}
\newcommand{\J}{J}

\begin{document}

[Previously](https://agentfoundations.org/item?id=1656) we derived a regret bound for DRL which assumed the advisor is "locally sane." Such an advisor can only take actions that don't lose any value in the long term. In particular, if the environment contains a latent *catastrophe* that manifests with a certain rate (such as the possibility of an UFAI), a locally sane advisor has to take the optimal course of action to mitigate it, since every delay yields a positive probability of the catastrophe manifesting and leading to permanent loss of value. This state of affairs is unsatisfactory, since we would like to have performance guarantees for an AI that can mitigate catastrophes that the human operator cannot mitigate on their own. To address this problem, we introduce a new form of DRL where in every hypothetical environment the set of uncorrupted states is divided into "dangerous" (impending catastrophe) and "safe" (catastrophe was mitigated). The advisor is then only required to be locally sane in safe states, whereas in dangerous states certain "leaking" of long-term value is allowed. We derive a regret bound in this setting as a function of the time discount factor, the expected value and standard deviation of the catastrophe mitigation time for the optimal policy, and the "value leak" rate (i.e. essentially the rate of catastrophe occurrence). The form of this regret bound implies that in certain asymptotic regimes, the agent attains near-optimal expected utility (and in particular mitigates the catastrophe with probability close to 1), whereas the advisor on its own *fails* to mitigate the catastrophe with probability close to 1. Thus, this formalism can be regarded as a simple model of aligned superintelligence: the agent is *aligned* since near-optimal utility is achieved despite the presence of corrupted states (which are treated more or less the same as before) and it is *super*intelligent since its performance is vastly better than the performance of the advisor.

***

Appendix A contains the proofs...

\section{Notation}

Whatever...

\section{Results}

We start by formalising the concept of a "mitigable catastrophe" in the language of MDPs.

\#Definition 1

Fix $\delta\in(0,1)$ and $\tau_1,\tau_2 \in (0,\infty)$. A *catastrophe MDP $M$ with $\delta$-mitigation policy $\pi^1: \St_M \M \A_M$ and moments $\tau_1,\tau_2$* is an MDP together with a partition of $\St_M$ into subsets $\St_M:=\SF_M \sqcup \SD_M \sqcup \SC_M$ (safe, dangerous and corrupt states respectively) and an initial state $s_0 \in \SD_M$ s.t.

i. $\pi^1$ is 1-optimal.

ii. For any $s \in \SF_M$, $\T_{M\pi^1}\AP{\SF_M \mid s}=1$.

iii. For any $s \in \SD_M$, $\T_{M\pi^1}\AP{\SC_M \mid s} < \delta$.

iv. $$\sum_{n=0}^\infty n \AP{\T_{M\pi^1}^{n+1}\AP{\SF_M \mid s_0}-\T_{M\pi^1}^{n}\AP{\SF_M \mid s_0}} \leq \tau_1$$

v. $$\sum_{n=0}^\infty n^2 \AP{\T_{M\pi^1}^{n+1}\AP{\SF_M \mid s_0}-\T_{M\pi^1}^{n}\AP{\SF_M \mid s_0}} \leq \tau_2$$

***

Next, we introduce the notion of an MDP perturbation. We will use it by considering perturbations of a catastrophe MDP which "eliminate the catastrophe."

\#Definition 2

Fix $\delta\in(0,1)$ and consider a catastrophe MDP $M$ (in general, it doesn't have to be the same $\delta$ as in Definition 1). An MDP $\tilde{M}$ is said to be a *$\delta$-perturbation of $M$* when

i. $\St_{\tilde{M}} = \St_M$

ii. $\A_{\tilde{M}} = \A_M$

iii. $\R_{\tilde{M}}=\R_M$

iv. For any $s \in \SF_M$ and $a \in \A_M$, $\T_{\tilde{M}}\AP{s,a}=\T_{M}\AP{s,a}$

v. For any $s \in \SD_M$ and $a \in \A_M$, $\Dtva{\T_{\tilde{M}}\AP{s,a},\T_{M}\AP{s,a}} < \delta$

***

Similarly, we can consider perturbations of a policy.

\#Definition 3

Fix $\delta\in(0,1)$ and consider a catastrophe MDP $M$. Given $\pi: \St_M \M \A_M$ and $\tilde{\pi}: \St_M \M \A_M$, $\tilde{\pi}$ is said to be a *$\delta$-perturbation of $\pi$* when

i. For any $s \in \SF_M$, $\tilde{\pi}(s) = \pi(s)$.

ii. For any $s \in \SD_M$, $\Dtva{\tilde{\pi}(s),\pi(s)} < \delta$.

***

We will also need to introduce policy-specific value functions and Q-functions.

\#Definition 4

Fix an MDP $M$ and $\pi: \St_M \M \A_M$. We define $\V_{M\pi}: \St_M \times (0,1) \rightarrow [0,1]$ and $\Q_{M\pi}: \St_M \times \A_M \times (0,1) \rightarrow [0,1]$ by

$$\V_{M\pi}(s,\gamma) := (1-\gamma) \sum_{n=0}^\infty \gamma^n \Ea{\T_{M\pi}^n(s)}{\R_M}$$

$$\Q_{M\pi}(s,a,\gamma) := (1-\gamma) \R_M(s) + \gamma \Ea{t \sim \T_{M\pi}(s)}{\V_{M\pi}(t,\gamma)}$$

For each $k \in \Nats$, we also define $\V_{M\pi}^k: \St_M \rightarrow [0,1]$ and $\Q_{M\pi}^k: \St_M \times \A_M \rightarrow [0,1]$ by

$$\V_{M\pi}^k(s) := \frac{\D^k \V_{M\pi}(s,\gamma)}{\D\gamma^k}\bigg\vert_{\gamma=1}$$

$$\Q_{M\pi}^k(s,a) := \frac{\D^k \Q_{M\pi}(s,a,\gamma)}{\D\gamma^k}\bigg\vert_{\gamma=1}$$

***

Now we give the new (weaker) condition on the advisor policy. For notational simplicity, we assume the policy is stationary. It is easy to generalize these results to non-stationary advisor policies and to policies that depend on irrelevant additional information (i.e. policies for universes that are *realizations* of the MDP).

\#Definition 5

Fix $\epsilon,\delta \in (0,1)$. Consider a catastrophe MDP $M$ with $\delta$-mitigation policy $\pi^1: \St_M \M \A_M$. A policy $\pi: \St_M \M \A_M$ is said to be *locally $(\epsilon,\delta)$-sane* when there exists a $\delta$-perturbation $\tilde{M}$ of $M$ and a $\delta$-perturbation $\tilde{\pi}$ of $\pi$ s.t.

i. For any $s \in \SF_M$, $\T_{M\pi}\AP{\SF_M \mid s} = 1$.

ii. For any $s \in \SD_M$, $\T_{\tilde{M}\tilde{\pi}}\AP{\SC_M \mid s} = 0$.

iii. For any $s \in \St_M \setminus \SC_M$, $\Ea{a \sim \tilde{\pi}(s)}{\Q_{\tilde{M}\pi^1}^0\AP{s,a}} \geq \V_{\tilde{M}\pi^1}^0\AP{s}$

iv. if $s \in \St_M \setminus \SC_M$ is s.t. $\Ea{a \sim \tilde{\pi}(s)}{\Q_{\tilde{M}\pi^1}^0\AP{s,a}} = \V_{\tilde{M}\pi^1}^0\AP{s}$ then there is $a \in \A_M$ s.t. $\Q_{\tilde{M}\pi^1}^1\AP{s,a} \geq \V_{\tilde{M}\pi^1}^1\AP{s}$ and $\tilde{\pi}(a \mid s) > \epsilon$???

v. Condition about 2nd order??

If $M$ is an arbitrary MDP...

\#Lemma 1

Foo

***

Bar

\section{Appendix A}

\#Proposition A.1

Hahaha

\#Proof of Proposition A.1

Mwhahaha

\end{document}



