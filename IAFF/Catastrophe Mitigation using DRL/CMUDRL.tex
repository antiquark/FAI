%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{dom}

% autosize deliminaters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}

% operators that require brackets
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\Var}{\operatorname{Var}}

% operators that require parentheses
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Sym}{\operatorname{Sym}}

\newcommand{\Prj}{\operatorname{pr}}

\newcommand{\D}{\mathrm{d}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\text{tv}}\AP{#1}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\In}{\mathcal{I}}
\newcommand{\FH}{(\A \times \Ob)^*}
\newcommand{\IH}{(\A \times \Ob)^\omega}
\newcommand{\Ado}{\bar{\Ob}}
\newcommand{\Ada}{\bar{\A}}
\newcommand{\Adi}{{\bar{\In}}}
\newcommand{\Adao}{\overline{\A \times \Ob}}
\newcommand{\Adfh}{\Adao^*}
\newcommand{\Adih}{\Adao^\omega}
\DeclareMathOperator{\HD}{hdom}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\UC}{\mathcal{U}}
\newcommand{\SF}{\St^{\text{F}}}
\newcommand{\SD}{\St^{\text{D}}}
\newcommand{\SC}{\St^{\text{C}}}
\newcommand{\MF}{M^{\text{F}}}
\newcommand{\TF}{\tau^{\text{F}}}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\V}{\operatorname{V}}
\newcommand{\Q}{\operatorname{Q}}
\newcommand{\EU}{\operatorname{EU}}

\newcommand{\Dl}{\mathcal{D}}
\newcommand{\Do}{\mathfrak{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Z}{Z}
\newcommand{\J}{J}

\begin{document}

[Previously](https://agentfoundations.org/item?id=1656) we derived a regret bound for DRL which assumed the advisor is "locally sane." Such an advisor can only take actions that don't lose any value in the long term. In particular, if the environment contains a latent *catastrophe* that manifests with a certain rate (such as the possibility of an UFAI), a locally sane advisor has to take the optimal course of action to mitigate it, since every delay yields a positive probability of the catastrophe manifesting and leading to permanent loss of value. This state of affairs is unsatisfactory, since we would like to have performance guarantees for an AI that can mitigate catastrophes that the human operator cannot mitigate on their own. To address this problem, we introduce a new form of DRL where in every hypothetical environment the set of uncorrupted states is divided into "dangerous" (impending catastrophe) and "safe" (catastrophe was mitigated). The advisor is then only required to be locally sane in safe states, whereas in dangerous states certain "leaking" of long-term value is allowed. We derive a regret bound in this setting as a function of the time discount factor, the expected value and standard deviation of the catastrophe mitigation time for the optimal policy, and the "value leak" rate (i.e. essentially the rate of catastrophe occurrence). The form of this regret bound implies that in certain asymptotic regimes, the agent attains near-optimal expected utility (and in particular mitigates the catastrophe with probability close to 1), whereas the advisor on its own *fails* to mitigate the catastrophe with probability close to 1. Thus, this formalism can be regarded as a simple model of aligned superintelligence: the agent is *aligned* since near-optimal utility is achieved despite the presence of corrupted states (which are treated more or less the same as before) and it is *super*intelligent since its performance is vastly better than the performance of the advisor.

***

Appendix A contains the proofs...

\section{Notation}

Whatever...

\section{Results}

We start by formalising the concepts of a "catastrophe" and "catastrophe mitigation" in the language of MDPs.

\#Definition 1

Fix $\delta \in (0,1)$. A *$\delta$-catastrophe MDP* is an MDP $M$ together with a partition of $\St_M$ into subsets $\St_M:=\SF_M \sqcup \SD_M \sqcup \SC_M$ (safe, dangerous and corrupt states respectively) s.t. there is some $\pi^2: \St_M \rightarrow \A_M$ with the properties

i. $\pi^2$ is 2-optimal.

ii. For any $s \in \SF_M$, $\T_{M\pi^2}\AP{\SF_M \mid s}=1$.

iii. For any $s \in \SD_M$, $\T_{M\pi^2}\AP{\SC_M \mid s} < \delta$.

\#Definition 2

Fix $\delta\in(0,1)$ and a $\delta$-catastrophe MDP $M$. Consider some $\pi^1: \St_M \rightarrow \A_M$. $\pi^1$ is said to be a *mitigation policy for $M$* when

i. $\pi^1$ is 1-optimal.

ii. For any $s \in \SF_M$, $\T_{M\pi^1}\AP{\SF_M \mid s}=1$.

iii. For any $s \in \SD_M$, $\T_{M\pi^1}\AP{\SC_M \mid s} < \delta$.

Fix $\tau_1,\tau_2 \in (0,\infty)$ and $s_0 \in \SD_M$. $\pi^1$ is said to have *mitigation time moments $\tau_1,\tau_2$ at $s_0$* when

iv. $$\sum_{n=0}^\infty n \AP{\T_{M\pi^1}^{n+1}\AP{\SF_M \mid s_0}-\T_{M\pi^1}^{n}\AP{\SF_M \mid s_0}} \leq \tau_1$$

v. $$\sum_{n=0}^\infty n^2 \AP{\T_{M\pi^1}^{n+1}\AP{\SF_M \mid s_0}-\T_{M\pi^1}^{n}\AP{\SF_M \mid s_0}} \leq \tau_2$$

***

Next, we introduce the notion of an MDP perturbation. We will use it by considering perturbations of a catastrophe MDP which "eliminate the catastrophe."

\#Definition 3

Fix $\delta,\delta'\in(0,1)$ and consider a $\delta'$-catastrophe MDP $M$. An MDP $\tilde{M}$ is said to be a *$\delta$-perturbation of $M$* when

i. $\St_{\tilde{M}} = \St_M$

ii. $\A_{\tilde{M}} = \A_M$

iii. $\R_{\tilde{M}}=\R_M$

iv. For any $s \in \SF_M$ and $a \in \A_M$, $\T_{\tilde{M}}\AP{s,a}=\T_{M}\AP{s,a}$

v. For any $s \in \SD_M$ and $a \in \A_M$, $\Dtva{\T_{\tilde{M}}\AP{s,a},\T_{M}\AP{s,a}} < \delta$

***

Similarly, we can consider perturbations of a policy.

\#Definition 4

Fix $\delta,\delta'\in(0,1)$ and consider a $\delta'$-catastrophe MDP $M$. Given $\pi: \St_M \M \A_M$ and $\tilde{\pi}: \St_M \M \A_M$, $\tilde{\pi}$ is said to be a *$\delta$-perturbation of $\pi$* when

i. For any $s \in \SF_M$, $\tilde{\pi}(s) = \pi(s)$.

ii. For any $s \in \SD_M$, $\Dtva{\tilde{\pi}(s),\pi(s)} < \delta$.

***

We will also need to introduce policy-specific value functions, Q-functions and relatively $k$-optimal actions.

\#Definition 5

Fix an MDP $M$ and $\pi: \St_M \M \A_M$. We define $\V_{M\pi}: \St_M \times (0,1) \rightarrow [0,1]$ and $\Q_{M\pi}: \St_M \times \A_M \times (0,1) \rightarrow [0,1]$ by

$$\V_{M\pi}(s,\gamma) := (1-\gamma) \sum_{n=0}^\infty \gamma^n \Ea{\T_{M\pi}^n(s)}{\R_M}$$

$$\Q_{M\pi}(s,a,\gamma) := (1-\gamma) \R_M(s) + \gamma \Ea{t \sim \T_{M\pi}(s)}{\V_{M\pi}(t,\gamma)}$$

For each $k \in \Nats$, we define $\V_{M\pi}^k: \St_M \rightarrow [0,1]$, $\Q_{M\pi}^k: \St_M \times \A_M \rightarrow [0,1]$ and $\A_{M\pi}^k: \St_M \rightarrow 2^{\A_M}$ by

$$\V_{M\pi}^k(s) := \frac{\D^k \V_{M\pi}(s,\gamma)}{\D\gamma^k}\bigg\vert_{\gamma=1}$$

$$\Q_{M\pi}^k(s,a) := \frac{\D^k \Q_{M\pi}(s,a,\gamma)}{\D\gamma^k}\bigg\vert_{\gamma=1}$$

$$\A_{M\pi}^0(s) := \{a \in \A_M \mid \Q_{M\pi}^0(s,a) \geq \V_{M\pi}^0(s)\}$$

$$\A_{M\pi}^{k+1}(s) := \{a \in \A_{M\pi}^k(s) \mid \Q_{M\pi}^{k+1}(s,a) \geq \V_{M\pi}^{k+1}(s) \text{ or } \exists j \leq k: \Q_{M\pi}^{j}(s,a) > \V_{M\pi}^{j}(s)\}$$

***

Now we give the new (weaker) condition on the advisor policy. For notational simplicity, we assume the policy is stationary. It is easy to generalize these results to non-stationary advisor policies and to policies that depend on irrelevant additional information (i.e. policies for universes that are *realizations* of the MDP).

\#Definition 6

Fix $\epsilon,\delta \in (0,1)$. Consider a $\delta$-catastrophe MDP $M$ with mitigation policy $\pi^1: \St_M\ \rightarrow A_M$. A policy $\pi: \St_M \M \A_M$ is said to be *locally $(\epsilon,\delta)$-sane* for $M$ when there exists a $\delta$-perturbation $\tilde{M}$ of $M$ and a $\delta$-perturbation $\tilde{\pi}$ of $\pi$ s.t.

i. For any $s \in \SF_M$: $$\T_{M\pi}\AP{\SF_M \mid s} = 1$$

ii. For any $s \in \SD_M$: $$\T_{\tilde{M}\tilde{\pi}}\AP{\SC_M \mid s} = 0$$

iii. For any $s \in \St_M \setminus \SC_M$: $$\Supp{\tilde{\pi}(s)} \subseteq \A_{\tilde{M}\pi^1}^0(s)$$

iv. For any $s \in \St_M \setminus \SC_M$: $$\tilde{\pi}\AP{\pi^1(s) \mid s} > \epsilon$$

\Comment{For $M$ an arbitrary MDP, $s_0 \in \St_M$ and $\tau_1,\tau_2 \in (0,\infty)$, we say $\pi$ is *locally $(\epsilon,\delta)$-sane for $\AP{M,s_0}$ with moments $\tau_1,\tau_2$* when *either* $\pi$ is locally $\epsilon$-sane for $\AP{M,s_0}$ *or* there is *some* way to view $M$ as a $\delta$-catastrophe MDP with $s_0 \in \SD_M$ such that the corresponding mitigation policy $\pi^1$ has moments $\tau_1,\tau_2$ at $s_0$ and $\pi$ is locally $(\epsilon,\delta)$-sane for $M$ in the corresponding sense.}

***

Note that a locally $(\epsilon,\delta)$-sane policy still has to be $0$-optimal in $\SF_M$. This requirement seems reasonably realistic, since, roughly speaking, it only means that there is *some* way to "rearrange the universe" that the agent can achieve, and that would be "endorsed" by the advisor, s.t this rearrangement doesn't destroy substantially much value and s.t. after this rearrangement, there is no "impending catastrophe" that the agent has to prevent and the advisor wouldn't be able to prevent in its place. In particular, this rearrangement may involve creating some subagents inside the environment and *destroying the original agent*, in which case any policy on $\SF_M$ is "vacuously" optimal (since all actions have no effect).

We can now formulate the main result.

\#Theorem 1

Fix an interface $\In=(\A,\Ob)$, $N \in \Nats$, $\epsilon \in (0,1)$ and for each $k \in [N]$, an MDP $\MF_k$ s.t. $\A_{\MF_k} = \A$. Now, consider $\delta\in(0,1)$ and for each $k \in [N]$, an $\In$-universe $\upsilon^k=(\mu^k,r^k)$ which is an $\Ob$-realization of a $\delta$-catastrophe MDP $M^k$ with state function $S^k$ s.t.

i. $\SF_{M^k} = \St_{\MF_k}$

ii. For each $s \in \St_{\MF_k}$ and $a \in \A$, $\T_{M^k}(s,a) \mid \St_{\MF_k} = \T_{\MF_k}(s,a)$.

iii. For each $s \in \St_{\MF_k}$, $\R_{M^k}(s)=\R_{\MF_k}(s)$.

iv. Given $k,j \in [N]$ and $h \in \HD{\mu^k} \cap \HD{\mu^j}$, if $S^k(h) \in \St_{M^k} \setminus \SC_{M^k}$ and $S^j(h) \in \St_{M^j} \setminus \SC_{M^j}$, then $r^k(h)=r^j(h)$ (this condition means that in uncorrupted states, the reward is observable).

Consider also $\alpha\in(0,1)$, $\tau_{1,2} \in (0,\infty)$ and $\sigma^k$ a locally $(\epsilon,\delta)$-sane policy for $M^k$. Assume $M^k$ has a mitigation policy with mitigation time moments $\tau_{1,2}$. Then, there exists an $\Adi$-policy $\pi^*$ s.t. for any $k \in [N]$

$$\EU_{\upsilon^k}^*(1-\alpha) - \EU_{\bar{\upsilon}^k\AB{\sigma^k}}^{\pi^*}(1-\alpha) = O\AP{\tau_2 \alpha + (\tau_1 \alpha)^{1/4} + \delta \AP{\tau_2 + \tau_1 \alpha^{-3/4}}}$$

Here, $\epsilon$ and the $\MF_k$ are regarded as *fixed* and we don't explicitly examine their effect on regret, whereas $\alpha$, $\delta$, $\tau_{1,2}$ and the $M^k$ are regarded as variable with the asymptotics $\alpha,\delta \rightarrow 0$, $\tau_{1,2} \rightarrow \infty$.

***

To clarify the behavior of the regret bound, we give a simple example.

\#Example 1

Let $\A = \{0,1,*\}$, $\Ob=\Bool$. For any $n \in \Nats$ and $k \in [N]$, we fix some $w_n^k \in \Bool^n$ and define the catastrophe MDP $M_n^k$ by

* $\SD_{M_n^k} = \Bool^{\leq n}$, $\SF_{M_n^k} = \{\bot,\top\}$, $\SC_{M_n^k} = \varnothing$ (adding corrupted states is an easy exercise).

* If $s \in \Bool^{< n}$ and $a \in \Bool$ then 

$$\T_{M_n^k}(sa \mid s,a) = 1 - \delta$$

$$\T_{M_n^k}(\bot \mid s,a) = \delta$$

$$\T_{M_n^k}(s \mid sa,*) = 1 - \delta$$

$$\T_{M_n^k}(\bot \mid sa,*) = \delta$$

* If $a \in {0,1}$ then

$$\T_{M_n^k}(\top \mid w_n^k,a) = 1$$

* If $s \in \Bool^n \setminus w_n^k$ and $a \in {0,1}$ then

$$\T_{M_n^k}(\bot \mid s,a) = 1$$

* If $s \in \{\bot,\top\}$ and $a \in \A$ then

$$\T_{M_n^k}(s \mid s,a) = 1$$

* $\R_{M_n^k}(\bot)=0$, if $s \in \St_{M_n^k} \setminus \bot$ then $\R_{M_n^k}(s)=1$.

* $S_n^k(\Estr_{\A \times \Ob})=\Estr_{\Bool}$ and $S_n^k(hao)=\bot$ iff $o = 0$ (this defines a unique $S_n^k$).

* If $s \in \Bool^{<n} \cup \{\bot,\top\}$ then $\sigma_n^k(a \mid s) = \frac{1}{3}$ for any $a \in \A$.

* $\sigma_n^k(0 \mid w_n^k) = \epsilon$, $\sigma_n^k(* \mid w_n^k) = 1 - \epsilon$.

* If $s \in \Bool^n \setminus w_n^k$ then $\sigma_n^k(0 \mid s) = \delta$, $\sigma_n^k(* \mid s) = 1 - \delta$.

We can take $\tau_1 = n$, $\tau_2 = 0$. Fix any $\lambda \in (0,\infty)$ and consider the asymptotic regime $n \rightarrow \infty$, $\alpha_n = \Theta\Big(n^{-1-\lambda}\Big)$, $\delta_n = O\AP{n^{-7/4-\lambda}}$. According to Theorem 1, we get

$$\EU_{\upsilon_n^k}^*(1-\alpha_n) - \EU_{\bar{\upsilon}_n^k\AB{\sigma_n^k}}^{\pi_n^*}(1-\alpha_n) = O\AP{\AP{n \cdot n^{-1-\lambda}}^{1/4}+n^{-7/4-\lambda}\cdot n \cdot \AP{n^{-1-\lambda}}^{-3/4}} = O\AP{n^{-\lambda/4}}$$

The probability of a catastrophe (n.e ending up in state $\bot$) for the optimal policy for a given $k$ is $O\AP{n\delta}=O\AP{n^{-3/4-\lambda}}$. Therefore, the probability of a catastrophe for policy $\pi_n^*$ is $O\AP{n^{-3/4-\lambda}+n^{-\lambda/4}}=O\AP{n^{-\lambda/4}}$. On the other hand, it is easy to see that for $\delta_n = \omega\AP{2^{-n}}$, the policy $\sigma_n^k$ has a probability of catastrophe $1-o(1)$ (and in particular  regret $\Omega(1)$): it spends $\Omega(2^n)$ time "exploring" $\Bool^{\leq n}$ with a probability $\delta$ of a catastrophe on every step.

Note that this example can be interpreted as a version of Christiano's [approval-directed agent](https://ai-alignment.com/model-free-decisions-6e6609f5d99e), if we regard the state $s \in \Bool^{i}$ as a "plan of action" that the advisor may either approve or not. But in this formalism, it is a special case of consequentialist reasoning.

***

Theorem 1 speaks of a finite set of environments, but as before (see Proposition 1 [here](https://agentfoundations.org/item?id=1550) and Corollary 3 [here](https://agentfoundations.org/item?id=1656)), there is a "structural" equivalent, n.e. we can use it to produce corollaries about Bayesian agents with priors over a countable set of environments. The difference is, in this case we consider asymptotic regimes in which the environment is also variable, so the probability weight of the environment in the prior will affect the regret bound. We leave out the details for now.

\section{Appendix A}

We start by deriving a more general and more precise version of the non-catastrophic regret bound, in which the optimal policy is replaced by an arbitrary "reference policy" (later it will be related to the mitigation policy) and the dependence on the MDPs is expressed via a bound on their $\V^1$ and $\V^2$ functions.

\#Definition A.N1

Fix $\epsilon\in(0,1)$. Consider an MDP $M$ and policies $\pi,\sigma: \St_M \M \A_M$. $\sigma$ is called *$\epsilon$-sane relatively to $\pi$* when for any $s \in \St_M$

i. $\Supp{\sigma(s)} \subseteq \A_{M\pi}^0$

ii. There is $a \in \A_{M\pi}^1$ s.t. $\sigma(a \mid s) > \epsilon$.

\#Lemma A.N1

Fix an interface $\In=(\A,\Ob)$, $N \in \Nats$ and $\epsilon \in (0,1)$. Now, consider for each $k \in [N]$, an $\In$-universe $\upsilon^k=(\mu^k,r)$ which is an $\Ob$-realization of an MDP $M_k$ with state function $S^k$ and policies $\pi^k,\sigma^k: \St_{M^k} \M \A$. Consider also $\alpha\in(0,1)$, $\tau_{1,2} \in (0,\infty)$ and assume that 

i. $\sigma^k$ is $\epsilon$-sane relatively to $\pi^k$.

ii. For any $s \in \St_{M^k}$, $\Abs{\V^1_{M^k\pi^k}(s)} \leq \tau_1$.

iii. For any $s \in \St_{M^k}$, $\Abs{\V^2_{M^k\pi^k}(s)} \leq \tau_2$.

Then, there exists an $\Adi$-policy $\pi^*$ s.t. for any $k \in [N]$

$$\EU_{\upsilon^k}^{\pi^k}(1-\alpha) - \EU_{\bar{\upsilon}^k\AB{\sigma^k}}^{\pi^*}(1-\alpha) \leq O\AP{\tau_2 \alpha + (\tau_1 \alpha)^{1/4}}$$

Here, $\epsilon$ is regarded as *fixed* and we don't explicitly examine its effect on regret, whereas $\alpha$, $\tau_{1,2}$ and the $M_k$ are regarded as variable with the asymptotics $\alpha \rightarrow 0$, $\tau_{1,2} \rightarrow \infty$.

***

TBD: Proof of Lemma A.N1

\#Proposition A.N2

Fix $\tau_1, \tau_2, \TF_1, \TF_2 \in (0,\infty)$. Consider an MDP $M$, a partition $\St_M = \SD_M \sqcup \SF_M$, a state $s_0 \in \SD_M$ and a policy $\pi: \St_M \M \A_M$. Assume that

i. For any $s \in \SF_M$, $\T_{M\pi}\AP{\SF_M \mid s}=1$.

ii. $$\sum_{n=0}^\infty n \AP{\T_{M\pi}^{n+1}\AP{\SF_M \mid s_0}-\T_{M\pi}^{n}\AP{\SF_M \mid s_0}} \leq \tau_1$$

iii. $$\sum_{n=0}^\infty n^2 \AP{\T_{M\pi}^{n+1}\AP{\SF_M \mid s_0}-\T_{M\pi}^{n}\AP{\SF_M \mid s_0}} \leq \tau_2$$

iv. For any $s \in \SF_M$, $\Abs{\V^1_{M\pi}(s)} \leq \TF_1$.

v. For any $s \in \SF_M$, $\Abs{\V^2_{M\pi}(s)} \leq \TF_2$. 

Then:

a. For any $s \in \St_M$, $\Abs{\V^1_{M\pi}(s)} = O\AP{\TF_1 + \tau_1}$

b. For any $s \in \St_M$, $\Abs{\V^2_{M\pi}(s)} = O\AP{\TF_2 + \TF_1 \tau_1 + \tau_2}$

***

Note that Proposition A.N2 is really about Markov chains rather than MDPs, but don't make it explicit to avoid introduce more notation.

TBD: Proof of Proposition A.N2

\end{document}



