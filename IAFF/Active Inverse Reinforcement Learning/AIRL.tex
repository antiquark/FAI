%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{dom}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}

\newcommand{\Prj}{\operatorname{pr}}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}
\newcommand{\PF}{\xrightarrow{\circ}}

\newcommand{\Prob}{\mathcal{P}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\FH}{(\A \times \Ob)^*}
\newcommand{\IH}{(\A \times \Ob)^\omega}
\newcommand{\Ado}{\bar{\Ob}}
\newcommand{\Ada}{\bar{\A}}
\newcommand{\Adi}{\bar{\I}}
\newcommand{\Adao}{\overline{\A \times \Ob}}
\newcommand{\Adfh}{\Adao^*}
\newcommand{\Adih}{\Adao^\omega}
\DeclareMathOperator{\HD}{hdom}
\newcommand{\Hy}{\mathcal{H}}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\V}{\operatorname{V}}
\newcommand{\W}{\operatorname{W}}
\newcommand{\EU}{\operatorname{EU}}

\begin{document}

We introduce a reinforcement-like learning setting we call *active inverse reinforcement learning* (AIRL). In AIRL, the agent can, at any point of time, defer the choice of action to an "advisor". The agent knows neither the environment nor the reward function, whereas the advisor knows both. Thus, AIRL can be regarded as a special case of CIRL. A similar setting was studied in [Clouse 1997](https://web.cs.umass.edu/publication/docs/1997/UM-CS-1997-026.pdf), but as far as we can tell, the relevant literature offers few theoretical results and virtually all researchers focus on the MDP case (*please correct me if I'm wrong*). On the other hand, we consider general environments (not necessarily MDP or even POMDP) and prove a natural performance guarantee.

The use of an advisor allows us to kill two birds with one stone: learning the reward function and safe exploration (i.e. avoiding both the Scylla of "[Bayesian paranoia](http://proceedings.mlr.press/v40/Leike15.pdf)" and the Charybdis of falling into traps). We prove that, given certain assumption about the advisor, a Bayesian AIRL agent (whose prior is supported on some countable set of hypotheses) is guaranteed to attain most of the value in the slow falling time discount (long-term planning) limit (assuming one of the hypotheses in the prior is true). The assumption about the advisor is quite strong, but the advisor is not required to be fully optimal: a "soft maximizer" satisfies the conditions. Moreover, we allow for the existence of "corrupt states" in which the advisor stops being a relevant signal, thus demonstrating that this approach can deal with wireheading and avoid manipulating the advisor, at least in principle (the assumption about the advisor is still unrealistically strong). Finally we consider advisors that don't know the environment but have some *beliefs* about the environment, and show that in this case the agent converges to Bayes-optimality w.r.t. the advisor's beliefs, which is arguably the best we can expect.

***

All the proofs are in the Appendix.

\section{Notation}

Whatever...

\section{Results}

An *interface* $\I = (\A,\Ob)$ is a pair of finite sets  ("actions" and "observations"). An $\I$-*policy* is a function $\pi: \FH \rightarrow \Delta\A$. An $\I$-*environment* is a partial function $\mu: \FH \times \A \PF \Delta\Ob$ s.t. 

\begin{enumerate}[i.]

\item $\Estr_{\A \times \Ob} \times \A \subseteq \Dom \mu$
\item Given $h \in \FH$ and $aob \in \A \times \Ob \times \A$, $haob \in \Dom \mu$ iff $h \in \Dom \mu$ and $\mu(ha)(o) > 0$.

\end{enumerate}

It is easy to see that $\Dom \mu$ is always of the form $X \times \A$ for some $X \subseteq \FH$. We denote $\HD \mu := X$.

Given an $\I$-policy $\pi$ and an $\I$-environment $\mu$, we get $\mu^\pi \in \Delta\IH$ in the usual way. We will use the notation $\E_\mu^\pi := \E_{\mu^\pi}$ for expected values.

An $\I$-*reward function* is a partial function $r: (\A \times \Ob)^* \PF [0,1]$. An $\I$-*universe* is a pair $(\mu,r)$ where $\mu$ is an $\I$-environment and $r$ is an $\I$-reward function s.t. $\Dom{r} \supseteq \HD{\mu}$. We denote the space of $\I$-universes by $\Upsilon_\I$. Given an $\I$-reward function $r$ and $t \in (0,\infty)$, we have the associated *utility function* $\Ut_t^r: \IH \PF [0,1]$ defined by

$$\Ut_t^{r}(x):=\frac{\sum_{n=0}^\infty e^{-n/t} r(x_{:n})}{\sum_{n=0}^\infty e^{-n/t}}$$

Here and throughout, we use geometric time discount, however this choice is mostly for notational simplicity. More or less all results carry over to other shapes of the time discount function.

Denote $\Pi_{\I}$\ the space of $\I$-policies. An $\I$-*metapolicy* is a family $\{\pi_t \in \Pi_\I\}_{t \in (0, \infty)}$, where the parameter $t$ is thought of as setting the scale of the time discount. An $\I$-*meta-universe* is a family $\{\upsilon_t \in \Upsilon\}_{t \in (0, \infty)}$. This latter concept is useful for analyzing multi-agent systems, where the environment contains other agents and we study the asymptotics when all agents' time discount scales go to infinity. We won't focus on the multi-agent case in this essay, but for future reference, it seems useful to make sure the results hold in the greater generality of meta-universes.

Given an $\I$-policy $\pi$, an $\I$-universe $\upsilon$ and $t > 0$, we denote $\EU_\upsilon^\pi(t):=\E_\mu^\pi[\Ut^r_t]$ (this is well-defined since $\Ut^r_t$ is defined on the support of $\mu^\pi$). We will omit $\I$ when it is obvious from the context.

\#Definition 1

Fix an interface $\I$. Consider $\pi^*$ a metapolicy and $\Hy$ a set of meta-universes. $\pi^*$ is said to *learn* $\Hy$ when for any $\upsilon \in \Hy$

$$\lim_{t \rightarrow \infty} (\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t) - \EU_{\upsilon_t}^{\pi^*_t}(t)) = 0$$

$\Hy$ is said to be *learnable* when there exists $\pi^*$ that learns $\Hy$.

***

Our notion of learnability is closely related to the notion of *sublinear regret*, as defined in [Leike 2016](https://arxiv.org/abs/1611.08944), except that we allow the policy to explicitly depend on the time discount scale.

\#Proposition 1

Fix an interface $\I$. Consider $\Hy$ a countable learnable set of meta-universes. Consider any $\zeta \in \Delta\Hy$ s.t. $\Supp \zeta = \Hy$. Consider $\pi^\zeta$ a $\zeta$-Bayes optimal metapolicy, i.e.

$$\pi^\zeta_t \in \Argmax{\pi \in \Pi} \E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi}(t)]$$

Then, $\pi^\zeta$ learns $\Hy$.

***

Proposition 1 shows that Bayesian agents are optimal in a "frequentist" sense for any countable class of meta-universes s.t. *some* metapolicy is optimal in this sense.

Another handy property of learnability is the following.

\#Proposition 2

Fix an interface $\I$. Let $\Hy$ be a countable set of meta-universes s.t. any finite $\mathcal{G} \subseteq \Hy$ is learnable. Then, $\Hy$ is learnable.

***

We now introduce the formalism needed to discuss advisors. Define $\Ada:=\A \sqcup \{\bot\}$, $\Ado:=\Ada \times \Ob$ and $\Adi:=(\Ada,\Ado)$. Here, the $\Ada$ factor of $\Ado$ is the action taken by the advisor, assumed to be observable by the agent. The environments we will consider are s.t. this action is $\bot$ unless the agent deferred to the advisor at this point of time, which is specified by the agent taking action $\bot$. It will also be the case that whenever the agent takes action $\bot$, the advisor cannot take action $\bot$.

Denote $\Adao:= \Ada \times \Ado \setminus \{\bot\bot o \mid o \in \Ob\}$. Given $abo \in \Adao$, we define $\underline{abo} \in \A \times \Ob$ by

$$\underline{abo}:=\begin{cases} ao \text{ if } a\ne\bot \\ bo \text{ if } a=\bot \end{cases}$$

Given $h \in \Adfh$, we define $\underline{h} \in \FH$ by $\underline{h}_n:=\underline{h_n}$. 

\#Definition 2

An $\Adi$-policy $\alpha$ is said to be *autonomous* when for any $h \in \Adfh$, $\alpha(h)(\bot)=0$.

***

Consider an $\I$-environment $\mu$ and an autonomous $\Adi$-policy $\alpha$, which we think of as the advisor policy. We define the $\Adi$-environment $\bar{\mu}[\alpha]$ as follows. For any $h \in \Adfh$ s.t. $\underline{h} \in \HD \mu$, $a,b \in \Ada$ and $o \in \Ob$:

$$\bar{\mu}[\alpha](ha)(bo):=\begin{cases} \mu(\underline{h}a)(o) \text{ if } a\ne\bot,\, b=\bot \\ \alpha(h)(b)\cdot\mu(\underline{h}b)(o) \text{ if } a=\bot,\,b\ne\bot \\ 0 \text{ if } a\ne\bot,\, b\ne\bot \text{ or } a=b=\bot \end{cases}$$

It is easy to the above is a well-defined $\Adi$-environment with $\HD \mu \subseteq \Adfh$. 

Given an $\I$-universe $\upsilon=(\mu,r)$, we define the $\Adi$-reward function $\bar{r}: \Adfh \PF [0,1]$ by $\bar{r}(h):=r(\underline{h})$ and the $\Adi$-universe $\bar{\upsilon}[\alpha]:=(\bar{\mu}[\alpha],\bar{r})$.

We now introduce the conditions on the advisor policy which will allow us to prove a learnability theorem. First, we specify an advisor that always remains "approximately rational."

Given a universe $\upsilon=(\mu,r)$ and $t \in (0, \infty)$ we define $\V_t^\upsilon: \HD{\mu} \rightarrow [0,1]$, $\W_t^\upsilon: \HD{\mu} \times \A \rightarrow [0,1]$ and $\A^\upsilon_t: \HD{\mu} \rightarrow 2^\A$ by

$$\V_t^\upsilon(h):=\max_{\pi \in \Pi} {\E_{x \sim \mu^\pi}[\frac{\sum_{n=\Abs{h}}^\infty e^{-n/t} r(x_{:n})}{\sum_{n=\Abs{h}}^\infty e^{-n/t}} \mid h \sqsubset x]}$$

$$\W_t^\upsilon(ha):=\E_{o \sim \mu(ha)}[V_t^\upsilon(hao)]$$

$$\A^\upsilon_t(h) := \Argmax{a \in \A} \W_t^\upsilon(h)$$

\#Definition 3

Fix an interface $\I$. Consider a universe $\upsilon=(\mu,r)$. Let $t,\beta \in (0,\infty)$. A policy $\alpha$ is called *strictly $\beta$-rational* for $(\upsilon,t)$ when for any $h \in \Dom{\mu}$ and $a \in \A$

$$\alpha(h)(a) \leq \exp{[\beta(\W^{\upsilon}_t(ha)-\V^\upsilon_t(h))]} \max_{a^* \in \A^\upsilon_t(h)} \alpha(h)(a^*)$$

***

Now we deal with the possibility of the advisor becoming "corrupt". In practical implementations where the "advisor" is a human operator, this can correspond to several types of events, e.g. sabotaging the channel that transmits data from the operator to the AI ("wireheading"), manipulation of the operator or replacement of the operator by a different entity.

\#Definition 4

Fix an interface $\I$. Consider a family $\{\kappa_t: \FH \rightarrow [0,1]\}_{t \in (0,\infty)}$, where we think of $\kappa_t(h)$ as the probability of a certain event occurring right after history $h$, conditional on it not occurring before. Define $\hat{\kappa}_t: \FH \rightarrow [0,1]$ by

$$\hat{\kappa}_t(h):= \prod_{n = 0}^{\Abs{h}} \kappa_t(h_{:n})$$

We think of $\hat{\kappa}_t(h)$ as the *accumulated* probability the event occurred during history $h$. Consider a meta-universe $\upsilon=(\mu,r)$. $\kappa$ is said to be a $\upsilon$-*avoidable transition probability* when there is a meta-policy $\pi^*$ s.t.

\begin{enumerate}[i.]

\item $\lim_{t \rightarrow \infty} {(\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t)-\EU_{\upsilon_t}^{\pi^*_t}(t))} = 0$
\item $\lim_{t \rightarrow \infty} \E_{x \sim \mu_t^{\pi^*_t}}[\lim_{n \rightarrow \infty} \hat{\kappa}_t(x_{:n})] = 0$

***

That is, $\kappa$ is $\upsilon$-avoidable when it is possible to avoid the associated event while retaining most of the value. Consider a meta-universe $\upsilon=(\mu,r)$ and $\kappa$ a $\upsilon$-avoidable transition probability. Denote $-\upsilon:=(\mu,1-r)$. We define the reward function $r^\kappa_t$ by

$$r^\kappa_t(h):=\kappa_t(\Estr_{\A \times \Ob}) \V^{-\upsilon}_t(\Estr_{\A \times \Ob})+\sum_{n=1}^{\Abs{h}} (1-\hat{\kappa}_t(h_{:n-1}))\kappa_t(h_{:n}) \V^{-\upsilon}_t(h_{:n})+(1-\hat{\kappa}_t(h))r_t(h)$$

We think of $r^\kappa$ as representing a process wherein once the event represented by $\kappa$ occurs, the agent starts *minimizing* the utility function. We also use the notation $\upsilon^\kappa:=(\mu,r^\kappa)$.

\#Definition 5

Consider a meta-universe $\upsilon=(\mu,r)$. Let $\beta \in (0,\infty)$. An autonomous $\Adi$-metapolicy $\alpha$ is called *$\beta$-rational* for $\upsilon$ when there exists a $\upsilon$-avoidable transition probability $\kappa$ (that we think of as the probability for the advisor to become "corrupt") and autonomous $\Adi$-metapolicies $\alpha^*,\alpha_*$ (representing advisor policy conditional on non-corruption and corruption respectively) s.t. 

\begin{enumerate}[i.]

\item For any $h \in \Adfh$, $\alpha_t(h) = \hat{\kappa}_t(\underline{h}) \alpha_*(h) + (1 - \hat{\kappa}_t(\underline{h}))\alpha^*(h)$.
\item $\alpha^*_t$ is strictly $\beta$-rational for $(\bar{\upsilon}^\kappa_t[\alpha], t)$.

\end{enumerate}

\end{enumerate}

***

Our definition of $\beta$-rationality requires the advisor to be extremely averse to corruption: the advisor behaves as if, once a corruption occurs, the agent policy becomes the worst possible. In general, this seems much too strong: by the time corruption occurs, the agent might have already converged into accurate beliefs about the universe that allow it to detect the corruption and keep operating without the advisor. Even better, the agent can usually outperform *the worst possible policy* using the prior alone. Moreover, we can differentiate between different degrees of corruption and treat them accordingly. We leave those further improvements for future work.

% Main theorem!

\section{Appendix A}

\#Proof of Proposition 1

Fix $\pi^*$ a metapolicy that learns $\Hy$. Consider $\epsilon > 0$ and let $\Hy_\epsilon \subseteq \Hy$ be finite s.t. $\zeta(\Hy \setminus \Hy_\epsilon) < \epsilon$. For $t \gg 0$ and every $\upsilon \in \Hy_\epsilon$ we have

$$\EU_{\upsilon_t}^{\pi^*_t}(t) \geq \max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t) - \epsilon$$

Also

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^*_t}(t)] \geq \E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^*_t}(t); \upsilon \in \Hy_\epsilon]$$

Combining, we get

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t); \upsilon \in \Hy_\epsilon] - \epsilon$$

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t)] -  \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t); \upsilon \not\in \Hy_\epsilon] - \epsilon$$

By definition of $\Hy_\epsilon$, this implies

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t)] -  2\epsilon$$

For any $\upsilon \in \Hy$, we get

$$\EU_{\upsilon_t}^{\pi^\zeta_t}(t) \geq \max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t) - \frac{2\epsilon}{\zeta(\upsilon)}$$

Taking $\epsilon$ to 0, we get the desired result.

\#Proof of Proposition 2

Let $\Hy = \{\upsilon^k\}_{k \in \Nats}$. For each $k \in \Nats$, let $\pi^k$ learn $\{\upsilon^l\}_{l < k}$. Choose $\{t^k \in (0,\infty)\}_{k \in \Nats}$ s.t.

\begin{enumerate}[i.]

\item $t^0 = 0$
\item $t^k < t^{k+1}$
\item $\lim_{k \rightarrow \infty} t^k = \infty$
\item For any $l < k$ and $t \geq t^k$, $\EU_{\upsilon^l_t}^{\pi^k_t}(t) \geq \max_{\pi \in \Pi} \EU_{\upsilon^l_t}^\pi(t) - \frac{1}{k+1}$.

Now define $\pi^*_t:=\pi_t^{\max\{k \mid t \geq t^k\}}$. Clearly, $\pi^*$ learns $\Hy$.

\end{enumerate}

\end{document}



