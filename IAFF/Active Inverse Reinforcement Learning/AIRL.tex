%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{dom}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}

\newcommand{\Prj}{\operatorname{pr}}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}
\newcommand{\PF}{\xrightarrow{\circ}}

\newcommand{\Prob}{\mathcal{P}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\FH}{(\A \times \Ob)^*}
\newcommand{\IH}{(\A \times \Ob)^\omega}
\newcommand{\Ado}{\bar{\Ob}}
\newcommand{\Ada}{\bar{\A}}
\newcommand{\Adi}{\bar{\I}}
\newcommand{\Adao}{\overline{\A \times \Ob}}
\newcommand{\Adfh}{\Adao^*}
\newcommand{\Adih}{\Adao^\omega}
\DeclareMathOperator{\HD}{hdom}
\newcommand{\Hy}{\mathcal{H}}

\newcommand{\Ut}{\operatorname{U}}
\newcommand{\V}{\operatorname{V}}
\newcommand{\W}{\operatorname{W}}
\newcommand{\EU}{\operatorname{EU}}

\begin{document}

We introduce a reinforcement-like learning setting we call *active inverse reinforcement learning* (AIRL). In AIRL, the agent can, at any point of time, defer the choice of action to an "advisor". The agent knows neither the environment nor the reward function, whereas the advisor knows both. Thus, AIRL can be regarded as a special case of CIRL. A similar setting was studied in [Clouse 1997](https://web.cs.umass.edu/publication/docs/1997/UM-CS-1997-026.pdf), but as far as we can tell, the relevant literature offers few theoretical results and virtually all researchers focus on the MDP case (*please correct me if I'm wrong*). On the other hand, we consider general environments (not necessarily MDP or even POMDP) and prove a natural performance guarantee.

The use of an advisor allows us to kill two birds with one stone: learning the reward function and safe exploration (i.e. avoiding both the Scylla of "[Bayesian paranoia](http://proceedings.mlr.press/v40/Leike15.pdf)" and the Charybdis of falling into traps). We prove that, given certain assumption about the advisor, a Bayesian AIRL agent (whose prior is supported on some countable set of hypotheses) is guaranteed to attain most of the value in the slow falling time discount (long-term planning) limit (assuming one of the hypotheses in the prior is true). The assumption about the advisor is quite strong, but the advisor is not required to be fully optimal: a "soft maximizer" satisfies the conditions. Moreover, we allow for the existence of "corrupt states" in which the advisor stops being a relevant signal, thus demonstrating that this approach can deal with wireheading and avoid manipulating the advisor, at least in principle (the assumption about the advisor is still unrealistically strong). Finally we consider advisors that don't know the environment but have some *beliefs* about the environment, and show that in this case the agent converges to Bayes-optimality w.r.t. the advisor's beliefs, which is arguably the best we can expect.

***

All the proofs are in the Appendix.

\section{Notation}

Whatever...

\section{Results}

An *interface* $\I = (\A,\Ob)$ is a pair of finite sets  ("actions" and "observations"). An $\I$-*policy* is a partial function $\pi: \FH \PF \Delta\A$ s.t.

\begin{enumerate}[i.]

\item $\Estr_{\A \times \Ob} \in \Dom \pi$
\item Given $h \in \FH$ and $ao \in \A \times \Ob$, $hao \in \Dom \pi$ iff $h \in \Dom \pi$ and $\pi(h)(a) > 0$.

\end{enumerate}

An $\I$-*environment* is a partial function $\mu: \FH \times \A \PF \Delta\Ob$ s.t. 

\begin{enumerate}[i.]

\item $\Estr_{\A \times \Ob} \times \A \subseteq \Dom \mu$
\item Given $h \in \FH$ and $aob \in \A \times \Ob \times \A$, $haob \in \Dom \mu$ iff $h \in \Dom \mu$ and $\mu(ha)(o) > 0$.

\end{enumerate}

It is easy to see that $\Dom \mu$ is always of the form $X \times \A$ for some $X \subseteq \FH$. We denote $\HD \mu := X$.

Given an $\I$-policy $\pi$ and an $\I$-environment $\mu$, we get $\mu^\pi \in \Delta\IH$ in the usual way. We will use the notation $\E_\mu^\pi := \E_{\mu^\pi}$ for expected values.

An $\I$-*reward function* is a function $r: (\A \times \Ob)^* \rightarrow [0,1]$. An $\I$-*universe* is a pair $(\mu,r)$ where $\mu$ is an $\I$-environment and $r$ is an $\I$-reward function. We denote the space of $\I$-universes by $\Upsilon_\I$. Given an $\I$-reward function $r$ and $t \in (0,\infty)$, we have the associated *utility function* $\Ut_t^r: \IH \rightarrow [0,1]$ defined by

$$\Ut_t^{r}(h):=\frac{\sum_{n=0}^\infty e^{-n/t} r(h_{:n})}{\sum_{n=0}^\infty e^{-n/t}}$$

Here and throughout, we use geometric time discount, however this choice is mostly for notational simplicity. More or less all results carry over to other shapes of the time discount function.

Denote $\Pi_{\I}$\ the space of $\I$-policies. An $\I$-*metapolicy* is a family $\{\pi_t \in \Pi_\I\}_{t \in (0, \infty)}$, where the parameter $t$ is thought of as setting the scale of the time discount. An $\I$-*meta-universe* is a family $\{\upsilon_t \in \Upsilon\}_{t \in (0, \infty)}$. This latter concept is useful for analyzing multi-agent systems, where the environment contains other agents and we study the asymptotics when all agents' time discount scales go to infinity. We won't focus on the multi-agent case in this essay, but for future reference, it seems useful to make sure the results hold in the greater generality of meta-universes.

Given an $\I$-policy $\pi$, an $\I$-universe $\upsilon$ and $t > 0$, we denote $\EU_\upsilon^\pi(t):=\E_\mu^\pi[\Ut^r_t]$. We will omit $\I$ when it is obvious from the context.

\#Definition 1

Fix an interface $\I$. Consider $\pi^*$ a metapolicy and $\Hy$ a set of meta-universes. $\pi^*$ is said to *learn* $\Hy$ when for any $\upsilon \in \Hy$

$$\lim_{t \rightarrow \infty} (\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t) - \EU_{\upsilon_t}^{\pi^*_t}(t)) = 0$$

$\Hy$ is said to be *learnable* when there exists $\pi^*$ that learns $\Hy$.

***

Our notion of learnability is closely related to the notion of *sublinear regret*, as defined in [Leike 2016](https://arxiv.org/abs/1611.08944), except that we allow the policy to explicitly depend on the time discount scale.

\#Proposition 1

Fix an interface $\I$. Consider $\Hy$ a countable learnable set of meta-universes. Consider any $\zeta \in \Delta\Hy$ s.t. $\Supp \zeta = \Hy$. Consider $\pi^\zeta$ a $\zeta$-Bayes optimal metapolicy, i.e.

$$\pi^\zeta_t \in \Argmax{\pi \in \Pi} \E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi}(t)]$$

Then, $\pi^\zeta$ learns $\Hy$.

***

Proposition 1 shows that Bayesian agents are optimal in a "frequentist" sense for any countable class of meta-universes s.t. *some* metapolicy is optimal in this sense.

Another handy property of learnability is the following.

\#Proposition 2

Fix an interface $\I$. Let $\Hy$ be a countable set of meta-universes s.t. any finite $\mathcal{G} \subseteq \Hy$ is learnable. Then, $\Hy$ is learnable.

***

We now introduce the formalism needed to discuss advisors. Define $\Ada:=\A \sqcup \{\bot\}$, $\Ado:=\Ada \times \Ob$ and $\Adi:=(\Ada,\Ado)$. Here, the $\Ada$ factor of $\Ado$ is the action taken by the advisor, assumed to be observable by the agent. The environments we will consider are s.t. this action is $\bot$ unless the agent deferred to the advisor at this point of time, which is specified by the agent taking action $\bot$. It will also be the case that whenever the agent takes action $\bot$, the advisor cannot take action $\bot$.

Denote $\Adao:= \Ada \times \Ado \setminus \{\bot\bot o \mid o \in \Ob\}$. Given $abo \in \Adao$, we define $\underline{abo} \in \A \times \Ob$ by

$$\underline{abo}:=\begin{cases} ao \text{ if } a\ne\bot \\ bo \text{ if } a=\bot \end{cases}$$

Given $h \in \Adfh$, we define $\underline{h} \in \FH$ by $\underline{h}_n:=\underline{h_n}$. Consider an $\I$-environment $\mu$ and an $\I$-policy $\alpha$, which we think of as the advisor policy. We define the $\Adi$-environment $\bar{\mu}[\alpha]$ as follows. For any $h \in \Adfh$ s.t. $\underline{h} \in \HD \mu$, $a,b \in \Ada$ and $o \in \Ob$:

$$\bar{\mu}[\alpha](ha)(bo):=\begin{cases} \mu(\underline{h}a)(o) \text{ if } a\ne\bot,\, b=\bot \\ \alpha(\underline{h})(b)\cdot\mu(\underline{h}b)(o) \text{ if } a=\bot,\,b\ne\bot \\ 0 \text{ if } a\ne\bot,\, b\ne\bot \text{ or } a=b=\bot \end{cases}$$

It is easy to the above is a well-defined $\Adi$-environment with $\HD \mu \subseteq \Adfh$.

We now introduce the conditions on the advisor policy which will allow us to prove a learnability theorem.

Given a universe $\upsilon=(\mu,r)$ and $t \in (0, \infty)$ we define $\V_t^\upsilon: \HD{\mu} \rightarrow [0,1]$, $\W_t^\upsilon: \HD{\mu} \times \A \rightarrow [0,1]$ and $\A^\upsilon_t: \HD{\mu} \rightarrow 2^\A$ by

$$\V_t^\upsilon(h):=\max_{\pi \in \Pi} {\E_{x \sim \mu^\pi}[\frac{\sum_{n=\Abs{h}}^\infty e^{-n/t} r(x_{:n})}{\sum_{n=\Abs{h}}^\infty e^{-n/t}} \mid h \sqsubset x]}$$

$$\W_t^\upsilon(ha):=\E_{o \sim \mu(ha)}[V_t^\upsilon(hao)]$$

$$\A^\upsilon_t(h) := \Argmax{a \in \A} \W_t^\upsilon(h)$$

\#Definition 2

Fix an interface $\I$ and consider a universe $\upsilon=(\mu,r)$. Let $t,\beta \in (0,\infty)$. A policy $\alpha$ is called *strictly $\beta$-rational* for $(\upsilon,t)$ when for any $h \in \HD{\mu}$ and $a \in \A$

$$\alpha(h)(a) \leq \exp{[\beta(\W^\upsilon_t(ha)-\V^\upsilon_t(h))]} \max_{a^* \in \A^\upsilon_t(h)} \alpha(h)(a^*) $$

FOO

\section{Appendix A}

\#Proof of Proposition 1

Fix $\pi^*$ a metapolicy that learns $\Hy$. Consider $\epsilon > 0$ and let $\Hy_\epsilon \subseteq \Hy$ be finite s.t. $\zeta(\Hy \setminus \Hy_\epsilon) < \epsilon$. For $t \gg 0$ and every $\upsilon \in \Hy_\epsilon$ we have

$$\EU_{\upsilon_t}^{\pi^*_t}(t) \geq \max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t) - \epsilon$$

Also

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^*_t}(t)] \geq \E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^*_t}(t); \upsilon \in \Hy_\epsilon]$$

Combining, we get

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t); \upsilon \in \Hy_\epsilon] - \epsilon$$

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t)] -  \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t); \upsilon \not\in \Hy_\epsilon] - \epsilon$$

By definition of $\Hy_\epsilon$, this implies

$$\E_{\upsilon \sim \zeta}[\EU_{\upsilon_t}^{\pi^\zeta_t}(t)] \geq \E_{\upsilon \sim \zeta}[\max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t)] -  2\epsilon$$

For any $\upsilon \in \Hy$, we get

$$\EU_{\upsilon_t}^{\pi^\zeta_t}(t) \geq \max_{\pi \in \Pi} \EU_{\upsilon_t}^{\pi}(t) - \frac{2\epsilon}{\zeta(\upsilon)}$$

Taking $\epsilon$ to 0, we get the desired result.

\#Proof of Proposition 2

Let $\Hy = \{\upsilon^k\}_{k \in \Nats}$. For each $k \in \Nats$, let $\pi^k$ learn $\{\upsilon^l\}_{l < k}$. Choose $\{t^k \in (0,\infty)\}_{k \in \Nats}$ s.t.

\begin{enumerate}[i.]

\item $t^0 = 0$
\item $t^k < t^{k+1}$
\item $\lim_{k \rightarrow \infty} t^k = \infty$
\item For any $l < k$ and $t \geq t^k$, $\EU_{\upsilon^l_t}^{\pi^k_t}(t) \geq \max_{\pi \in \Pi} \EU_{\upsilon^l_t}^\pi(t) - \frac{1}{k+1}$.

Now define $\pi^*_t:=\pi_t^{\max\{k \mid t \geq t^k\}}$. Clearly, $\pi^*$ learns $\Hy$.

\end{enumerate}

\end{document}



