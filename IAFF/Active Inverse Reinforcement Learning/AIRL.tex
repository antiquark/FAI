%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Comment}[1]{}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}

\DeclareMathOperator{\Prj}{pr}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}

\newcommand{\Prob}{\mathcal{P}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Hy}{\mathcal{H}}

\newcommand{\Ut}{\operatorname{U}}

\begin{document}

We introduce a reinforcement-like learning setting we call *active inverse reinforcement learning* (AIRL). In AIRL, the agent can, at any point of time, defer the choice of action to an "advisor". The agent knows neither the environment nor the reward function, whereas the advisor knows both. Thus, AIRL can be regarded as a special case of CIRL. A similar setting was studied in [Clouse 1997](https://web.cs.umass.edu/publication/docs/1997/UM-CS-1997-026.pdf), but as far as we can tell, the relevant literature offers few theoretical results and virtually all researchers focus on the MDP case (*please correct me if I'm wrong*). On the other hand, we consider general environments (not necessarily MDP or even POMDP) and prove a natural performance guarantee.

The use of an advisor allows us to kill two birds with one stone: learning the reward function and safe exploration (i.e. avoiding both the Scylla of "[Bayesian paranoia](http://proceedings.mlr.press/v40/Leike15.pdf)" and the Charybdis of falling into traps). We prove that, given certain assumption about the advisor, a Bayesian AIRL agent (whose prior is supported on some countable set of hypotheses) is guaranteed to attain most of the value in the slow falling time discount (long-term planning) limit (assuming one of the hypotheses in the prior is true). The assumption about the advisor is quite strong, but the advisor is not required to be fully optimal: a "soft maximizer" satisfies the conditions. Moreover, we allow for the existence of "corrupt states" in which the advisor stops being a relevant signal, thus demonstrating that this approach can deal with wireheading and avoid manipulating the advisor, at least in principle (the assumption about the advisor is still unrealistically strong). Finally we consider advisors that don't know the environment but have some *beliefs* about the environment, and show that in this case the agent converges to Bayes-optimality w.r.t. the advisor's beliefs, which is arguably the best we can expect.

***

All the proofs are in the Appendix.

\section{Notation}

Whatever...

\section{Results}

We fix $\A$ and $\Ob$ finite sets ("actions" and "observations"). A *policy* is a mapping $\pi: (\A \times \Ob)^* \rightarrow \Delta\A$ and an *environment* is a mapping $\mu:(\A \times \Ob)^* \times \A \rightarrow \Delta\Ob$. A *reward function* is a mapping $r: (\A \times \Ob)^* \rightarrow [0,1]$. A *universe* is a pair $(\mu,r)$ where $\mu$ is an environment and $r$ is a reward function. We denote the space of universes by $\Upsilon$. Given a policy $\pi$ and an environment $\mu$, we get $\mu^\pi \in \Delta(\A \times \Ob)^\omega$ in the usual way. We will use the notation $\E_\mu^\pi := \E_{\mu^\pi}$ for expected values.

Given a reward function $r$ and $t \in (0,\infty)$, we have the associated *utility function* $\Ut_t^r: (A \times \Ob)^\omega \rightarrow [0,1]$ defined by

$$\Ut_t^{r}(h):=\frac{\sum_{n=0}^\infty e^{-n/t} r(h_{:n})}{\sum_{n=0}^\infty e^{-n/t}}$$

Here and throughout, we use geometric time discount, however this choice is mostly for notational simplicity. More or less all results carry over to other shapes of the time discount function.

A *metapolicy* is a family $\{\pi_t: (\A \times \Ob)^* \rightarrow \Delta\A\}_{t \in (0, \infty)}$, where the parameter $t$ is thought of as setting the scale of the time discount. A *meta-universe* is a family $\{\upsilon_t \in \Upsilon\}_{t \in (0, \infty)}$. This latter concept is useful for analyzing multi-agent systems, where the environment contains other agents and we study the asymptotics when all agents' time discount scales go to infinity. We won't focus on the multi-agent case in this essay, but for future reference, it seems useful to make sure the results hold in the greater generality of meta-universes.

Denote $\Pi:=\{(\A \times \Ob)^* \rightarrow \Delta\A\}$, the space of policies.

\#Definition 1

Consider $\pi^*$ a metapolicy and $\Hy$ a set of meta-universes. $\pi^*$ is said to *learn* $\Hy$ when for any $(\mu,r) \in \Hy$

$$\lim_{t \rightarrow \infty} (\max_{\pi \in \Pi} \E_{\mu_t}^{\pi}[\Ut_t^{r_t}] - \E_{\mu_t}^{\pi^*_t}[\Ut_t^{r_t}]) = 0$$

$\Hy$ is said to be *learnable* when there exists $\pi^*$ that learns $\Hy$.

\#Proposition 1

Consider $\Hy$ a countable learnable set of meta-universes. Consider any $\zeta \in \Delta\Hy$ s.t. $\Supp \zeta = \Hy$. Consider $\pi^\zeta$ a $\zeta$-Bayes optimal metapolicy, i.e.

$$\pi^\zeta_t \in \Argmax{\pi \in \Pi} \E_{(\mu,r) \sim \zeta}[\E_{\mu_t}^{\pi}[\Ut_t^{r_t}]]$$

Then, $\pi^\zeta$ learns $\Hy$.

***

Bar

\section{Appendix A}

\#Proof of Proposition 1

Fix $\pi^*$ a metapolicy that learns $\Hy$. Consider $\epsilon > 0$ and let $\Hy_\epsilon \subseteq \Hy$ be finite s.t. $\zeta(\Hy \setminus \Hy_\epsilon) < \epsilon$. For any $\delta > 0$ and $t \gg 0$, for every $(\mu,r) \in \Hy_\epsilon$ we have

$$\max_{\pi \in \Pi} \E_{\mu_t}^{\pi}[\Ut_t^{r_t}] \leq \E_{\mu_t}^{\pi^*_t}[\Ut_t^{r_t}] + \delta$$

$$\E_{(\mu,r) \sim \zeta}[\E_{\mu_t}^{\pi^\zeta_t}[\Ut_t^{r_t}]] \geq \E_{(\mu,r) \sim \zeta}[\E_{\mu_t}^{\pi^*_t}[\Ut_t^{r_t}]]$$

$$\E_{(\mu,r) \sim \zeta}[\E_{\mu_t}^{\pi^\zeta_t}[\Ut_t^{r_t}]] \geq \E_{(\mu,r) \sim \zeta}[\E_{\mu_t}^{\pi^*_t}[\Ut_t^{r_t}]; (\mu,r) \in \Hy_\epsilon] + \E_{(\mu,r) \sim \zeta}[\E_{\mu_t}^{\pi^*_t}[\Ut_t^{r_t}]; (\mu,r) \not\in \Hy_\epsilon]$$

\end{document}



