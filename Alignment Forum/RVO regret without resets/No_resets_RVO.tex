%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Co}[1]{}
% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Dom}{dom}
\DeclareMathOperator{\Sp}{span}
% autosize delimiters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}
\newcommand{\AC}[1]{\left\{#1\right\}}
\newcommand{\APM}[2]{\left(#1\;\middle\vert\;#2\right)}
\newcommand{\ABM}[2]{\left[#1\;\middle\vert\;#2\right]}
\newcommand{\ACM}[2]{\left\{#1\;\middle\vert\;#2\right\}}
% probability theory
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\CP}[3]{\underset{#1}{\operatorname{Pr}}\ABM{#2}{#3}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\CE}[3]{\underset{#1}{\operatorname{E}}\ABM{#2}{#3}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\CI}[3]{\underset{#1}{\operatorname{I}}\ABM{#2}{#3}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}\AP{#1\middle\vert\middle\vert#2}}
\newcommand{\RD}[3]{\operatorname{D}_{#1}\AP{#2\middle\vert\middle\vert#3}}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\text{tv}}\AP{#1}}
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
% power set
\newcommand{\PS}[1]{\mathcal{P}\AP{#1}}
% differential
\newcommand{\D}{\mathrm{d}}
% arg
\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}
% numbers
\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}
% empty string
\newcommand{\Estr}{\boldsymbol{\lambda}}
% limits
\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}
% more delimiters
\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\Floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\Chev}[1]{\left\langle #1 \right\rangle}
\newcommand{\Quote}[1]{\left\ulcorner #1 \right\urcorner}
% arrows
\newcommand{\K}{\xrightarrow{\mathrm{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}
% Paper specific
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\El}{\mathrm{L}}
\newcommand{\Hy}{\mathcal{H}}
\DeclareMathOperator{\RVO}{\dim_{RVO}}
\DeclareMathOperator{\MB}{\dim_{MB}}
\DeclareMathOperator{\LD}{\dim_{loc}}
\newcommand{\DRVO}{D_{\mathrm{RVO}}}
\newcommand{\DMB}{D_{\mathrm{MB}}}
\newcommand{\DL}{D_{\mathrm{loc}}}
\newcommand{\N}{\mathrm{N}}
\newcommand{\V}{\mathrm{V}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\EU}{\mathrm{EU}}
\newcommand{\Reg}{\mathrm{R}}
\newcommand{\PSR}{\text{PS}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\CS}{\mathrm{CS}}
\newcommand{\W}{\mathrm{W}}
\newcommand{\AT}{\mathrm{A}}
\newcommand{\THy}{\mathrm{H}_*}
\newcommand{\SHy}{\mathrm{H}}

\begin{document}

\textbf{TLDR:}\Co{b} I derive a variant of the RL regret bound by [Osband and Van Roy (2014)](https://arxiv.org/abs/1406.1853), that applies to learning without resets of environments without traps. The advantage of this regret bound over those known in the literature, is that it scales with certain learning-theoretic dimensions rather than number of states and actions. My goal is building on this result to derive this type of regret bound for [DRL](https://agentfoundations.org/item?id=1656), and, later on, other settings interesting from an AI\ alignment perspective.

***

[Previously](https://www.alignmentforum.org/posts/zTf946PQwN2AN3X3Y/entropic-regret-i-deterministic-mdps) I derived a regret bound for \textit{deterministic}\Co{i} environments that scales with prior entropy and "prediction dimension". That bound behaves as $O\AP{\sqrt{1-\gamma}}$ in the episodic setting but only as $O\AP{\sqrt[3]{1-\gamma}}$ in the setting without resets. Moreover, my attempts to generalize the result to stochastic environments led to bounds that are \textit{even weaker}\Co{i} (have a lower exponent). Therefore, I decided to put that line of attack on hold, and use Osband and Van Roy's technique instead, leading to an $O\AP{\sqrt{1-\gamma}}$ bound in the stochastic setting without resets. The main disadvantage is, this bound doesn't scale down with the entropy of the prior, but this does not seem as important as dependence on $\gamma$.

\begin{Huge}Results\end{Huge}

[Russo and Van Roy](https://papers.nips.cc/paper/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration) introduced the concept of "eluder dimension" (which we will call "eluder number") for the benefit of the multi-armed bandit setting, and Osband and Van Roy extended it to a form suitable for studying reinforcement learning. The following two definitions are adapted from the latter.

\textbf{Definition 1}\Co{b}

\textit{Consider a set $\X$, a real inner product space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and $\epsilon\in\Reals^+$. Consider also $n\in\Nats$, a sequence $\AC{x_k\in\X}_{k\in[n]}$ and $x^*\in\X$. $x^*$ is said to be} $\AP{\F,\epsilon}$-dependant on $\AC{x_k}$ \textit{when, for any $f,\tilde{f}\in\F$}\Co{i}

$$\sum_{k=0}^{n-1}\Norm{f\AP{x_k}-\tilde{f}\AP{x_k}}^2 \leq \epsilon^2 \implies \Norm{f\AP{x^*}-\tilde{f}\AP{x^*}}\leq\epsilon$$

\textit{Otherwise, $x^*$ is said to be}\Co{i} $\AP{\F,\epsilon}$-independent of $\AC{x_k}$.

\textbf{Definition 2}\Co{b}

\textit{Consider a set $\X$, a real inner product space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and $\epsilon\in\Reals^+$. The}\Co{i} eluder number $\El(\F,\epsilon)$ \textit{is the supremum of the set of $n\in\Nats$ for which there is $\AC{x_k\in\X}_{k\in[n]}$ and $\epsilon'\geq\epsilon$ s.t. for all $m\in[n]$, $x_m$ is $\AP{\F,\epsilon'}$-independent of $\AC{x_k\in\X}_{k\in[m]}$.}\Co{i}

% Is eluder dimension logarithmic for cellular automata?

As opposed to other learning-theoretic dimensions, the eluder number depends on the parameter $\epsilon$. However, in natural examples it seems to be always bounded by a logarithmic function of $\epsilon$. This motivates the following definition

\textbf{Definition 3}\Co{b}

\textit{Consider a set $\X$, a real inner product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. The}\Co{i} Russo-Van Roy-Osband dimension (RVO dimension) of $\F$ \textit{is defined by}\Co{i}

$$\RVO{\F}:=\limsup_{\epsilon \rightarrow 0}{\frac{\El(\F,\epsilon)}{\ln\frac{1}{\epsilon}}}$$

It is easy to see that, for finite $\F$, $\El(\F,\epsilon)\leq\Abs{\F}$ and in particular $\RVO\F=0$. Osband and Van Roy also show that, for finite $\X$ and finite-dimensional $\Y$

$$\RVO\F\leq \frac{2e}{e-1}\Abs{\X}\AP{4\dim{\Y}-1}$$

(See "Proposition 2" in their paper.) The bound on prediction dimension for "cellular decision processes" we demonstrated [before](https://www.alignmentforum.org/posts/zTf946PQwN2AN3X3Y/entropic-regret-i-deterministic-mdps) also carries over, since eluder number is trivially bounded by prediction dimension in the deterministic case. It would also be interesting to investigate stochastic versions of cellular decision processes, but we will not attempt it at present.

Another concept we need to formulate the regret bound is the Minkowski–Bouligand dimension.

\textbf{Definition 4}\Co{b}

\textit{Consider a set $\X$, a real inner product space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and $\epsilon\in\Reals^+$. A set $A\subseteq\F$ is said to be an}\Co{i} $\epsilon$-covering of $\F$ \textit{when}\Co{i}

$$\forall f\in\F\exists\tilde{f}\in A: \sup_{x\in\X}\Norm{f(x)-\tilde{f}(x)}<\epsilon$$

Note that we always consider the covering number of function families w.r.t. the $\sup$ norm.

\textbf{Definition 5}\Co{b}

\textit{Consider a set $\X$, a real inner product space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and $\epsilon\in\Reals^+$.\ The}\Co{i} covering number $\N(\F,\epsilon)$ \textit{is the infimum of the set of $n\in\Nats$ for which there is an $\epsilon$-covering of $\F$ of size $n$.}\Co{i}

\textbf{Definition 6}\Co{b}

\textit{Consider a set $\X$, a real inner product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. The}\Co{i} Minkowski–Bouligand dimension (MB dimension) of $\F$ \textit{is defined by}\Co{i}

$$\MB{\F}:=\limsup_{\epsilon \rightarrow 0}{\frac{\ln{\N(\F,\epsilon)}}{\ln\frac{1}{\epsilon}}}$$

For finite $\F$ and $\epsilon\ll1$, it's obvious that $\N(\F,\epsilon)=\Abs{\F}$, and in particular $\MB\F=0$. It is also possible to show that, for finite $\X$ and finite-dimensional $\Y$, $\MB{\F}\leq\Abs{\X}\dim{\Y}$.

Note that, in general, RVO and MB dimensions are fractional.

Finally, we need yet another (but rather simple) notion of "dimension".

\textbf{Definition 7}\Co{b}

\textit{Consider a set $\X$, a vector space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. The}\Co{i} local dimension of $\F$ \textit{is defined by}\Co{i}

$$\LD{\F}:=\max_{x\in\X}{\dim\Sp\ACM{f(x)}{f\in\F}}$$

Obviously $\LD{\F}\leq\Abs{\F}$ and $\LD{\F}\leq\dim{\Y}$.

Consider finite non-empty sets $\St$ (states) and $\A$ (actions). Observe that $\Delta\St$ can be regarded as a subset of the vector space $\Reals^\St$ and the later can be equipped with the inner product $\Chev{v,w}:=\sum_{s\in\St} v_s w_s$. This allows us to speak of the RVO, MB and local dimensions of a hypothesis class of transition kernels $\Hy\subseteq\AC{\St\times\A\K\St}$ (that is, in this case $\X=\St\times\A$ and $\Y=\Reals^\St$). We can now formulate the regret bound.

\textbf{Theorem 1}\Co{b}

\textit{There is some $C\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider any finite non-empty sets $\St$ and $\A$, $\R:\St\rightarrow[0,1]$ (reward function), closed set $\Hy\subseteq\AC{\St\times\A\K\St}$ and Borel probability measure $\zeta$ on $\Hy$ (prior). Assume that for any $\T\in\Hy$ and $s,s'\in\St$, $\V_{\T\R}^0(s)=\V_{\T\R}^0\AP{s'}.$}\Co{i} [i.e. there are no traps. This is a stronger condition than the condition $\A^0_{\T\R}(s) = \A$ we used before: not only that long-term value cannot be lost \textit{in expectation}\Co{i}, it cannot be lost at all.] \textit{We define $\tau_{\zeta\R}$ by}\Co{i}

$$\tau_{\zeta\R}:=\Ea{\T\sim\zeta}{\max_{s\in\St}\Abs{\V^1_{\T\R}(s)}}$$

\textit{Denote $\DL:=\LD{\Hy}$, $\DMB:=\MB{\Hy}$ and $\DRVO:=\RVO{\Hy}$. Then, there is a family of policies $\AC{\pi^\dagger_\gamma:\St^*\times\St\K\A}_{\gamma\in(0,1)}$ s.t. for any $\epsilon\in\Reals^+$}\Co{i}

$$\limsup_{\gamma \rightarrow 1}\frac{\Ea{\T\sim\zeta}{\EU^*_{\T\R}(\gamma)-\EU^{\pi^\dagger_\gamma}_{\T\R}(\gamma)}}{\tau_{\zeta\R}\AP{\DL-1}\ln{\frac{1}{1-\gamma}}\sqrt{\AP{\DMB+\epsilon}\AP{\DRVO+\epsilon}(1-\gamma)}}\leq C$$

Here (like in previous essays), $\V^1_{\T\R}(s)$ is the first derivative of the value function at $\gamma=1$ for transition kernel $\T$, reward function $\R$ and state $s$; $\EU^{\pi}_{\T\R}(\gamma)$ is the expected utility for policy $\pi$ and geometric time discount parameter $\gamma$; $\EU^*_{\T\R}(\gamma)$ is the maximal expected utility over policies. The expression in the numerator is, thereby, the Bayesian regret.

A few directions for improving on this result:

* It is not hard to see from the proof that it is also possible to write down a concrete bound for fixed $\gamma$ (rather than considering the $\gamma\rightarrow 1$ limit) using the covering and eluder numbers, but its form is somewhat convoluted.

* It is probably possible to get an *anytime* policy with this form of regret, using PSRL with \textit{dynamic}\Co{i} episode duration.

* It is interesting to try to make do with the weaker no-traps condition $\A^0_{\T\R}(s) = \A$, especially since a stronger no-traps conditions would translate to a stronger condition on the advisor in DRL.

* Like I said in the start, I was unable to derive a satisfactory "entropic" regret bound using RVO dimension. However, at the time I did not try to use local dimension: its significance only became apparent to me when working on the current approach.

* It seems tempting to generalize local dimension by allowing the values of $f(x)$ to lie on some \textit{nonlinear} manifold of given dimension for any given $x$. This approach, if workable, might require a substantially more difficult proof.

\begin{Huge}Proof\end{Huge}

The proof follows Osband and Van Roy rather closely. The main differences are:

* No resets (at the price of a "no traps" assumption)

* Geometric time discount instead of step function (finite horizon) time discount

* We only consider finite MDPs

* Instead of assuming a certain value function is Lipschitz (this assumption plays a role even in the discrete special case since the Lipschitz constant appears in their regret bound), we use the bound on $\V^1$ that we need in the setting without resets anyway. ??? maybe it's not "anyway" but it *is* the only way we do episode decoupling ???

\textbf{Proposition A.N1}\Co{b}

In the setting of Theorem 1, fix $T\in\Nats^+$ and let $\pi_{\zeta\R T}^{\PSR}: \St^*\times\St\K\A$ be the policy implemented by a PSRL algorithm with prior $\zeta$, reward function $\R$ and episode length $T$. Let $(\Omega,P)$ be a probability space governing both the uncertainty about the true hypothesis, the stochastic behavior of the environment and the random sampling inside the algorithm (see the proof of "Lemma 1" in [this](https://www.alignmentforum.org/posts/zTf946PQwN2AN3X3Y/entropic-regret-i-deterministic-mdps) previous essay or the proof of "Theorem 1" in [another](https://agentfoundations.org/item?id=1739) previous essay). Furthermore, let $\THy:\Omega\rightarrow\Hy$ be a random variable representing the true hypothesis, $\AC{\SHy_n:\Omega\rightarrow\Hy}_{n\in\Nats}$ be the random variables s.t. $\SHy_n$ represents the hypothesis sampled at time $n$ (i.e. during episode number $\Floor{n/T}$), $\AC{\Theta_n:\Omega\rightarrow\St}_{n\in\Nats}$ be random variables s.t. $\Theta_n$ represents the state at time $n$ and $\AC{\AT_n:\Omega\rightarrow\A}_{n\in\Nats}$ be random variables s.t. $\AT_n$ represents the action taken at time $n$. Then

$$\Ea{\T\sim\zeta}{\EU^*_{\T\R}(\gamma)-\EU^{\pi_{\zeta\R T}^{\PSR}}_{\T\R}(\gamma)}=\sum_{n=0}^\infty\gamma^{n}\Ea{}{\Ea{s\sim \SHy_n\AP{\Theta_n,\AT_n}}{\V_{\SHy_n\R}(s,\gamma)}-\Ea{s\sim \THy\AP{\Theta_n,\AT_n}}{\V_{\SHy_n\R}(s,\gamma)}}$$

\textbf{Proof of Proposition A.N1}\Co{b}

% Proof uses Proposition B.1 from https://agentfoundations.org/item?id=1723

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Proposition A.N6}\Co{b}

% Existence of quadratic form
\textit{Consider a real finite-dimensional normed vector space $V$ and a linear subspace $W\subseteq V$. Then, there exists a positive semidefinite symmetric bilinear form $Q: V \times V \rightarrow \Reals$ s.t.}\Co{i}

1. \textit{For any $v\in V$, $Q(v,v)\leq\Norm{v}^2$}\Co{i}

2. \textit{For any $w\in W$, $\AP{\dim{W}}^2 Q(w,w)\geq \Norm{w}^2$}\Co{i}

\textbf{Proof of Proposition A.N6}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Proposition A.N4}\Co{b}

!!! Fix formulation: any bounded distribution !!!

\textit{Consider a finite set $\St$ and some $\mu\in\Delta\St$. Let $\sigma=3$ and define $\iota:\St\rightarrow\Reals^\St$ by $\iota(s)_t:=[[s=t]]$. Then, $\iota_*\mu$ is $\sigma$-sub-Gaussian, i.e., for any $\lambda\in\Reals^\St$}\Co{i}

$$\Ea{y\sim\iota_*\mu}{\exp\AP{\lambda\cdot\AP{y-\mu}}} \leq \exp\AP{\frac{\sigma^2\Norm{\lambda}^2}{2}}$$

\textit{Here, $\mu$ is regarded as a vector in $\Reals^\St$ in the obvious way. Note that $\mu=\Ea{y\sim\iota_*\mu}{y}$.}\Co{i}

\textbf{Proof of Proposition A.N4}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Definition A.N1}\Co{b}

!!! Fix formulation: arbitrary quadratic form !!!

\textit{Consider a set $\X$, a real inner product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Assume $\F$ is compact w.r.t. the product topology on $\X\rightarrow\Y\cong\prod_{x\in\X}\Y$, where $\Y$ is equipped with the topology induced by the inner product. Consider also some $n\in\Nats$, $\bold{x}\in\X^n$, $\bold{y}\in\Y^n$ and $r\in\Reals^+$. We then use the notation}\Co{i}

$$\LS^\F[\bold{xy}]:=\Argmin{f\in\F}{\sum_{m=0}^{n-1}\Norm{f\AP{\bold{x}_m}-\bold{y}_m}^2}$$

$$\CS^\F_{r}[\bold{xy}]:=\ACM{f\in\F}{\sum_{m=0}^{n-1}\Norm{f\AP{\bold{x}_m}-\LS^\F[\bold{xy}]\AP{\bold{x}_m}}^2\leq r^2}$$

I chose the notation $\LS$ as an abbreviation of "least squares" and $\CS$ as an abbreviation of "confidence set". Note that $\LS$ is somewhat ambiguous (and therefore, so is $\CS$) since there might be multiple minima, but this will not be important in the following (i.e. an arbitrary minimum can be chosen).

\textbf{Proposition A.N5}\Co{b}

!!! Split: canonical form vs. general form !!!

\textit{There is some $C_{\mathrm{A.N5}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider finite sets $\X,\St$ and some $\F\subseteq\AC{\X\K\St}$. Let $\AC{\mathfrak{H}_n\subseteq\PS{\X^\omega\times\St^\omega}}_{n\in\Nats}$ be the natural filtration, i.e.}\Co{i}

$$\mathfrak{H}_n:=\ACM{A'\subseteq\X^\omega\times\St^\omega}{A'=\ACM{\bold{xs}}{\bold{xs}_{:n}\in A},\ A\subseteq\X^n\times\St^n}$$

\textit{Consider also $f^*\in\F$ and $\mu\in\Delta\AP{\X^\omega\times\St^\omega}$ s.t. for any $n\in\Nats$ $x\in\X$, and $s\in\St$}\Co{i}

$$\CP{\bold{xs}\sim\mu}{\bold{s}_n=s}{\bold{x}_n=x,\ \mathfrak{H}_n} = f^*(s\mid x)$$

\textit{Fix $\epsilon\in\Reals^+$, $\delta\in(0,1)$. Denote}\Co{i}

$$\beta(t):=C_{\mathrm{A.N5}}\AP{\ln{\frac{\N(\F,\epsilon)}{\delta}}+\epsilon t\ln{\frac{et}{\delta}}}$$

\textit{Then,}\Co{i}

$$\Pa{\bold{xs}\sim\mu}{f^*\not\in\bigcap_{n=0}^\infty\CS^\F_{\sqrt{\beta(n+1)}}\AB{\bold{xs}_{:n}}} \leq \delta$$

\textit{Here, the $\CS$ is defined by thinking of $\St$ and $\Delta\St$ as subsets of $\Y=\Reals^\St$.}\Co{i}

\textbf{Proof of Proposition A.N5}\Co{b}

% A.N4 + B.N2
TBD \textbf{Q.E.D.}\Co{b}

\textbf{Definition A.N2}\Co{b}

!!! Fix formulation: arbitrary quadratic form !!!

\textit{Consider a set $\X$, some $x\in\X$, a real inner product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. The}\Co{i} width of $\F$ at $x$ \textit{is defined by}\Co{i}

$$\W^\F(x):=\sup_{f,\tilde{f}\in\F}\Norm{f(x)-\tilde{f}(x)}$$  

\textbf{Proposition A.N2}\Co{b}

\textit{There is some $C_{\mathrm{A.N2}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider a set $\X$, a real inner product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Consider also some $\bold{x}\in\X^\omega$, $\bold{y}\in\Y^\omega$, $T\in\Nats^+$, $\gamma\in(0,1)$, $\theta\in\Reals^+$, $\eta_0,\eta_1\in\Reals^+$ and $\delta\in(0,1)$. For any $t\in\Reals^+$, define $\beta(t)$ by}\Co{i}

$$\beta(t):=\eta_0 + \eta_1t\ln{\frac{et}{\delta}}$$

For any $n\in\Nats$, define $\F_n$ by

$$\F_n:=\CS^\F_{\sqrt{\beta(n+1)}}[\bold{xy}_{:n}]$$

Then,

$$\sum_{l=0}^\infty\sum_{m=0}^{T-1}{\gamma^{lT+m}[[\W^{F_{lT}}\AP{\bold{x}_{lT+m}}>\theta]]} \leq C_{\mathrm{A.N2}}\RVO{\F}\cdot\AP{\theta^{-2}\beta\AP{\frac{1}{1-\gamma}}+T}\ln{\frac{1}{\theta}}$$

\textbf{Proof of Proposition A.N2}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Proposition A.N3}\Co{b}

!!! Split: canonical form vs. general form !!!

\textit{There is some $C_{\mathrm{A.N3}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider a set $\X$, a real inner product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Assume that for any $x\in\X$ and $f\in\F$, $\Norm{f(x)}\leq 1$. Consider also some $\bold{x}\in\X^\omega$, $\bold{y}\in\Y^\omega$, $T\in\Nats^+$, $\gamma\in(0,1)$, $\eta_0,\eta_1\in\Reals^+$ and $\delta\in(0,1)$. Define $\beta$ and $\F_n$ the same way as in Proposition A.N2. Denote $D:=\RVO{\F}$. Then,}\Co{i}

$$\sum_{l=0}^\infty\sum_{m=0}^{T-1}{\gamma^{lT+m}\W^{F_{lT}}\AP{\bold{x}_{lT+m}}} \leq C_{\mathrm{A.N3}}\AP{1+D T\ln\frac{1}{1-\gamma}+\sqrt{D\beta\AP{\frac{1}{1-\gamma}}\frac{1}{1-\gamma}\ln{\frac{1}{1-\gamma}}}}$$

\textbf{Proof of Proposition A.N3}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Proof of Theorem 1}\Co{b}

We take $\pi^\dagger_\gamma:=\pi^\PSR_{\zeta\R T}$ for some $T\in\Nats^+$ to be specified later. We denote the Bayesian regret by

$$\Reg(\gamma):=\Ea{\T\sim\zeta}{\EU^*_{\T\R}(\gamma)-\EU^{\pi_{\gamma}^{\dagger}}_{\T\R}(\gamma)}$$

By Proposition A.N1

$$\Reg(\gamma)=\sum_{n=0}^\infty\gamma^{n}\Ea{}{\Q_{J_{\Floor{n/T}}\R}\AP{\Theta_n,A_n,\gamma}-\Q_{KR}\AP{\Theta_n,A_n,\gamma}}???$$

By definition of $\Q$, it follows that

$$\Reg(\gamma)=\sum_{n=0}^\infty\gamma^{n+1}\Ea{}{?-?}$$

% Use \tau and the no-traps condition

% Apply A.N5 (confidence)
 
% Balance \delta = 1-\gamma
 
% Apply A.N3 (widths)

% Balance \epsilon = (1-\gamma)^2

\textbf{Q.E.D.}\Co{b}

\begin{Huge}Appendix\end{Huge}

The proposition below appeared in Osband and Van Roy as "Lemma 1", so we state it without proof.

\textbf{Proposition B.N1}\Co{b}

\textit{TBD}\Co{i}

% episode decoupling seems to be redundant, it is already taken care of using the "Lipschitz constant"
\Co{...

\textbf{Proposition B.N3}\Co{b}

Consider some $\gamma\in(0,1)$, $\tau\in(0,\infty)$, $T\in\Nats^+$, a universe..., some $\pi^*: ? \rightarrow \A$ and some $\pi^0: ? \K \A$. Assume that $\gamma \geq \gamma_M$. For any $n \in \Nats$, let $\pi^*_n$ be a policy s.t. for any $h \in ?$

$$\pi^*_n(h):=\begin{cases} \pi^0(h) \text{ if } \Abs{h} < nT \\ \pi^*(h) \text{ otherwise} \end{cases}$$

Assume that for any $h \in ?$

i. $$\pi^*(s) \in \A_{M}^\omega\AP{S(h)}$$

ii. $$\Supp{\pi^0(h)} \subseteq \A_{M}^0\AP{S(h)}$$

iii. For any $\theta\in(\gamma,1)$ $$\Abs{\frac{\D\V_{M}\AP{S(h),\theta}}{\D\theta}} \leq \tau$$

Then

$$\EU^{*}_\upsilon(\gamma)-\EU^{\pi^0}_\upsilon(\gamma) \leq (1-\gamma)\sum_{n=0}^\infty \sum_{m=0}^{T-1} \gamma^{nT+m}\left(\E{x\sim\mu\bowtie\pi^*_n}\left[r\left(x_{:nT+m}\right)\right]-\E{x\sim\mu\bowtie\pi^0}\left[r\left(x_{:nT+m}\right)\right]\right) + \frac{2\tau\gamma^T(1-\gamma)}{1-\gamma^T}$$}

The next proposition appeared (in slightly greater generality) in Osband and Van Roy as "Proposition 5".

\textbf{Proposition B.N2}\Co{b}

\textit{There is some $C_{\mathrm{B.N2}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider finite set $\X$, finite-dimensional real inner product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Let $\AC{\mathfrak{H}_n\subseteq\PS{\X^\omega\times\Y^\omega}}_{n\in\Nats}$ be the natural filtration, i.e.}\Co{i}

$$\mathfrak{H}_n:=\ACM{A'\subseteq\X^\omega\times\Y^\omega}{A'=\ACM{\bold{xy}}{\bold{xy}_{:n}\in A},\ A\subseteq\X^n\times\Y^n\text{ Borel}}$$

\textit{Consider also $f^*\in\F$ and $\mu\in\Delta\AP{\X^\omega\times\Y^\omega}$ s.t. for any $n\in\Nats$ and $x\in\X$}\Co{i}

$$\CE{\bold{xy}\sim\mu}{\bold{y}_n}{\bold{x}_n=x,\ \mathfrak{H}_n} = f^*(x)$$

\textit{Assume that $M\in\Reals^+$ is s.t. $\Norm{\bold{y}_n}\leq M$ with $\mu$-probability $1$ for all $n\in\Nats$. Assume also that $\sigma\in\Reals^+$ is s.t. $\mu$ is $\sigma$-sub-Gaussian, i.e., for any $\lambda\in\Y$, $n\in\Nats$ and $x\in\X$}\Co{i}

$$\CE{\bold{xy}\sim\mu}{\exp\AP{\Chev{\lambda,\ \bold{y}_n-f^*(x)}}}{\bold{x}_n=x,\ \mathfrak{H}_n} \leq \exp\AP{\frac{\sigma^2\Norm{\lambda}^2}{2}}$$

\textit{Fix $\epsilon\in\Reals^+$, $\delta\in(0,1)$. Denote}\Co{i}

$$\beta(t):=C_{\mathrm{B.N2}}\AP{\sigma^2 \ln{\frac{\N(\F,\epsilon)}{\delta}}+\epsilon t\AP{M+\sigma\ln{\frac{et}{\delta}}}}$$

\textit{Then,}\Co{i}

$$\Pa{\bold{xy}\sim\mu}{f^*\not\in\bigcap_{n=0}^\infty\CS^\F_{\sqrt{\beta(n+1)}}\AB{\bold{xy}_{:n}}} \leq \delta$$

Note that we removed a square root in the definition of $\beta$ compared to equation (7) in Osband and Van Roy. This only makes $\beta$ larger (up to a constant factor) and therefore, only makes the claim weaker.

\end{document}


