%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Co}[1]{}
% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Dom}{dom}
\DeclareMathOperator{\Sp}{span}
% autosize delimiters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}
\newcommand{\AC}[1]{\left\{#1\right\}}
\newcommand{\APM}[2]{\left(#1\;\middle\vert\;#2\right)}
\newcommand{\ABM}[2]{\left[#1\;\middle\vert\;#2\right]}
\newcommand{\ACM}[2]{\left\{#1\;\middle\vert\;#2\right\}}
% probability theory
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\CP}[3]{\underset{#1}{\operatorname{Pr}}\ABM{#2}{#3}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\CE}[3]{\underset{#1}{\operatorname{E}}\ABM{#2}{#3}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\Var}[2]{\underset{#1}{\operatorname{Var}}\AB{#2}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\CI}[3]{\underset{#1}{\operatorname{I}}\ABM{#2}{#3}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}\AP{#1\middle\vert\middle\vert#2}}
\newcommand{\RD}[3]{\operatorname{D}_{#1}\AP{#2\middle\vert\middle\vert#3}}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\text{tv}}\AP{#1}}
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
% power set
\newcommand{\PS}[1]{\mathcal{P}\AP{#1}}
% differential
\newcommand{\D}{\mathrm{d}}
% arg
\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}
% numbers
\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}
% linear algebra
\newcommand{\Id}{\mathrm{I}}
\newcommand{\PD}{\mathrm{PD}}
\newcommand{\PSD}{\mathrm{PSD}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\End}{\mathrm{End}}
\DeclareMathOperator{\Img}{im}
% empty string
\newcommand{\Estr}{\boldsymbol{\lambda}}
% limits
\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}
% more delimiters
\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\Floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\Chev}[1]{\left\langle #1 \right\rangle}
\newcommand{\Quote}[1]{\left\ulcorner #1 \right\urcorner}
% arrows
\newcommand{\K}{\xrightarrow{\mathrm{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}
% Paper specific
\newcommand{\B}{B}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\El}{\mathrm{L}}
\newcommand{\Hy}{\mathcal{H}}
\DeclareMathOperator{\RVO}{\dim_{RVO}}
\DeclareMathOperator{\MB}{\dim_{MB}}
\DeclareMathOperator{\LD}{\dim_{loc}}
\newcommand{\DRVO}{D_{\mathrm{RVO}}}
\newcommand{\DMB}{D_{\mathrm{MB}}}
\newcommand{\DL}{D_{\mathrm{loc}}}
\newcommand{\N}{\mathrm{N}}
\newcommand{\V}{\mathrm{V}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\EU}{\mathrm{EU}}
\newcommand{\Reg}{\mathrm{R}}
\newcommand{\PSR}{\text{PS}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\CS}{\mathrm{CS}}
\newcommand{\W}{\mathrm{W}}
\newcommand{\AT}{\mathrm{A}}
\newcommand{\THy}{\mathrm{H}_*}
\newcommand{\SHy}{\mathrm{H}}
\newcommand{\Ev}{\mathcal{E}}
\newcommand{\De}{\Delta}
\newcommand{\CSE}{G}

\begin{document}

\textbf{TLDR:}\Co{b} I derive a variant of the RL regret bound by [Osband and Van Roy (2014)](https://arxiv.org/abs/1406.1853), that applies to learning without resets of environments without traps. The advantage of this regret bound over those known in the literature, is that it scales with certain learning-theoretic dimensions rather than number of states and actions. My goal is building on this result to derive this type of regret bound for [DRL](https://agentfoundations.org/item?id=1656), and, later on, other settings interesting from an AI\ alignment perspective.

***

[Previously](https://www.alignmentforum.org/posts/zTf946PQwN2AN3X3Y/entropic-regret-i-deterministic-mdps) I derived a regret bound for \textit{deterministic}\Co{i} environments that scales with prior entropy and "prediction dimension". That bound behaves as $O\AP{\sqrt{1-\gamma}}$ in the episodic setting but only as $O\AP{\sqrt[3]{1-\gamma}}$ in the setting without resets. Moreover, my attempts to generalize the result to stochastic environments led to bounds that are \textit{even weaker}\Co{i} (have a lower exponent). Therefore, I decided to put that line of attack on hold, and use Osband and Van Roy's technique instead, leading to an $O\AP{\sqrt{1-\gamma}}$ bound in the stochastic setting without resets. This bound doesn't scale down with the entropy of the prior, but this does not seem as important as dependence on $\gamma$.

\begin{Huge}Results\end{Huge}

[Russo and Van Roy (2013)](https://papers.nips.cc/paper/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration) introduced the concept of "eluder dimension"  for the benefit of the multi-armed bandit setting, and Osband and Van Roy extended it to a form suitable for studying reinforcement learning. We will consider the following, slightly modified version of their definition.

Given a real vector space $\Y$, we will use $\PSD(\Y)$ to denote the set of positive semidefinite bilinear forms on $\Y$ and $\PD(\Y)$ to denote the set of positive definite bilinear forms on $\Y$. Given a bilinear form $\B:\Y\times\Y\rightarrow\Reals$, we will slightly abuse notation by also regarding it as a linear functional $\B:\Y\otimes\Y\rightarrow\Reals$. Thereby, we have $\B(v \otimes w) = \B(v, w)$ and $\B v^{\otimes 2}=\B (v,v)$. Also, if $\Y$ is finite-dimensional and $\B$ is non-degenerate, we will denote $\B ^{-1}:\Y^*\times\Y^*\rightarrow\Reals$ the unique bilinear form which satisfies

$$\forall v\in\Y,\alpha\in\Y^*:\AP{\forall\beta\in\Y^*:\B ^{-1}(\alpha,\beta)=\beta v}\iff\AP{\forall w\in\Y:\B (v,w)=\alpha w}$$ 

\textbf{Definition 1}\Co{b}

\textit{Consider a set $\X$, a real vector space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and a family $\AC{\B _x\in\PSD(\Y)}_{x\in\X}$. Consider also $n\in\Nats$, a sequence $\AC{x_k\in\X}_{k\in[n]}$ and $x^*\in\X$. $x^*$ is said to be} $\AP{\F,\B }$-dependant on $\AC{x_k}$ \textit{when, for any $f,\tilde{f}\in\F$}\Co{i}

$$\sum_{k=0}^{n-1}\B _{x_k}\AP{f\AP{x_k}-\tilde{f}\AP{x_k}}^{\otimes 2}\leq1\implies \B _{x^*}\AP{f\AP{x^*}-\tilde{f}\AP{x^*}}^{\otimes 2}\leq1$$

\textit{Otherwise, $x^*$ is said to be}\Co{i} $\AP{\F,\B }$-independent of $\AC{x_k}$.

\textbf{Definition 2}\Co{b}

\textit{Consider a set $\X$, a real vector space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. The}\Co{i} Russo-Van Roy-Osband dimension (RVO dimension) $\RVO{\F}$ \textit{is the supremum of the set of $n\in\Nats$ for which there is $\AC{x_k\in\X}_{k\in[n]}$ and $\B $ s.t. for all $m\in[n]$, $x_m$ is $\AP{\F,\B }$-independent of $\AC{x_k\in\X}_{k\in[m]}$.}\Co{i}

We have the following basic bounds on RVO dimension.

\textbf{Proposition 1}\Co{b}

\textit{Consider a set $\X$, a real vector space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Then}\Co{i}

$$\RVO{\F}\leq\Abs{\X}$$

\textbf{Proposition 2}\Co{b}

\textit{Consider a set $\X$, a real vector space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Then}\Co{i}

$$\RVO{\F}\leq\frac{\Abs{\F}\AP{\Abs{\F}-1}}{2}$$

Another concept we need to formulate the regret bound is the Minkowski–Bouligand dimension.

\textbf{Definition 3}\Co{b}

\textit{Consider a set $\X$, a real vector space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and a family $\AC{\B _x\in\PSD(\Y)}_{x\in\X}$. A set $A\subseteq\F$ is said to be a}\Co{i} $\B $-covering of $\F$ \textit{when}\Co{i}

$$\forall f\in\F\exists\tilde{f}\in A: \sup_{x\in\X}{\B _x\AP{f(x)-\tilde{f}(x)}^{\otimes 2}}<1$$

\textbf{Definition 4}\Co{b}

\textit{Consider a set $\X$, a real vector space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and a family $\AC{\B _x\in\PSD(\Y)}_{x\in\X}$. The}\Co{i} covering number $\N(\F,\B )$ \textit{is the infimum of the set of $n\in\Nats$ for which there is a $\B $-covering of $\F$ of size $n$.}\Co{i}

\textbf{Definition 5}\Co{b}

\textit{Consider a finite set $\X$, a finite-dimensional real vector product space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Fix any $\AC{\B_x \in\PD(\Y)}_{x\in\X}$.  The}\Co{i} Minkowski–Bouligand dimension (MB dimension) of $\F$ \textit{is defined by}\Co{i}

$$\MB{\F}:=\limsup_{\epsilon \rightarrow 0}{\frac{\ln{\N\AP{\F,\epsilon^{-2} \B }}}{\ln\frac{1}{\epsilon}}}$$

It is easy to see the above is indeed well-defined, i.e. doesn't depend on the choice of $\B $. This is because given any $\B ,\tilde{\B }$, there are constants $c_1,c_2\in\Reals^+$ s.t. for all $\F$ and $\epsilon$

$$c_1 \N\AP{\F,\epsilon^{-2} \B } \leq \N\AP{\F,\epsilon^{-2} \tilde{\B }} \leq c_2\N\AP{\F,\epsilon^{-2} \B }$$

Similarly, for any $\AC{B_x\in\PSD(\Y)}_{x\in\X}$, we have

$$\MB{\F}\geq\limsup_{\epsilon \rightarrow 0}{\frac{\ln{\N\AP{\F,\epsilon^{-2} \B }}}{\ln\frac{1}{\epsilon}}}$$

For finite $\F$ and $\epsilon\ll1$, it's obvious that $\N\AP{\F,\epsilon^{-2}\B }\leq\Abs{\F}$, and in particular $\MB\F=0$. It is also possible to show that, for any $\F$, $\MB{\F}\leq\Abs{\X}\dim{\Y}$.

Note that, in general, MB dimension is fractional.

We will need yet another (but rather simple) notion of "dimension".

\textbf{Definition 6}\Co{b}

\textit{Consider a set $\X$, a vector space $\Y$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. The}\Co{i} local dimension of $\F$ \textit{is defined by}\Co{i}

$$\LD{\F}:=\max_{x\in\X}{\dim\Sp\ACM{f(x)}{f\in\F}}$$

Obviously $\LD{\F}\leq\Abs{\F}$ and $\LD{\F}\leq\dim{\Y}$.

Consider finite non-empty sets $\St$ (states) and $\A$ (actions). Observe that $\Delta\St$ can be regarded as a subset of the vector space $\Reals^\St$. This allows us to speak of the RVO, MB and local dimensions of a hypothesis class of transition kernels $\Hy\subseteq\AC{\St\times\A\K\St}$ (that is, in this case $\X=\St\times\A$ and $\Y=\Reals^\St$). 

\Co{Finally, there is a certain regularity condition in our main theorem that depends on the following parameters of MDPs.

\textbf{Definition 7}\Co{b}

\textit{Consider finite non-empty sets $\St$ and $\A$, $\R:\St\rightarrow[0,1]$ (reward function) and $\T:\St\times\A\K\St$ (transition kernel). The value function $\V_{\T\R}(s,\gamma)$ is always continuous and piecewise differentiable w.r.t. $\gamma$. Let $\partial_+\V_{\T\R}(s,\gamma)$ and $\partial_-\V_{\T\R}(s,\gamma)$ denote its right and left derivative respectively. We denote}\Co{i}

$$\partial_{\max}\V_{\T\R}(s,\gamma):=\sup_{x\in(\gamma,1)}{\partial_+\V_{\T\R}(s,x)}$$

$$\partial_{\min}\V_{\T\R}(s,\gamma):=\inf_{x\in(\gamma,1)}{\partial_-\V_{\T\R}(s,x)}$$

Note that, since $\V_{\T\R}(s,\gamma)$ is the maximum of a finite set of differentiable functions (the value functions corresponding to the deterministic stationary policies $\St\rightarrow\A$), we have $\partial_+\V_{\T\R}(s,\gamma)\geq\partial_-\V_{\T\R}(s,\gamma)$.}

We can now formulate the regret bound.

\textbf{Theorem 1}\Co{b}

\textit{There is some $C\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider any finite non-empty sets $\St$ and $\A$, $\R:\St\rightarrow[0,1]$, closed set $\Hy\subseteq\AC{\St\times\A\K\St}$ and Borel probability measure $\zeta$ on $\Hy$ (prior). We define the maximal bias span $\tau_{\zeta\R}$ by}\Co{i}

$$\tau_{\zeta\R}:=\limsup_{\gamma\rightarrow1}{\max_{\T\in\Hy}\frac{\max_{s\in\St}\V_{\T\R}(s,\gamma)-\min_{s\in\St}\V_{\T\R}(s,\gamma)}{1-\gamma}}$$

\textit{Denote $\DL:=\LD{\Hy}$, $\DMB:=\MB{\Hy}$ and $\DRVO:=\RVO{\Hy}$. Then, there is a family of policies $\AC{\pi^\dagger_\gamma:\St^*\times\St\K\A}_{\gamma\in(0,1)}$ s.t.}\Co{i}

$$\limsup_{\gamma \rightarrow 1}\frac{\Ea{\T\sim\zeta}{\EU^*_{\T\R}(\gamma)-\EU^{\pi^\dagger_\gamma}_{\T\R}(\gamma)}}{\tau_{\zeta\R}\DL\sqrt{\AP{\DMB+1}\DRVO(1-\gamma)\ln{\frac{1}{1-\gamma}}}}\leq C$$

Here (like in previous essays), $\V_{\T\R}(s,\gamma)$ is the value function for transition kernel $\T$, reward function $\R$, state $s$ and geometric time discount parameter $\gamma$; $\EU^{\pi}_{\T\R}(\gamma)$ is the expected utility for policy $\pi$; $\EU^*_{\T\R}(\gamma)$ is the maximal expected utility over policies. The expression in the numerator is, thereby, the Bayesian regret.

The implicit condition $\tau_{\zeta\R}<\infty$ implies that for any $\T\in\Hy$ and $s,s'\in\St$, $\V_{\T\R}^0(s)=\V_{\T\R}^0\AP{s'}$. This is a no-traps condition stronger than the condition $\A^0_{\T\R}(s) = \A$ we used before: not only that long-term value cannot be lost \textit{in expectation}\Co{i}, it cannot be lost at all. For any \textit{finite}\Co{i} $\Hy$ satisfying this no-traps condition, we have

$$\tau_{\zeta\R}=\max_{\T\in\Hy}\AP{\max_{s\in\St}{\V^1_{\T\R}(s)}-\min_{s\in\St}{\V^1_{\T\R}(s)}}<\infty$$

A few directions for improving on this result:

* It is not hard to see from the proof that it is also possible to write down a concrete bound for fixed $\gamma$ (rather than considering the $\gamma\rightarrow 1$ limit), but its form is somewhat convoluted.

* It is probably possible to get an *anytime* policy with this form of regret, using PSRL with \textit{dynamic}\Co{i} episode duration.

* It is interesting to try to make do with the weaker no-traps condition $\A^0_{\T\R}(s) = \A$, especially since a stronger no-traps conditions would translate to a stronger condition on the advisor in DRL.

* It is interesting to study RVO dimension in more detail. For example, I'm not even sure whether Proposition 2 is the best possible bound in terms of $\Abs{\F}$ or it's e.g. possible to get a linear bound.

* It seems tempting to generalize local dimension by allowing the values of $f(x)$ to lie on some \textit{nonlinear} manifold of given dimension for any given $x$. This approach, if workable, might require a substantially more difficult proof.

* The "cellular decision processes" discussed [previously](https://www.alignmentforum.org/posts/zTf946PQwN2AN3X3Y/entropic-regret-i-deterministic-mdps) in "Proposition 3" have exponentially high local dimension, meaning that this regret bound is ineffective for them. We can consider a variant in which, on every time step, only one cell or a very small number of cells (possibly chosen randomly) change. This would have low local dimension. One way to interpret it is, as a continuous time process in which each cell has a certain *rate* of changing its state.

\begin{Huge}Proofs\end{Huge}

\textbf{Proof of Proposition 1}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Proof of Proposition 2}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

Given a measurable space $X$, some $\mu,\nu\in\Delta X$ and a measurable function $f:X\rightarrow\Reals$, we will use the notation

$$\Ea{x\sim\mu-\nu}{f(x)}:=\Ea{x\sim\mu}{f(x)}-\Ea{x\sim\nu}{f(x)}$$

Given $X,Y$ measurable spaces, some $K,L:X\K Y$ and a measurable function $f:Y\rightarrow\Reals$, we will use the notation

$$\CE{y\sim K}{f(y)}{x}:=\Ea{y\sim K(x)}{f(y)}$$

$$\CE{y\sim K-L}{f(y)}{x}:=\Ea{y\sim K(x)-L(x)}{f(y)}$$

Given finite sets $\St,\A$, some $\T:\St\times\A\K\St$ and $\pi:\St\rightarrow\A$, we will use the notation $\T\pi:\St\K\St$ defined by

$$\T\pi(s):=\T\AP{s,\pi(s)}$$

\textbf{Proposition A.N1}\Co{b}

\textit{In the setting of Theorem 1, fix $T\in\Nats^+$ and $\gamma\in(0,1)$. Let $\Pi:\Hy\times\St\rightarrow\A$ be a Borel measurable mapping s.t. $\Pi(\T)$ is an optimal policy, i.e.}\Co{i}

$$\EU^{\Pi(\T)}_{\T\R}(\gamma)=\EU^{*}_{\T\R}(\gamma)$$

\textit{Let $\pi_{\zeta\Pi T}^{\PSR}: \St^*\times\St\K\A$ be the policy implemented by a PSRL algorithm with prior $\zeta$ and episode length $T$, s.t. whenever hypothesis $\T$ is sampled, policy $\Pi(\T)$ is followed. Denote the Bayesian regret by}\Co{i}

$$\Reg(\gamma):=\Ea{\T\sim\zeta}{\EU^*_{\T\R}(\gamma)-\EU^{\pi_{\zeta\Pi T}^{\PSR}}_{\T\R}(\gamma)}$$

\textit{Let $(\Omega,P)$ be a probability space governing both the uncertainty about the true hypothesis, the stochastic behavior of the environment and the random sampling inside the algorithm.}\Co{i} [See the proof of "Lemma 1" in [this](https://www.alignmentforum.org/posts/zTf946PQwN2AN3X3Y/entropic-regret-i-deterministic-mdps) previous essay or the proof of "Theorem 1" in [another](https://agentfoundations.org/item?id=1739) previous essay.]\textit{ Furthermore, let $\THy:\Omega\rightarrow\Hy$ be a random variable representing the true hypothesis, $\AC{\SHy_n:\Omega\rightarrow\Hy}_{n\in\Nats}$ be the random variables s.t. $\SHy_n$ represents the hypothesis sampled at time $n$ (i.e. during episode number $\Floor{n/T}$), $\AC{\Theta_n:\Omega\rightarrow\St}_{n\in\Nats}$ be random variables s.t. $\Theta_n$ represents the state at time $n$ and $\AC{\AT_n:\Omega\rightarrow\A}_{n\in\Nats}$ be random variables s.t. $\AT_n$ represents the action taken at time $n$. Denote $\Ev_l:=\SHy_*\Pi\AP{\SHy_{lT}}$ and $\bar{\Ev}_l:=\SHy_{lT}\Pi\AP{\SHy_*}$. Then}\Co{i}

$$\Reg(\gamma)=\sum_{n=0}^\infty\gamma^{n+1}\Ea{}{\CE{\SHy_n-\THy}{\V_{\SHy_n\R}(\gamma)}{\Theta_n,\AT_n}}+\sum_{l=0}^\infty{\gamma^{(l+1)T}}\Ea{}{\CE{\bar{\Ev}_{l}^T-\Ev_{l}^T}{\V_{\SHy_*\R}(\gamma)}{\Theta_{lT}}}$$

Here, $\Ev_l^T$ means raising $\Ev_l$ to the $T$-th power w.r.t. Markov kernel composition, and the same for $\bar{\Ev}_l^T$.

We will use the notation $\V_{\T\pi\R}(s,\gamma)$ to stand for the expected utility achieved by policy $\pi$ when starting from state $s$ with transition kernel $\T$, reward function $\R$ and time discount parameter $\gamma$.

\textbf{Proof of Proposition A.N1}\Co{b}

For any $n\in\Nats$, define $\Pi_n:\Hy\times\Hy\times\St^*\times\St\K\A$ as follows.

$$\Pi_n\AP{\T_1,\T_2,h,s}:=\begin{cases} \Pi\AP{\T_1,s} \text{ if } \Abs{h}<n \\ \Pi\AP{\T_2,s} \text{ if }\Abs{h} \geq n \end{cases}$$

That is, $\Pi_n\AP{\T_1,\T_2}$ is a policy that follows $\Pi\AP{\T_1}$ for time $n$ and $\Pi\AP{\T_2}$ afterwards. 

In the following, we use the shorthand notation 

$$\V_*(s):=\V_{\SHy_*\R}(s,\gamma)$$

$$\V_l(s):=\V_{\SHy_{lT}\R}(s,\gamma)$$

%$$\V_{lk}:=\V_{\SHy_{*}\Pi_{T-k}\AP{\SHy_{lT},\SHy_*}\R}\AP{\Theta_{lT+k},\gamma}$$
$$\V_{lk}(s):=\V_{\SHy_{*}\Pi_{T-k}\AP{\SHy_{lT},\SHy_*}\R}\AP{s,\gamma}$$

It is easy to see that

$$\Reg(\gamma)=\sum_{l=0}^\infty{\gamma^{lT}\Ea{}{\V_{*}\AP{\Theta_{lT}}-\V_{l0}\AP{\Theta_{lT}}}}$$

The above is essentially what appeared as "Proposition B.1" [before](https://agentfoundations.org/item?id=1723), for the special case of PSRL, and where we regard every episode as a single time step in a new MDP in which every action is a policy for the duration of an episode in the original MDP.

By definition, $\SHy_{lT}$ and $\SHy_*$ have the same distribution even when conditioned by the history up to $lT$. Therefore

$$\Ea{}{\V_{*}\AP{\Theta_{lT}}}=\Ea{}{\V_{l}\AP{\Theta_{lT}}}$$

It follows that

$$\Reg(\gamma)=\sum_{l=0}^\infty{\gamma^{lT}\Ea{}{\V_{l}\AP{\Theta_{lT}}-\V_{l0}\AP{\Theta_{lT}}}}$$

We now prove by induction on $k\in[T+1]$ that

$$\Ea{}{\V_{l}\AP{\Theta_{lT}}-\V_{l0}\AP{\Theta_{lT}}}=\sum_{n=lT}^{lT+k-1}\gamma^{n-lT+1}\Ea{}{\CE{\SHy_n-\THy}{\V_{l}}{\Theta_n,\AT_n}}+\gamma^k\Ea{}{\V_{l}\AP{\Theta_{lT+k}}-\V_{lk}\AP{\Theta_{lT+k}}}$$

For $k=0$ this is a tautology. For any $k\in[T]$, the Bellman equation says that

$$\V_l(s)=(1-\gamma)\R(s)+\gamma\CE{\SHy_{lT}\Pi\AP{\SHy_{lT}}}{\V_l}{s}$$

$$\V_{lk}(s)=(1-\gamma)\R(s)+\gamma\CE{\SHy_{*}\Pi\AP{\SHy_{lT}}}{\V_{l,k+1}}{s}$$

It follows that

$$\Ea{}{\V_{l}\AP{\Theta_{lT+k}}-\V_{lk}\AP{\Theta_{lT+k}}}=\gamma\Ea{}{\CE{\SHy_{lT}\Pi\AP{\SHy_{lT}}}{\V_l}{\Theta_{lT+k}}-\CE{\SHy_{*}\Pi\AP{\SHy_{lT}}}{\V_{l,k+1}}{\Theta_{lT+k}}}$$

Since $\Pi\AP{\SHy_{lT}}$ is exactly the policy followed by PSRL at time $lT+k$, we get

$$\Ea{}{\V_{l}\AP{\Theta_{lT+k}}-\V_{lk}\AP{\Theta_{lT+k}}}=\gamma\Ea{}{\CE{\SHy_{lT}}{\V_l}{\Theta_{lT+k},\AT_{lT+k}}-\CE{\SHy_{*}}{\V_{l,k+1}}{\Theta_{lT+k},\AT_{lT+k}}}$$

We now subtract and add $\CE{\SHy_*}{\V_l}{\Theta_{lT+k},\AT_{lT+k}}$, and use the fact that $\SHy_*\AP{\Theta_{lT+k},\AT_{lT+k}}$ is the conditional distribution of $\Theta_{lT+k+1}$.

$$\Ea{}{\V_{l}\AP{\Theta_{lT+k}}-\V_{lk}\AP{\Theta_{lT+k}}}=\gamma\Ea{}{\CE{\SHy_{lT}-\SHy_{*}}{\V_l}{\Theta_{lT+k},\AT_{lT+k}}+\V_l\AP{\Theta_{lT+k+1}}-\V_{l,k+1}\AP{\Theta_{lT+k+1}}}$$

Applying this identity to the second second term on the right hand side of the induction hypothesis, we prove the induction step. For $k=T$, we get, denoting $n_0:=lT$ and $n_1:=(l+1)T$

$$\Ea{}{\V_{l}\AP{\Theta_{n_0}}-\V_{l0}\AP{\Theta_{n_0}}}=\sum_{n=n_0}^{n_1-1}\gamma^{n-n_0+1}\Ea{}{\CE{\SHy_n-\THy}{\V_{l}}{\Theta_n,\AT_n}}+\gamma^T\Ea{}{\V_{l}\AP{\Theta_{n_1}}-\V_{*}\AP{\Theta_{n_1}}}$$

Clearly 

$$\Ea{}{\V_l\AP{\Theta_{n_1}}}=\Ea{}{\CE{\Ev_l^T}{\V_l}{\Theta_{n_0}}}$$

$$\Ea{}{\V_*\AP{\Theta_{n_1}}}=\Ea{}{\CE{\Ev_l^T}{\V_*}{\Theta_{n_0}}}$$

Using the definition of PSRL, we can exchange and true and sampled hypothesis and get

$$\Ea{}{\V_l\AP{\Theta_{n_1}}}=\Ea{}{\CE{\bar{\Ev}_l^T}{\V_*}{\Theta_{n_0}}}$$

It follows that

$$\Ea{}{\V_{l}\AP{\Theta_{n_0}}-\V_{l0}\AP{\Theta_{n_0}}}=\sum_{n=n_0}^{n_1-1}\gamma^{n-n_0+1}\Ea{}{\CE{\SHy_n-\THy}{\V_{l}}{\Theta_n,\AT_n}}+\gamma^T\Ea{}{\CE{\bar{\Ev}_l^T-\Ev_l^T}{\V_*}{\Theta_{n_0}}}$$

Applying this to each term in the earlier expression for $\Reg(\gamma)$, we get the desired result. \textbf{Q.E.D.}\Co{b}

\textbf{Proposition A.N6}\Co{b}

\textit{Consider a real finite-dimensional normed vector space $\Y$ and a linear subspace $\Z\subseteq \Y$. Then, there exists $\B \in\PSD(\Y)$ s.t.}\Co{i}

1. \textit{For any $v\in \Y$, $\B v^{\otimes 2}\leq\Norm{v}^2$}\Co{i}

2. \textit{For any $v\in \Z$, $\AP{\dim{\Z}}^2 \B v^{\otimes 2}\geq \Norm{v}^2$}\Co{i}

\textbf{Proof of Proposition A.N6}\Co{b}

We assume $\dim{\Z}>0$ since otherwise the claim is trivial (take $B=0$).

By Theorem B.N1 (see Appendix), there is $\tilde{B}\in\PSD(\Z)$ s.t. for any $v\in\Z$

$$\tilde{B}v^{\otimes2}\leq\Norm{v}^2\leq\dim{\Z}\cdot\tilde{B}v^{\otimes2}$$

By Corollary B.N1 (see Appendix), there is a projection operator $P:\Y\rightarrow\Y$ s.t. $\Img{\Y}=\Z$ and $\Norm{P}\leq\sqrt{\dim{\Z}}$. We define $B\in\PSD(\Y)$ by

$$B(v,w):=\frac{\tilde{B}\AP{Pv,Pw}}{\dim{\Z}}$$

For any $v\in\Y$, we have

$$Bv^{\otimes2}=\frac{\tilde{B}\AP{Pv,Pv}}{\dim{\Z}}\leq\frac{\Norm{Pv}^2}{\dim{\Z}}\leq\frac{\Norm{P}^2\Norm{v}^2}{\dim{\Z}}\leq\Norm{v}^2$$

For any $v\in\Z$, we have $Pv=v$ and therefore

$$\Norm{v}^2\leq\dim{\Z}\cdot\tilde{B}v^{\otimes2}=\AP{\dim{\Z}}^2\frac{\tilde{B}(Pv,Pv)}{\dim{\Z}}=\AP{\dim{\Z}}^2Bv^{\otimes2}$$
 
\textbf{Q.E.D.}\Co{b}

\textbf{Proposition A.N4}\Co{b}

\textit{Consider a finite-dimensional real vector space $\Y$, some $\B \in\PD(\Y)$ and a Borel probability measure $\mu\in\Delta\Y$ s.t. $\Pa{y\sim\mu}{\B y^{\otimes 2} \leq 1} = 1$. Let $y_0:=\Ea{y\sim\mu}{y}$ and $\sigma:=2\sqrt{2}$. Then, $\mu$ is $\sigma$-sub-Gaussian w.r.t. $\B $, i.e., for any $\alpha\in\Y^*$}\Co{i}

$$\Ea{y\sim\mu}{\exp\AP{\alpha\AP{y-y_0}}} \leq \exp\AP{\frac{\sigma^2\B ^{-1}\alpha^{\otimes 2}}{2}}$$

\textbf{Proof of Proposition A.N4}\Co{b}

By isomorphism, it is sufficient to consider the case $\Y=\Reals^n$, $B(v,w)=v\cdot w$, $\alpha(v)=tv_0$ for some $n\in\Nats$ and $t\in[0,\infty)$. For this form of $\alpha$ it is sufficient to consider the case $n=1$. It remains to show that

$$\Ea{y\sim\mu}{e^{t\AP{y-y_0}}}\leq e^{\frac{1}{2}\sigma^2t^2}=e^{4t^2}$$

Here, we assume that $\Pa{y\sim\mu}{\Abs{y}\leq1}=1$.

We consider separately the cases $t\geq\frac{1}{2}$ and $t<\frac{1}{2}$. In the first case

$$\Ea{y\sim\mu}{e^{t\AP{y-y_0}}}\leq \max_{y\in[-1,+1]}{e^{t(y-y_0)}} \leq e^{2t}\leq e^{(2t)^2}=e^{4t^2}$$

In the second case, we use that for any $x\in(-\infty,+1]$

$$e^x<1+x+\frac{e}{2}x^2$$

The above holds because, at $x=0$ the left hand side and the right hand have the same value and first derivative, and for any $x\in(-\infty,+1)$, the second derivative of the left hand side is less than the second derivative of the right hand side. We get

$$\Ea{y\sim\mu}{e^{t\AP{y-y_0}}}\leq\Ea{y\sim\mu}{1+t\AP{y-y_0}+\frac{e}{2}t^2\AP{y-y_0}^2}=1+\frac{e}{2}t^2\Var{y\sim\mu}{y}\leq1+\frac{e}{2}t^2\leq e^{\frac{e}{2}t^2}\leq e^{4t^2}$$

\textbf{Q.E.D.}\Co{b}

\textbf{Definition A.N1}\Co{b}

\textit{Consider a set $\X$, a finite-dimensional real vector space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and a family $\AC{\B _x\in\PSD(\Y)}_{x\in\X}$. Assume $\F$ is compact w.r.t. the product topology on $\X\rightarrow\Y\cong\prod_{x\in\X}\Y$. Consider also some $n\in\Nats$, $\bold{x}\in\X^n$, $\bold{y}\in\Y^n$ and $r\in\Reals^+$. We then use the notation}\Co{i}

$$\LS^{\F}[\bold{xy},\B]:=\Argmin{f\in\F}{\sum_{m=0}^{n-1}\B _{\bold{x}_m}\AP{f\AP{\bold{x}_m}-\bold{y} }^{\otimes2}}$$

$$\CS^{\F}[\bold{xy},B]:=\ACM{f\in\F}{\sum_{m=0}^{n-1}\B _{\bold{x}_m}\AP{f\AP{\bold{x}_m}-\LS_\B ^\F[\bold{xy}]\AP{\bold{x}_m}}^{\otimes2}\leq 1}$$

I chose the notation $\LS$ as an abbreviation of "least squares" and $\CS$ as an abbreviation of "confidence set". Note that $\LS$ is somewhat ambiguous (and therefore, so is $\CS$) since there might be multiple minima, but this will not be important in the following (i.e. an arbitrary minimum can be chosen).

\textbf{Proposition A.N5}\Co{b}

\textit{There is some $C_{\mathrm{A.N5}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider finite sets $\X,\St$, some $\F\subseteq\AC{\X\K\St}$ and a family $\AC{\B _x\in\PSD\AP{\Reals^\St}}_{x\in\X}$. Assume that for any $x\in\X$ and $\varphi\in\Delta\St$, $B_x\varphi^{\otimes2}\leq1$. Let $\AC{\mathfrak{H}_n\subseteq\PS{\X^\omega\times\St^\omega}}_{n\in\Nats}$ be the canonical filtration, i.e.}\Co{i}

$$\mathfrak{H}_n:=\ACM{A'\subseteq\X^\omega\times\St^\omega}{A'=\ACM{\bold{xs}}{\bold{xs}_{:n}\in A},\ A\subseteq\X^n\times\St^n}$$

\textit{Consider also $f^*\in\F$ and $\mu\in\Delta\AP{\X^\omega\times\St^\omega}$ s.t. for any $n\in\Nats$, $x\in\X$, and $s\in\St$}\Co{i}

$$\CP{\bold{xs}\sim\mu}{\bold{s}_n=s}{\bold{x}_n=x,\ \mathfrak{H}_n} = f^*(s\mid x)$$

\textit{Fix $\epsilon\in\Reals^+$, $\delta\in(0,1)$. Denote}\Co{i}

$$\beta(t):=C_{\mathrm{A.N5}}\AP{\ln{\frac{\N(\F,\epsilon^{-2}\B )}{\delta}}+\epsilon t\ln{\frac{et}{\delta}}}$$

\textit{Then,}\Co{i}

$$\Pa{\bold{xs}\sim\mu}{f^*\not\in\bigcap_{n=0}^\infty\CS^\F\AB{\bold{xs}_{:n},\beta(n+1)^{-1}\B}} \leq \delta$$


\textbf{Proof of Proposition A.N5}\Co{b}

For each $x\in\X$, choose a finite set $E_x$ and a linear mapping $P_x:\Reals^\St\rightarrow\Reals^{E_x}$ s.t. $B_x\AP{v,w}=P_x(v)\cdot P_x(w)$. Let $\tilde{\Y}:=\bigoplus_{x\in\X}\Reals^{E_x}$. Define $\tilde{\F}\subseteq\AC{\X\rightarrow\tilde{\Y}}$ by

$$\tilde{\F}:=\ACM{\tilde{f}:\X\rightarrow\tilde{\Y}}{\tilde{f}(x)=P_x\AP{f(x)},\ f\in\F}$$

Define $P^\omega:\X^\omega\times\St^\omega\rightarrow\X^\omega\times\tilde{\Y}^\omega$ by

$$P^\omega(\bold{xs})_n:=\bold{x}_nP_{\bold{x}_n}\AP{\bold{s}_n}$$

Let $\tilde{f}^*:=P^\omega\AP{f^*}$ and $\tilde{\mu}:=P^\omega_*\mu$. By Proposition A.N4, $\tilde{\mu}$ is $2\sqrt{2}$-sub-Gaussian. Applying Proposition B.N1 (see Appendix) to the tilde objects gives the desired result. Here, we choose the constant $C_{\text{A.N5}}$ s.t. the term $M=1$ in $\beta$ as defined in Proposition B.N1 is absorbed by the term $\sigma\ln{\frac{et}{\delta}}\geq1$ (note that $t\geq1$ since we substitute $t=n+1$, $\delta<1$ and $\sigma=2\sqrt{2}>1$). \textbf{Q.E.D.}\Co{b}

\Co{b}

\textbf{Definition A.N2}\Co{b}

\textit{Consider a set $\X$, some $x\in\X$, a real vector space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and a family $\AC{\B _x\in\PSD(\Y)}_{x\in\X}$. The}\Co{i} $\B $-width of $\F$ at $x$ \textit{is defined by}\Co{i}

$$\W^\F(x,B):=\sup_{f,\tilde{f}\in\F}\sqrt{\B _x\AP{f(x)-\tilde{f}(x)}^{\otimes2}}$$  

\textbf{Proposition A.N2}\Co{b}

\textit{There is some $C_{\mathrm{A.N2}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider a set $\X$, a real vector space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and a family $\AC{\B _x\in\PSD(\Y)}_{x\in\X}$. Consider also some $\bold{x}\in\X^\omega$, $\bold{y}\in\Y^\omega$, $T\in\Nats^+$, $\gamma\in(0,1)$, $\theta\in\Reals^+$, $\eta_0,\eta_1\in\Reals^+$ and $\delta\in(0,1)$. Denote}\Co{i}

$$\beta(t):=\eta_0 + \eta_1t\ln{\frac{et}{\delta}}$$

\textit{For any $n\in\Nats$, define $\F_n$ by}\Co{i}

$$\F_n:=\CS^\F\AB{\bold{xy}_{:n},\beta(n+1)^{-1}\B}$$

\textit{Then,}\Co{i}

$$\sum_{l=0}^\infty\sum_{m=0}^{T-1}{\gamma^{lT+m}[[\W^{F_{lT}}\AP{\bold{x}_{lT+m},B}>\theta]]} \leq C_{\mathrm{A.N2}}\RVO{\F}\cdot\AP{\theta^{-2}\beta\AP{\frac{1}{1-\gamma}}+T}$$

\textbf{Proof of Proposition A.N2}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Proposition A.N3}\Co{b}

\textit{There is some $C_{\mathrm{A.N3}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider a set $\X$, a real vector space $\Y$, some $\F\subseteq\AC{\X\rightarrow\Y}$ and a family $\AC{\B _x\in\PSD(\Y)}_{x\in\X}$. Assume that for any $x\in\X$ and $f\in\F$, $\B _x{f(x)}^{\otimes2}\leq 1$. Consider also some $\bold{x}\in\X^\omega$, $\bold{y}\in\Y^\omega$, $T\in\Nats^+$, $\gamma\in(0,1)$, $\eta_0,\eta_1\in\Reals^+$ and $\delta\in(0,1)$. Define $\beta$ and $\F_n$ the same way as in Proposition A.N2. Denote $D:=\RVO{\F}$. Then,}\Co{i}

$$\sum_{l=0}^\infty\sum_{m=0}^{T-1}{\gamma^{lT+m}\W^{F_{lT}}\AP{\bold{x}_{lT+m},B}} \leq C_{\mathrm{A.N3}}\AP{1+D T\ln\frac{1}{1-\gamma}+\sqrt{D\beta\AP{\frac{1}{1-\gamma}}\frac{1}{1-\gamma}}}$$

\textbf{Proof of Proposition A.N3}\Co{b}

TBD \textbf{Q.E.D.}\Co{b}

\textbf{Proof of Theorem 1}\Co{b}

We take $\pi^\dagger_\gamma:=\pi^\PSR_{\zeta\Pi T}$ where $\Pi$ is as in Proposition A.N1 and $T\in\Nats^+$ will be specified later. Denote the Bayesian regret by

$$\Reg(\gamma):=\Ea{\T\sim\zeta}{\EU^*_{\T\R}(\gamma)-\EU^{\pi_{\gamma}^{\dagger}}_{\T\R}(\gamma)}$$

By Proposition A.N1

$$\Reg(\gamma)=\sum_{n=0}^\infty\gamma^{n+1}\Ea{}{\CE{\SHy_n-\THy}{\V_{\SHy_n\R}(\gamma)}{\Theta_n,\AT_n}}+\sum_{l=0}^\infty{\gamma^{(l+1)T}}\Ea{}{\CE{\bar{\Ev}_{l}^T-\Ev_{l}^T}{\V_{\SHy_*\R}(\gamma)}{\Theta_{lT}}}$$

$$\Reg(\gamma)\leq\sum_{n=0}^\infty\gamma^{n}\Ea{}{\Abs{\CE{\SHy_n-\THy}{\V_{\SHy_n\R}(\gamma)}{\Theta_n,\AT_n}}}+\sum_{l=0}^\infty{\gamma^{lT}}\Ea{}{\Abs{\CE{\bar{\Ev}_{l}^T-\Ev_{l}^T}{\V_{\SHy_*\R}(\gamma)}{\Theta_{lT}}}}$$

We will use the notation

$$\De\V(\gamma):=\max_{\T\in\Hy}{\AP{\max_{s\in\St}{\V_{\T\R}(s,\gamma)}-\min_{s\in\St}{\V_{\T\R}(s,\gamma)}}}$$

It follows that

$$\Reg(\gamma)\leq\Delta\V(\gamma)\AP{\sum_{n=0}^\infty\gamma^{n}\Ea{}{\Dtva{\SHy_n\AP{\Theta_n,\AT_n},\THy\AP{\Theta_n,\AT_n}} }+\sum_{l=0}^\infty\gamma^{lT}\Ea{}{\Dtva{\Ev_{lT}^T\AP{\Theta_{lT}},\bar{\Ev}_{lT}^T\AP{\Theta_{lT}}}}}$$

$$\Reg(\gamma)\leq\Delta\V(\gamma)\AP{\sum_{n=0}^\infty\gamma^{n}\Ea{}{\Dtva{\SHy_n\AP{\Theta_n,\AT_n},\THy\AP{\Theta_n,\AT_n}} }+\frac{1}{1-\gamma^T}}$$

Consider $\Y=\Reals^\St$ equipped with the $L^1$ norm. Given $s\in\St$ and $a\in\A$, consider also the subspace

$$W_{sa}:=\Sp\ACM{\T(s,a)}{\T\in\Hy}$$

By Proposition A.N6, there is $\B_{sa}\in\PSD\AP{\Y}$ s.t.

1. For any $\varphi\in\Delta\St$

$$\B_{sa}\varphi^{\otimes2}\leq1$$

2. For any $\T,\tilde{\T}\in\Hy$

$$\frac{1}{4}\DL^2 B_{sa}\AP{\T(s,a)-\tilde{\T}(s,a)}^{\otimes2} \geq \Dtva{\T(s,a),\tilde{\T}(s,a)}^2$$

We now apply Proposition A.N5 with $\delta:=\frac{1}{2}(1-\gamma)^2$ and $\epsilon:=(1-\gamma)^2$. We get

$$\Pa{}{\SHy_*\in\bigcap_{n=0}^\infty\CS^\Hy\AB{\Theta\AT_{:n}\Theta_n,\beta(n+1)^{-1}\B}} \geq 1-\frac{1}{2}(1-\gamma)^2$$

Here, $\beta$ was defined in Proposition A.N5.

Since the hypothesis $\SHy_n$ is sampled from the posterior, for any $l\in\Nats$ we also have

$$\Pa{}{\SHy_{lT}\in\CS^\Hy\AB{\Theta\AT_{:lT}\Theta_{lT},\beta(lT+1)^{-1}\B}} \geq 1-\frac{1}{2}(1-\gamma)^2$$

$$\Pa{}{\SHy_*,\SHy_{lT}\in\CS^\Hy\AB{\Theta\AT_{:lT}\Theta_{lT},\beta(lT+1)^{-1}\B}} \geq 1-(1-\gamma)^2$$

Denote

$$\CSE_n:=\AC{\SHy_*,\SHy_{lT}\in\CS^\Hy\AB{\Theta\AT_{:lT}\Theta_{lT},\beta(lT+1)^{-1}\B}}\subseteq\Omega$$

We get

$$\Reg(\gamma)\leq\Delta\V(\gamma)\sum_{n=0}^\infty\gamma^{n}\AP{\Ea{}{\Dtva{\SHy_n\AP{\Theta_n,\AT_n},\THy\AP{\Theta_n,\AT_n}};\CSE_n }+(1-\gamma)^2}$$

$$\Reg(\gamma)\leq\Delta\V(\gamma)\AP{\sum_{n=0}^\infty\gamma^{n}\Ea{}{\Dtva{\SHy_n\AP{\Theta_n,\AT_n},\THy\AP{\Theta_n,\AT_n}};\CSE_n }+1-\gamma+\frac{1}{1-\gamma^T}}$$

Using property 2 of B

$$\Reg(\gamma)\leq\Delta\V(\gamma)\AP{\frac{1}{2}\DL\sum_{n=0}^\infty\gamma^{n}\Ea{}{\sqrt{B_{\Theta_n\AT_n}\AP{\SHy_n\AP{\Theta_n,\AT_n}-\THy\AP{\Theta_n,\AT_n}}^{\otimes2}};\CSE_n }+1-\gamma+\frac{1}{1-\gamma^T}}$$

Denote

$$\Hy_l:=\CS^\Hy\AB{\Theta\AT_{:lT}\Theta_{lT},\beta(lT+1)^{-1}\B}$$

Clearly

$$\Pa{}{\sqrt{B_{\Theta_n\AT_n}\AP{\SHy_n\AP{\Theta_n,\AT_n}-\THy\AP{\Theta_n,\AT_n}}^{\otimes2}}\leq\W^{\Hy_{\Floor{n/T}}}\AP{\Theta_n\AT_n,B};\CSE_n}=1$$

Using this inequality, dropping the $;G_n$ (since it can only the right hand side smaller) and moving the sum inside the expected value, we get

$$\Reg(\gamma)\leq\Delta\V(\gamma)\AP{\frac{1}{2}\DL\Ea{}{\sum_{n=0}^\infty\gamma^{n}\W^{\Hy_{\Floor{n/T}}}\AP{\Theta_n\AT_n,B} }+1-\gamma+\frac{1}{1-\gamma^T}}$$

Applying Proposition A.N3, we conclude

$$\Reg(\gamma)\leq\Delta\V(\gamma)\AP{\frac{\DL C_{\mathrm{A.N3}}}{2}\AP{1+\DRVO T\ln\frac{1}{1-\gamma}+\sqrt{\DRVO\beta\AP{\frac{1}{1-\gamma}}\frac{1}{1-\gamma}}}+1-\gamma+\frac{1}{1-\gamma^T}}$$

Denote the second factor on the right hand side $F(\gamma)$, so that the inequality becomes 

$$\Reg(\gamma)\leq\De\V(\gamma)F(\gamma)$$

We set

$$T:=\Floor{\frac{1}{\sqrt{\DL\DRVO(1-\gamma)\ln\frac{1}{1-\gamma}}}}$$

Now, we analyze the $\gamma\rightarrow1$ limit.

$$\limsup_{\gamma\rightarrow1}{\frac{\Reg(\gamma)}{\sqrt{(1-\gamma)\ln{\frac{1}{1-\gamma}}}}}\leq \limsup_{\gamma\rightarrow1}{\frac{\De\V(\gamma)F(\gamma)}{\sqrt{(1-\gamma)\ln{\frac{1}{1-\gamma}}}}}$$

We assume $\tau_{\zeta\R}<\infty$, otherwise the theorem is vacuous. Multiplying the numerator and denominator on the right hand side by $\sqrt{1-\gamma}$, we get

$$\limsup_{\gamma\rightarrow1}{\frac{\Reg(\gamma)}{\sqrt{(1-\gamma)\ln{\frac{1}{1-\gamma}}}}}\leq  \tau_{\zeta\R}\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot F(\gamma)}$$

We will now analyze the contribution of each term in $F(\gamma)$ to the limit on the right hand side (using the fact that $\limsup\AP{F_1+F_2}\leq\limsup{F_1}+\limsup{F_2}$). We ignore multiplicative constants that can be absorbed into $C$.

The first term gives

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\DL} = 0$$

The second term gives (using our choice of $T$)

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\sqrt{\frac{\DL\DRVO\ln{\frac{1}{1-\gamma}}}{1-\gamma}}} = \sqrt{\DL\DRVO}$$

The third term gives (using the definitions of $\beta$ and our choices of $\delta$ and $\epsilon$)

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\DL\sqrt{\DRVO\AP{\ln{\frac{2\N\AP{\Hy,(1-\gamma)^{-4}\B}}{(1-\gamma)^2}}+(1-\gamma) \ln{\frac{2e}{(1-\gamma)^3}}}\frac{1}{1-\gamma}}}\leq$$

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\DL\sqrt{\frac{\DRVO}{1-\gamma}}\AP{\sqrt{\ln {\N\AP{\Hy,(1-\gamma)^{-4}\B}}}+\sqrt{\ln{\frac{2}{(1-\gamma)}}}+\sqrt{(1-\gamma) \ln{\frac{2e}{(1-\gamma)^3}}}}}$$

We analyze each subterm separately. The first subterm gives

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\DL\sqrt{\frac{\DRVO}{1-\gamma}\ln{\N\AP{\Hy,(1-\gamma)^{-4}\B }}}}=$$

$$\DL\sqrt{\DRVO}\limsup_{\gamma\rightarrow1}{\sqrt{\frac{\ln{\N\AP{\Hy,(1-\gamma)^{-4}\B}}}{\frac{1}{2}\ln{\frac{1}{(1-\gamma)^2}}}}}\leq$$

$$\DL\sqrt{2\DRVO\DMB}$$

The second subterm gives

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\DL\sqrt{\frac{\DRVO}{1-\gamma}\ln{\frac{2}{1-\gamma}}}}=\DL\sqrt{\DRVO}$$

The third subterm gives

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\DL\sqrt{\frac{\DRVO}{1-\gamma}(1-\gamma) \ln{\frac{2e}{(1-\gamma)^3}}}}=0$$

The fourth term gives

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot(1-\gamma)}=0$$

To analyze the fifth term, observe that $\lim_{\gamma\rightarrow1}{(1-\gamma)T(\gamma)}=0$. Hence

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\frac{1}{1-\gamma^{T(\gamma)}}}=$$

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\frac{1}{(1-\gamma)T(\gamma)}}=$$

$$\limsup_{\gamma\rightarrow1}{\sqrt{\frac{1-\gamma}{\ln{\frac{1}{1-\gamma}}}}\cdot\frac{\sqrt{\DL\DRVO(1-\gamma)\ln\frac{1}{1-\gamma}}}{(1-\gamma)}}=$$

$$\sqrt{\DL\DRVO}$$

Putting everything together, we observe that the expression $\DL\sqrt{\DRVO(\DMB+1)}$ dominates all the terms (up to a multiplicative constant). Here, we use that $\DL$ is an integer and hence $\DL\geq\sqrt{\DL}$. We conclude

$$\limsup_{\gamma\rightarrow1}{\frac{\Reg(\gamma)}{\sqrt{(1-\gamma)\ln{\frac{1}{1-\gamma}}}}}\leq C\tau_{\zeta\R}\DL\sqrt{\DRVO(\DMB+1)}$$

\textbf{Q.E.D.}\Co{b}

\begin{Huge}Appendix\end{Huge}

The following was proved in John (1948), available in the collection [Giorgi and Kjeldesen (2014)](https://www.springer.com/us/book/9783034804387) (the theorem in question is on page 214). 

\textbf{Theorem B.N1 (John)}\Co{b}

\textit{Consider a real finite-dimensional normed vector space $\Y$. Then, there exists $B\in\PD(\Y)$ s.t. for any $v\in\Y$}\Co{i}

$$Bv^{\otimes2}\leq\Norm{v}^2\leq\dim{\Y}\cdot Bv^{\otimes2}$$

The following is from [Kadets and Snobar (1971)](https://link.springer.com/article/10.1007%2FBF01106467).

\textbf{Theorem B.N2 (Kadets-Snobar)}\Co{b}

\textit{Consider a real Banach space $\Y$ and a finite-dimensional linear subspace $\Z\subseteq\Y$. Then, for any $\epsilon\in\Reals^+$, there is a projection operator $P:\Y\rightarrow\Y$ s.t. $\Img{P}=\Z$ and $\Norm{P}<\sqrt{\dim{\Z}}+\epsilon$.}\Co{i}

\textbf{Corollary B.N1}\Co{b}

\textit{Consider a real finite-dimensional normed vector space $\Y$ and a linear subspace $\Z\subseteq\Y$. Then, there is a projection operator $P^\star:\Y\rightarrow\Y$ s.t. $\Img{P^\star}=\Z$ and $\Norm{P^\star}\leq\sqrt{\dim{\Z}}$.}\Co{i}

\textbf{Proof of Corollary B.N1}\Co{b}

Let $\End(\Y)$ be the vector space of linear operators $\Y\rightarrow\Y$. Define

$$\mathrm{Pr}(\Y,\Z):=\ACM{P\in\End(\Y)}{P^2=P,\ \Img{P}=\Z}$$

$$X:=\ACM{P\in\mathrm{Pr}(\Y,\Z)}{\Norm{P}\leq\sqrt{\dim{\Z}}+1}$$

$\mathrm{Pr}(\Y,\Z)$ is an affine subspace of $\End(\Y)$ and $X$ is a compact set. Define $f:X\rightarrow\Reals$ by $f(P):=\Norm{P}$. $f$ is a continuous function. By Theorem B.N2, $\inf f \leq \sqrt{\dim\Z}$. Since $X$ is compact, $f$ attains its infimum at some $P^\star\in X$, and we have $f\AP{P^\star} \leq \sqrt{\dim\Z}$, \textbf{Q.E.D.}\Co{b}

The next proposition appeared (in slightly greater generality) in Osband and Van Roy as "Proposition 5". We will use $\Id_d\in\PD\AP{\Reals^d}$ to denote the form corresponding to the identity matrix.

\textbf{Proposition B.N1 (Osband-Van Roy)}\Co{b}

\textit{There is some $C_{\mathrm{B.N2}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider a finite set $\X$, the vector space $\Y:=\Reals^d$ for some $d\in\Nats$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Let $\AC{\mathfrak{H}_n\subseteq\PS{\X^\omega\times\Y^\omega}}_{n\in\Nats}$ be the canonical filtration, i.e.}\Co{i}

$$\mathfrak{H}_n:=\ACM{A'\subseteq\X^\omega\times\Y^\omega}{A'=\ACM{\bold{xy}}{\bold{xy}_{:n}\in A},\ A\subseteq\X^n\times\Y^n\text{ Borel}}$$

\textit{Consider also $f^*\in\F$ and $\mu\in\Delta\AP{\X^\omega\times\Y^\omega}$ s.t. for any $n\in\Nats$ and $x\in\X$}\Co{i}

$$\CE{\bold{xy}\sim\mu}{\bold{y}_n}{\bold{x}_n=x,\ \mathfrak{H}_n} = f^*(x)$$

\textit{Assume that $M\in\Reals^+$ is s.t. $\bold{y}_n\cdot\bold{y}_n\leq M^2$ with $\mu$-probability $1$ for all $n\in\Nats$. Assume also that $\sigma\in\Reals^+$ is s.t. $\mu$ is $\sigma$-sub-Gaussian, i.e., for any $\alpha\in\Y$, $n\in\Nats$ and $x\in\X$}\Co{i}

$$\CE{\bold{xy}\sim\mu}{\exp\AP{\alpha\cdot\AP{\bold{y}_n-f^*(x)}}}{\bold{x}_n=x,\ \mathfrak{H}_n} \leq \exp\AP{\frac{\sigma^2\AP{\alpha\cdot\alpha}}{2}}$$

\textit{Fix $\epsilon\in\Reals^+$, $\delta\in(0,1)$. Define $\beta:\Reals^+\rightarrow\Reals$ and, for each $n\in\Nats$, $Q_n\in\PD(\Y)$ by}\Co{i}

$$\beta(t):=C_{\mathrm{B.N2}}\AP{\sigma^2 \ln{\frac{\N\AP{\F,\epsilon^{-2}\Id_d}}{\delta}}+\epsilon t\AP{M+\sigma\ln{\frac{et}{\delta}}}}$$

\textit{Then,}\Co{i}

$$\Pa{\bold{xy}\sim\mu}{f^*\not\in\bigcap_{n=0}^\infty\CS^\F\AB{\bold{xy}_{:n},\beta(n+1)^{-1}\Id_d}} \leq \delta$$

Note that we removed a square root in the definition of $\beta$ compared to equation (7) in Osband and Van Roy. This only makes $\beta$ larger (up to a constant factor) and therefore, only makes the claim weaker.

The proposition below appeared in Osband and Van Roy as "Lemma 1".

\textbf{Proposition B.N2 (Osband-Van Roy)}\Co{b}

%\textit{There is some $C_{\mathrm{B.N2}}\in\Reals^+$ s.t. the following holds.}\Co{i}

\textit{Consider a set $\X$, the vector space $\Y=\Reals^d$ for some $d\in\Nats$ and some $\F\subseteq\AC{\X\rightarrow\Y}$. Consider also some $\bold{x}\in\X^\omega$, $\bold{y}\in\Y^\omega$, $T\in\Nats^+$, $N\in\Nats$, $\theta\in\Reals^+$, $\delta\in(0,1)$ and a nondecreasing sequence $\AC{\beta_n\in\Reals^+}_{n\in\Nats}$. For any $n\in\Nats$, define $\F_n$ by}\Co{i}

$$\F_n:=\CS^\F\AB{\bold{xy}_{:n},\beta_n^{-1}\Id_d}$$

\textit{Then,}\Co{i}

$$\sum_{l=0}^{N-1}\sum_{m=0}^{T-1}{[[\W^{F_{lT}}\AP{\bold{x}_{lT+m},\Id_d}>\theta]]} \leq \RVO{\F}\cdot\AP{4\theta^{-2}\beta_{NT}+T}$$

\end{document}


