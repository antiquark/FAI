%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1.6in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Co}[1]{}
\newcommand{\San}[1]{}
% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Dom}{dom}
\DeclareMathOperator{\Sp}{span}
% autosize delimiters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}
\newcommand{\AC}[1]{\left\{#1\right\}}
\newcommand{\APM}[2]{\left(#1\;\middle\vert\;#2\right)}
\newcommand{\ABM}[2]{\left[#1\;\middle\vert\;#2\right]}
\newcommand{\ACM}[2]{\left\{#1\;\middle\vert\;#2\right\}}
% probability theory
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\CP}[3]{\underset{#1}{\operatorname{Pr}}\ABM{#2}{#3}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\CE}[3]{\underset{#1}{\operatorname{E}}\ABM{#2}{#3}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\Var}[2]{\underset{#1}{\operatorname{Var}}\AB{#2}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\CI}[3]{\underset{#1}{\operatorname{I}}\ABM{#2}{#3}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}\AP{#1\middle\vert\middle\vert#2}}
\newcommand{\RD}[3]{\operatorname{D}_{#1}\AP{#2\middle\vert\middle\vert#3}}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\text{tv}}\AP{#1}}
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
% power set
\newcommand{\PS}[1]{\mathcal{P}\AP{#1}}
% differential
\newcommand{\D}{\mathrm{d}}
% arg
\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}
% numbers
\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}
% linear algebra
\newcommand{\Id}{\mathrm{I}}
\newcommand{\PD}{\mathrm{PD}}
\newcommand{\PSD}{\mathrm{PSD}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\End}{\mathrm{End}}
\DeclareMathOperator{\Img}{im}
% topology and category theory
\newcommand{\Mor}{\mathrm{Mor}}
\newcommand{\Pt}{\boldsymbol{\mathrm{pt}}}
% empty string
\newcommand{\Estr}{\boldsymbol{\lambda}}
% limits
\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}
% more delimiters
\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\Floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\Chev}[1]{\left\langle #1 \right\rangle}
\newcommand{\Quote}[1]{\left\ulcorner #1 \right\urcorner}
% arrows
\newcommand{\K}{\xrightarrow{\mathrm{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}
% Paper specific
\newcommand{\St}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Ob}{\mathcal{O}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\ER}{\mathrm{E}}
\newcommand{\Con}{\boldsymbol{\mathrm{ConSet}}}
\DeclareMathOperator{\Cone}{Cone}
\newcommand{\Ht}{\mathrm{h}}
\newcommand{\IS}{\mathcal{IS}}
\DeclareMathOperator{\RVO}{\dim_{RVO}}
\DeclareMathOperator{\MB}{\dim_{MB}}
\newcommand{\DRVO}{D_{\mathrm{RVO}}}
\newcommand{\DMB}{D_{\mathrm{MB}}}
\newcommand{\N}{\mathrm{N}}
\newcommand{\V}{\mathrm{V}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\EU}{\mathrm{EU}}
\newcommand{\Reg}{\mathrm{R}}
\newcommand{\PSR}{\text{PS}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\CS}{\mathrm{CS}}
\newcommand{\W}{\mathrm{W}}
\newcommand{\AT}{\mathrm{A}}
\newcommand{\THy}{\mathrm{H}_*}
\newcommand{\SHy}{\mathrm{H}}
\newcommand{\Ev}{\mathcal{E}}
\newcommand{\De}{\Delta}
\newcommand{\CSE}{G}

\begin{document}

\textbf{TLDR:}\Co{b} We define a variant of reinforcement learning in which the reward is not perceived directly, but can be estimated at any given moment by some (possibly costly) experiment. We give two definitions of  "imperceptible reward function" and prove their equivalence. We also derive a regret bound for this setting.

\begin{Huge}Background\end{Huge}

In "classical" reinforcement learning the agent perceives the reward signal on every round of its interaction with the environment, whether through a distinct input channel or through some given way to compute the reward from the interaction history so far. On the other hand, we can rather easily imagine agents that optimize properties of their environment that they do not directly perceive. For example, if Alice donates money to the Against Malaria Foundation in order to save someone in Africa, then the result is usually not visible to Alice at the time it occurs, if ever. Similarly, a paperclip maximizer doesn't always perceive all the paperclips in the universe.

Now, it is possible to define the perceived reward as the \textit{subjective expected value}\Co{i} of the "true" imperceptible reward (see the Results section for details). Although this transformation preserves expected utility, it does \textit{not} preserve Bayesian regret. Indeed, Bayesian regret is the difference between the expected utility attained by the agent and the expected utility attained by a "reference" agent that knows the true environment from the onset. However, after the transformation, the reference agent will behave as if it knows the observable dynamics of the true environment but still pretends not to know the true environment for the purpose of computing the reward. Therefore, regret analysis requires us to consider the imperceptible reward honestly. In fact, as we will see, certain assumptions about the imperceptible reward function are needed even to make the problem learnable at all. Finally, this transformation makes the reward function more complex and hence applying a "generic" reinforcement learning algorithm after the transformation (instead of exploiting the special form of the resulting reward function) might carry a significant computational complexity penalty.

\textbf{Related Work:}\Co{b} [De Blanc 2011]\San{(https://arxiv.org/abs/1105.3821)}\Co{s} studies so called "ontological crises". That is, de Blanc examines the problem of translating a reward function from one ontology into another. Here, we avoid this problem by considering reward functions that are automatically defined in *all* ontologies. That said, it might still be interesting to think about how to specify our type of reward function starting from a reward function that is only defined in one particular ontology. We will return to this below.

[Krueger et al 2016]\San{(https://pdfs.semanticscholar.org/a47f/52b25ce1e56a03876d9c0fd7c45e63270eb4.pdf)}\Co{s} consider a setting where querying the reward is instantaneous and has a fixed cost. This is a special case of our setting, but we allow a much more general type of query and cost. Also, Krueger et al don't derive any theoretical regret bound (but they do present some experimental results).

Finally, multi-armed bandits with partial monitoring are closely related, see for example [Bartok et al 2013]\San{(http://www.mit.edu/~rakhlin/papers/partial_monitoring.pdf)}\Co{s}. However, bandits by definition assume a stateless environment, and also our approach is rather different than what is usually studied in partial monitoring.

The literature study was very cursory and I will be glad to know about prior work I missed!

\begin{Huge}Results\end{Huge}

Partially Observable Markov Decision\ Processes (POMDPs) serve as a natural starting point for thinking about imperceptible rewards. Indeed, it might seem like all we have to do is consider RL in a POMDP environment and let the reward to be a function of the (imperceptible) state. However, this setting is in general unlearnable even given assumptions that rule out traps.

A (finite) POMDP is defined by non-empty finite sets $\St$ (states), $\A$ (actions) and $\Ob$ (observations), the transition kernel $\T:\St\times\A\K\St\times\Ob$ and the reward function $\R:\St\rightarrow[0,1]$. As opposed to a "classical" POMDP, the value of $\R$ is assumed to be imperceptible. The perceptible setting is a special case of the imperceptible setting: we can always encode the reward into the observation.

To formulate a learning problem, we assume $\St$, $\A$ and $\Ob$ to be fixed and an initial state $s_0\in\St$ to be given, while $\T$ and $\R$ are unknown and belong to some hypothesis class $\Hy$:

$$\Hy\subseteq\AC{\St\times\A\K\St\times\Ob}\times\bigg\{\St\rightarrow[0,1]\bigg\}$$

Such a problem can be unlearnable even if $\R$ is known and there are no irreversible events that can happen:

\textbf{Example 1}\Co{b}

\textit{Suppose that $\St=\AC{s_0,s_-,s_+}$, $\A=\AC{a_-,a_+}$, $\Ob=\AC{\bot}$, $\R\AP{s_0}=\frac{1}{2}$, $\R\AP{s_-}=0, \R\AP{s_+}=1$ and $\Hy=\AC{\AP{\T_-,\R},\AP{\T_+,\R}}$ where for any $s\in\St$, $\T_-\APM{s_+,\bot}{s,a_-}=1$, $\T_-\APM{s_-,\bot}{s,a_+}=1$, $\T_+\APM{s_+,\bot}{s,a_+}=1$ and $\T_+\APM{s_-,\bot}{s,a_-}=1$. Since $\Abs{\Ob}=1$, there is no way to gain any information about which hypothesis is correct, however the optimal policies for the two hypotheses are different. Namely, for $\T_+$ we should always take action $a_+$ and for $\T_-$ we should always take action $a_-$.}\Co{i}

To formalize and illustrate the discussion in the Background section, suppose $\Hy$ is Borel and $\zeta\in\Delta\Hy$ is the prior. We can then define the "perceived effective reward" $\ER\R:\AP{\A\times\Ob}^*\rightarrow[0,1]$ by

$$\ER\R(ao):=\CE{\substack{(\T,\R)\sim\zeta\\\AP{s_{n+1},o'_n}\sim\T\AP{s_n,a_n}}}{\R\AP{s_{\Abs{h}}}}{o'=o}$$

It is then easy to see that the $\ER$ operator preserves expected utility: given any policy $\pi:\AP{\A\times\Ob}^*\K \A$ and $m\in\Nats$

$$\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\ER\R\AP{ao_{:m}}}=\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\R\AP{s_m}}$$

and therefore, for any geometric time discount parameter $\gamma\in[0,1)$

$$\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\sum_{n=0}^\infty\gamma^n\ER\R\AP{ao_{:n}}}=\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\sum_{n=0}^\infty\gamma^n\R\AP{s_n}}$$

On the other hand, Bayesian regret is not preserved since, in general

$$\Ea{(\T,\R)\sim\zeta}{\max_{\pi}\Ea{aos\sim\T\pi}{\sum_{n=0}^\infty\gamma^n\ER\R\AP{ao_{:n}}}}\ne\Ea{(\T,\R)\sim\zeta}{\max_{\pi}\Ea{aos\sim\T\pi}{\sum_{n=0}^\infty\gamma^n\R\AP{s_n}}}$$

Here $aos\sim\T\pi$ is shorthand notation for the same probability distribution as before.

Indeed, in Example 1 the LHS of the above is $\frac{1}{2}\cdot\frac{1}{1-\gamma}$ since $\ER\R\equiv\frac{1}{2}$, whereas the RHS is $\frac{1}{2}+\frac{\gamma}{1-\gamma}$.

The pathology of Example 1 comes about because reward is not only imperceptible but entirely \textit{unobservable}\Co{i}. That is, no experiment can produce any information about whether the reward on a given round $n > 0$ was 0 or 1. More specifically, the states $s_-$ and $s_+$ are assigned different rewards, but there is no observable difference between them. It is as if Alice would assign value, not to people in Africa (whose existence and well-being can be measured) but to some Flying Spaghetti Monster s.t. the world behaves exactly the same regardless of its existence or condition.

This observation suggests that, instead of assigning rewards to states which are just abstract labels in a model, we should assign rewards to states that are defined in terms of the observable consequences of the interaction of the agent with the environment. This leads us to the notion of "instrumental state", which we will now define formally.

First, we introduce some technical definitions for notational convenience.

\textbf{Definition 1}

\textit{$\Con$ is the category whose objects are pairs $(V,C)$ where $V$ is a real vector space and $C$ is a convex subset of $V$, and whose morphisms are}\Co{i}

$$\Mor\AP{(V,C),(W,D)}:=$$
$$\ACM{f:C\rightarrow D}{\exists A \in \Hom(V,W),w\in W \forall v\in C: f(v)=Av+w}$$

We omit describing identity and composition of morphisms since they are obvious.

$\Con$ has a final object that we denote by $\Pt$ (the one point set). $\Con$ has products: $(V,C)\times(W,D)\cong(V\oplus W,C\times D)$. For any finite set $A$, $\AP{\Reals^A,\Delta A}$ is an object in $\Con$. Also, we will sometimes abuse notation by regarding $C$ as an object of $\Con$ instead of $(V,C)$ (i.e. making $V$ implicit).

\textbf{Definition 2}\Co{b}

\textit{The functor $\Cone:\Con\rightarrow\Con$ is defined by}\Co{i}

$$\Cone{(V,C)}:=\AP{V\oplus\Reals,\ACM{(\lambda v,\lambda)}{\lambda\in[0,1], v\in C}}$$
$$\AP{\Cone{f}}(\lambda v, \lambda):=\AP{\lambda f(v), \lambda}$$

\textbf{Definition 3}\Co{b}

\textit{For any $(V,C)\in\Con$, we define $\Ht_{(V,C)}:\Cone{\big(V,C\big)}\rightarrow\AP{\Reals,[0,1]}$ by}\Co{i}

$$\Ht_{(V,C)}(\lambda v, \lambda):=\lambda$$

Note that $\Ht$ is a natural transformation from $\Cone$ to the constant functor with value $\AP{\Reals,[0,1]}$.

Now, we are ready to define instrumental states. We fix the sets $\A$ and $\Ob$.

\textbf{Definition 4}\Co{b}

\textit{For any $n\in\Nats$, we define} $\IS_n\in\Con$, the space of $n$ time steps instrumental states, \textit{recursively by}

$$\IS_0:=\Pt$$
$$\IS_{n+1}:=\prod_{a\in\A}\AP{\prod_{o\in\Ob}\Ht_{\IS_n}}^{-1}(\Delta\Ob)$$

Here, $\prod_{o\in\Ob}\Ht_{\IS_n}$ is a mapping from $\prod_{o\in\Ob}\Cone\IS_n$ to $[0,1]^\Ob$. The inverse image of a convex set (in this case $\Delta\Ob\subseteq[0,1]^\Ob$) under an affine mapping (in this case $\prod_{o\in\Ob}\Ht_{\IS_n}$) is also a convex set.

The semantics of Definition 4 is as follows. Any $\alpha\in\AP{\prod_{o\in\Ob}\Ht_{\IS_n}}^{-1}(\Delta\Ob)$ can be regarded as a pair consisting of some $\alpha_0\in\Delta\Ob$ (the image of $\alpha$ under $\prod_{o\in\Ob}\Ht_{\IS_n}$) and a mapping $\alpha_1:\Supp{\alpha_0}\rightarrow\IS_n$ ($\alpha_1(o)=\eta$ if and only if $\alpha_o$ is of the form $(\lambda\eta,\lambda)$). Given $\theta\in\IS_{n+1}$, $\theta_{a0}$ is the probability distribution over observations resulting from taking action $a$ in state $\theta$, whereas $\theta_{a1}(o)$ is the state resulting from taking action a in state $\theta$ when $o$ is observed.

The point of defining $\IS_n$ in this manner is that (i) different points in $\IS_n$ correspond to states that are truly not equivalent, i.e. can be empirically distinguished (statistically) and (ii) convex combinations of points in $\IS_n$ correspond precisely to probabilistic mixtures. 

$\IS_n$ is a bounded polytope but it is \textit{not} a simplex: we cannot regard it as just probability distributions over some set. For example, if $\Abs{\A}=\Abs{\Ob}=2$, then it's easy to see that $\IS_1$ is a square (one axis is the probability to get a given observation when taking one action, the other axis is the probability for the other action). 

\begin{Huge}Discussion\end{Huge}

TBD % Kamikaze...

\begin{Huge}Proofs\end{Huge}

\textbf{Proposition A.1}\Co{b}

\textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.}\Co{i}

$$A=\int_M{\left(B+CD^E-\sum_{n=0}^\infty\frac{X_n\left[Q^\gamma+R^{\int_0^1\boldsymbol{x}(t)\cdot A_n\left(v_n\otimes w_n\right) dt}\right]}{\alpha_n+\beta_n-\theta_n^\gamma F\left(B+\sin\phi\right)}\land\frac{\int_\mathcal{X}\rho_n}{\frac{df}{dx}\left(x_n\right)-v_n\mathrm{D}F}\right)}d\omega$$

\textbf{Proof of Proposition A.1}\Co{b}

Moo! $\blacksquare$

\end{document}


