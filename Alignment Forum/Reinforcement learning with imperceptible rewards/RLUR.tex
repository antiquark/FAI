%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1.6in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Co}[1]{}
\newcommand{\San}[1]{}
% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Dom}{dom}
\DeclareMathOperator{\Sp}{span}
% autosize delimiters
\newcommand{\AP}[1]{\left(#1\right)}
\newcommand{\AB}[1]{\left[#1\right]}
\newcommand{\AC}[1]{\left\{#1\right\}}
\newcommand{\APM}[2]{\left(#1\;\middle\vert\;#2\right)}
\newcommand{\ABM}[2]{\left[#1\;\middle\vert\;#2\right]}
\newcommand{\ACM}[2]{\left\{#1\;\middle\vert\;#2\right\}}
% probability theory
\newcommand{\Pa}[2]{\underset{#1}{\operatorname{Pr}}\AB{#2}}
\newcommand{\CP}[3]{\underset{#1}{\operatorname{Pr}}\ABM{#2}{#3}}
\newcommand{\PP}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{Pr}}}
\newcommand{\PPP}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{Pr}}}
\newcommand{\E}[1]{\underset{#1}{\operatorname{E}}}
\newcommand{\Ea}[2]{\underset{#1}{\operatorname{E}}\AB{#2}}
\newcommand{\CE}[3]{\underset{#1}{\operatorname{E}}\ABM{#2}{#3}}
\newcommand{\EE}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{E}}}
\newcommand{\EEE}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{E}}}
\newcommand{\Var}[2]{\underset{#1}{\operatorname{Var}}\AB{#2}}
\newcommand{\I}[1]{\underset{#1}{\operatorname{I}}}
\newcommand{\CI}[3]{\underset{#1}{\operatorname{I}}\ABM{#2}{#3}}
\newcommand{\Ia}[2]{\underset{#1}{\operatorname{I}}\AB{#2}}
\newcommand{\II}[2]{\underset{\substack{#1 \\ #2}}{\operatorname{I}}}
\newcommand{\III}[3]{\underset{\substack{#1 \\ #2 \\ #3}}{\operatorname{I}}}
\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}\AP{#1\middle\vert\middle\vert#2}}
\newcommand{\RD}[3]{\operatorname{D}_{#1}\AP{#2\middle\vert\middle\vert#3}}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}
\newcommand{\Dtva}[1]{\operatorname{d}_{\text{tv}}\AP{#1}}
\newcommand{\En}{\operatorname{H}}
\newcommand{\Ena}[1]{\operatorname{H}\AP{#1}}
% power set
\newcommand{\PS}[1]{\mathcal{P}\AP{#1}}
% differential
\newcommand{\D}{\mathrm{d}}
% arg
\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}
% numbers
\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}
% linear algebra
\newcommand{\Id}{\mathrm{I}}
\newcommand{\PD}{\mathrm{PD}}
\newcommand{\PSD}{\mathrm{PSD}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\End}{\mathrm{End}}
\DeclareMathOperator{\Img}{im}
% topology and category theory
\newcommand{\Mor}{\mathrm{Mor}}
\newcommand{\Pt}{\boldsymbol{\mathrm{pt}}}
% empty string
\newcommand{\Estr}{\boldsymbol{\lambda}}
% limits
\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}
% more delimiters
\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\Floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\Chev}[1]{\left\langle #1 \right\rangle}
\newcommand{\Quote}[1]{\left\ulcorner #1 \right\urcorner}
% arrows
\newcommand{\K}{\xrightarrow{\mathrm{k}}}
\newcommand{\PF}{\xrightarrow{\circ}}
% Paper specific
\newcommand{\St}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Ob}{\mathcal{O}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\ER}{\mathrm{E}}
\newcommand{\Con}{\boldsymbol{\mathrm{ConSet}}}
\DeclareMathOperator{\Cone}{Cone}
\newcommand{\Ht}{\mathrm{h}}
\newcommand{\IS}{\mathcal{IS}}
\newcommand{\Prj}{\mathrm{pr}}
\DeclareMathOperator{\RVO}{\dim_{RVO}}
\DeclareMathOperator{\MB}{\dim_{MB}}
\newcommand{\DRVO}{D_{\mathrm{RVO}}}
\newcommand{\DMB}{D_{\mathrm{MB}}}
\newcommand{\N}{\mathrm{N}}
\newcommand{\V}{\mathrm{V}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\EU}{\mathrm{EU}}
\newcommand{\Reg}{\mathrm{R}}
\newcommand{\PSR}{\text{PS}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\CS}{\mathrm{CS}}
\newcommand{\W}{\mathrm{W}}
\newcommand{\AT}{\mathrm{A}}
\newcommand{\THy}{\mathrm{H}_*}
\newcommand{\SHy}{\mathrm{H}}
\newcommand{\Ev}{\mathcal{E}}
\newcommand{\De}{\Delta}
\newcommand{\CSE}{G}
\usepackage{color}

\begin{document}

\textbf{TLDR:}\Co{b} We define a variant of reinforcement learning in which the reward is not perceived directly, but can be estimated at any given moment by some (possibly costly) experiment. We give two definitions of  "imperceptible reward function" and prove their equivalence. We also derive a regret bound for this setting.

\begin{Huge}Background\end{Huge}

In "classical" reinforcement learning the agent perceives the reward signal on every round of its interaction with the environment, whether through a distinct input channel or through some given way to compute the reward from the interaction history so far. On the other hand, we can rather easily imagine agents that optimize properties of their environment that they do not directly perceive. For example, if Alice donates money to the Against Malaria Foundation in order to save someone in Africa, then the result is usually not visible to Alice at the time it occurs, if ever. Similarly, Clippy the paperclip maximizer doesn't always perceive all the paperclips in the universe.

Now, it is possible to define the perceived reward as the \textit{subjective expected value}\Co{i} of the "true" imperceptible reward (see the Results section for details). Although this transformation preserves expected utility, it does \textit{not} preserve Bayesian regret. Indeed, Bayesian regret is the difference between the expected utility attained by the agent and the expected utility attained by a "reference" agent that knows the true environment from the onset. However, after the transformation, the reference agent will behave as if it knows the observable dynamics of the true environment but still pretends not to know the true environment for the purpose of computing the reward. Therefore, regret analysis requires us to consider the imperceptible reward honestly. In fact, as we will see, certain assumptions about the imperceptible reward function are needed even to make the problem learnable at all. Finally, this transformation makes the reward function more complex and hence applying a "generic" reinforcement learning algorithm after the transformation (instead of exploiting the special form of the resulting reward function) might carry a significant computational complexity penalty.

\textbf{Related Work:}\Co{b} [De Blanc 2011]\San{(https://arxiv.org/abs/1105.3821)}\Co{s} studies so called "ontological crises". That is, de Blanc examines the problem of translating a reward function from one ontology into another. Here, we avoid this problem by considering reward functions that are automatically defined in *all* ontologies. That said, it might still be interesting to think about how to specify our type of reward function starting from a reward function that is only defined in one particular ontology. We will return to this in the Discussion section.

[Krueger et al 2016]\San{(https://pdfs.semanticscholar.org/a47f/52b25ce1e56a03876d9c0fd7c45e63270eb4.pdf)}\Co{s} consider a setting where querying the reward is instantaneous and has a fixed cost. This is a special case of our setting, but we allow a much more general type of query and cost. Also, Krueger et al don't derive any theoretical regret bound (but they do present some experimental results).

Finally, multi-armed bandits with partial monitoring are closely related, see for example [Bartok et al 2013]\San{(http://www.mit.edu/~rakhlin/papers/partial_monitoring.pdf)}\Co{s}. However, bandits by definition assume a stateless environment, and also our approach is rather different than what is usually studied in partial monitoring.

The literature study was very cursory and I will be glad to know about prior work I missed!

\begin{Huge}Results\end{Huge}

Partially Observable Markov Decision\ Processes (POMDPs) serve as a natural starting point for thinking about imperceptible rewards. Indeed, it might seem like all we have to do is consider RL in a POMDP environment and let the reward to be a function of the (imperceptible) state. However, this setting is in general unlearnable even given assumptions that rule out traps.

A (finite) POMDP is defined by non-empty finite sets $\St$ (states), $\A$ (actions) and $\Ob$ (observations), the transition kernel $\T:\St\times\A\K\St\times\Ob$ and the reward function $\R:\St\rightarrow[0,1]$. As opposed to a "classical" POMDP, the value of $\R$ is assumed to be imperceptible. The perceptible setting is a special case of the imperceptible setting: we can always encode the reward into the observation.

To formulate a learning problem, we assume $\St$, $\A$ and $\Ob$ to be fixed and an initial state $s_0\in\St$ to be given, while $\T$ and $\R$ are unknown and belong to some hypothesis class $\Hy$:

$$\Hy\subseteq\AC{\St\times\A\K\St\times\Ob}\times\bigg\{\St\rightarrow[0,1]\bigg\}$$

Such a problem can be unlearnable even if $\R$ is known and there are no irreversible events that can happen:

\textbf{Example 1}\Co{b}

\textit{Suppose that $\St=\AC{s_0,s_-,s_+}$, $\A=\AC{a_-,a_+}$, $\Ob=\AC{\bot}$, $\R\AP{s_0}=\frac{1}{2}$, $\R\AP{s_-}=0, \R\AP{s_+}=1$ and $\Hy=\AC{\AP{\T_-,\R},\AP{\T_+,\R}}$ where for any $s\in\St$, $\T_-\APM{s_+,\bot}{s,a_-}=1$, $\T_-\APM{s_-,\bot}{s,a_+}=1$, $\T_+\APM{s_+,\bot}{s,a_+}=1$ and $\T_+\APM{s_-,\bot}{s,a_-}=1$. Since $\Abs{\Ob}=1$, there is no way to gain any information about which hypothesis is correct, however the optimal policies for the two hypotheses are different. Namely, for $\T_+$ we should always take action $a_+$ and for $\T_-$ we should always take action $a_-$.}\Co{i}

To formalize and illustrate the discussion in the Background section, suppose $\Hy$ is Borel and $\zeta\in\Delta\Hy$ is the prior. We can then define the "perceived effective reward" $\ER\R:\AP{\A\times\Ob}^*\rightarrow[0,1]$ by

$$\ER\R(ao):=\CE{\substack{(\T,\R)\sim\zeta\\\AP{s_{n+1},o'_n}\sim\T\AP{s_n,a_n}}}{\R\AP{s_{\Abs{h}}}}{o'=o}$$

It is then easy to see that the $\ER$ operator preserves expected utility: given any policy $\pi:\AP{\A\times\Ob}^*\K \A$ and $m\in\Nats$

$$\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\ER\R\AP{ao_{:m}}}=\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\R\AP{s_m}}$$

and therefore, for any geometric time discount parameter $\gamma\in[0,1)$

$$\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\sum_{n=0}^\infty\gamma^n\ER\R\AP{ao_{:n}}}=\Ea{\substack{(\T,\R)\sim\zeta\\a_n\sim\pi\AP{ao_{:n}}\\\AP{s_{n+1},o_n}\sim\T\AP{s_n,a_n}}}{\sum_{n=0}^\infty\gamma^n\R\AP{s_n}}$$

On the other hand, Bayesian regret is not preserved since, in general

$$\Ea{(\T,\R)\sim\zeta}{\max_{\pi}\Ea{aos\sim\T\pi}{\sum_{n=0}^\infty\gamma^n\ER\R\AP{ao_{:n}}}}\ne\Ea{(\T,\R)\sim\zeta}{\max_{\pi}\Ea{aos\sim\T\pi}{\sum_{n=0}^\infty\gamma^n\R\AP{s_n}}}$$

Here $aos\sim\T\pi$ is shorthand notation for the same probability distribution as before.

Indeed, in Example 1 the LHS of the above is $\frac{1}{2}\cdot\frac{1}{1-\gamma}$ since $\ER\R\equiv\frac{1}{2}$, whereas the RHS is $\frac{1}{2}+\frac{\gamma}{1-\gamma}$.

The pathology of Example 1 comes about because reward is not only imperceptible but entirely \textit{unobservable}\Co{i}. That is, no experiment can produce any information about whether the reward on a given round $n > 0$ was 0 or 1. More specifically, the states $s_-$ and $s_+$ are assigned different rewards, but there is no observable difference between them. It is as if Alice would assign value, not to people in Africa (whose existence and well-being can be measured) but to some Flying Spaghetti Monster s.t. the world behaves exactly the same regardless of its existence or condition.

This observation suggests that, instead of assigning rewards to states which are just abstract labels in a model, we should assign rewards to states that are defined in terms of the observable consequences of the interaction of the agent with the environment. This leads us to the notion of "instrumental state", which we will now define formally.

First, we introduce some technical definitions for notational convenience.

\textbf{Definition 1}

\textit{$\Con$ is the category whose objects are pairs $(V,C)$ where $V$ is a real vector space and $C$ is a convex subset of $V$, and whose morphisms are}\Co{i}

$$\Mor\AP{(V,C),(W,D)}:=$$
$$\ACM{f:C\rightarrow D}{\exists A \in \Hom(V,W),w\in W \forall v\in C: f(v)=Av+w}$$

We omit describing identity and composition of morphisms since they are obvious.

It is easy to see that $\Con$ has arbitrary limits. In particular, $\Con$ has a final object that we denote by $\Pt$ (the one point set), products ($(V,C)\times(W,D)\cong(V\oplus W,C\times D)$) and inverse limits of sequences. For any finite set $A$, $\AP{\Reals^A,\Delta A}$ is an object in $\Con$. Also, we will sometimes abuse notation by regarding $C$ as an object of $\Con$ instead of $(V,C)$ (i.e. making $V$ implicit).

\textbf{Definition 2}\Co{b}

\textit{The functor $\Cone:\Con\rightarrow\Con$ is defined by}\Co{i}

$$\Cone{(V,C)}:=\AP{V\oplus\Reals,\ACM{(\lambda v,\lambda)}{\lambda\in[0,1], v\in C}}$$
$$\AP{\Cone{f}}(\lambda v, \lambda):=\AP{\lambda f(v), \lambda}$$

\textbf{Definition 3}\Co{b}

\textit{For any $C\in\Con$, we define $\Ht_{C}:\Cone{C}\rightarrow[0,1]$ by}\Co{i}

$$\Ht_{C}(\lambda v, \lambda):=\lambda$$

Note that $\Ht$ is a natural transformation from $\Cone$ to the constant functor with value $[0,1]$.

Given $C\in\Con$ and $x\in \Cone C$ s.t. $x=(u,\lambda)$ for $\lambda > 0$, we denote $[x]:=\lambda^{-1}u\in C$.

Now, we are ready to define instrumental states. We fix the sets $\A$ and $\Ob$.

\textbf{Definition 4}\Co{b}

\textit{For any $n\in\Nats$, we define} $\IS_n\in\Con$, the space of $n$ time step instrumental states, \textit{recursively by}

$$\IS_0:=\Pt$$
$$\IS_{n+1}:=\prod_{a\in\A}\AP{\prod_{o\in\Ob}\Ht_{\IS_n}}^{-1}(\Delta\Ob)$$

Here, $\prod_{o\in\Ob}\Ht_{\IS_n}$ is a mapping from $\prod_{o\in\Ob}\Cone\IS_n$ to $[0,1]^\Ob$. The inverse image of a convex set (in this case $\Delta\Ob\subseteq[0,1]^\Ob$) under an affine mapping (in this case $\prod_{o\in\Ob}\Ht_{\IS_n}$) is also a convex set.

The semantics of Definition 4 is as follows. Any $\alpha\in\AP{\prod_{o\in\Ob}\Ht_{\IS_n}}^{-1}(\Delta\Ob)$ can be regarded as a pair consisting of some $\alpha'\in\Delta\Ob$ (the image of $\alpha$ under $\prod_{o\in\Ob}\Ht_{\IS_n}$) and a mapping $\alpha'':\Supp{\alpha'}\rightarrow\IS_n$ defined by $\alpha''(o):=\AB{\alpha_o}$. Given $\theta\in\IS_{n+1}$, $\theta_{a}'$ is the probability distribution over observations resulting from taking action $a$ in state $\theta$, whereas $\theta_{a}''(o)$ is the state resulting from taking action a in state $\theta$ conditional on observing $o$. This semantics can be made more formal as follows:

\Co{In the following, we will use the notation $\theta\mid a$ instead of $\theta_a$ and $\theta\mid ao$ instead of $\theta_{a1}(o)$.}

\textbf{Definition 5}\Co{b}

\textit{Given $\theta\in\IS_n$, we define $\Dom\theta\subseteq\AP{\A\times\Ob}^*$ recursively as follows:}

1. $\Estr\in\Dom\theta$

2. \textit{For all $h\in\AP{\A\times\Ob}^*$, $a\in\A$ and $o\in\Ob$: $aoh\in\Dom\theta$ iff $n>0$, $\Ht_{\IS_{n-1}}(\theta_{ao})>0$ and $h\in\Dom\AB{\theta_{ao}}$.}

\textbf{Definition 6}

\textit{Given $\theta\in\IS_n$, $h\in\Dom\theta$ and $a\in\A$, and assuming that $\Abs{h}<n$, we recursively define $\theta(ha)\in\Delta\Ob$ by}\Co{i}

1. For $h=\Estr$: $\theta( o\mid a):=\Ht_{\IS_{n-1}}(\theta_{ao})$.

2. For $h=bph'$ with some $b\in\A$, $p\in\Ob$ and $h'\in\AP{\A\times\Ob}^*$: $\theta(ha):=\AB{\theta_{bp}}(h'a)$.

The point of defining $\IS_n$ in this manner is that (i) different points in $\IS_n$ correspond to states that are truly not equivalent, i.e. can be empirically distinguished (statistically) and (ii) convex combinations of points in $\IS_n$ correspond precisely to probabilistic mixtures. 

$\IS_n$ is a bounded polytope but in general it is \textit{not} a simplex: we cannot regard it as just probability distributions over some set. For example, if $\Abs{\A}=\Abs{\Ob}=2$, then it's easy to see that $\IS_1$ is a square (one axis is the probability to get a given observation when taking one action, the other axis is the probability for the other action). On the other hand, if $\Abs{\A}=1$ then $\IS_n$ \textit{is}\Co{i} a simplex: it is canonically isomorphic to $\Delta\Ob^n$.

There are a natural morphisms $\Prj_n:\IS_{n+1}\rightarrow\IS_n$ whose semantics is forgetting about the behavior of the state at time step $n$:

\textbf{Definition 6}\Co{b}

\textit{We define $\Prj_n:\IS_{n+1}\rightarrow\IS_n$ for any $n\in\Nats$ recursively. $\Prj_0$ is the unique morphism from $\IS_1$ to $\IS_0\cong\Pt$. For any $n\in\Nats$, $\Prj_{n+1}$ is given by}\Co{i}

$$\Prj_{n+1}(\theta)_{ao}:=\AP{\Cone{\Prj_n}}\AP{\theta_{ao}}$$

We thereby get a sequence $\IS_0 \leftarrow \IS_1 \leftarrow \IS_2 \leftarrow \ldots$

\textbf{Definition 7}\Co{b}

\textit{We define}\Co{i} $\IS_\omega$, the space of (infinite time step) instrumental states \textit{by}\Co{i}

$$\IS_\omega:=\varprojlim_n \IS_n$$

\textit{We denote the canonical projections by}\Co{i} $\Prj^n:\IS_\omega\rightarrow\IS_n$.

Of course, $\IS_\omega$ can also be regarded as the space of all possible stochastic environments. Specifically, we have:

\textbf{Definition 8}\Co{b}

\textit{For any $\mu\in\IS_\omega$ we define $\Dom\mu\in\AP{\A\times\Ob}^*$ by}\Co{i}

$$\Dom\mu:=\bigcup_{n=0}^\infty \Dom\Prj^n\mu$$

\textbf{Definition 9}\Co{b}

\textit{Given $\mu\in\IS_\omega$, $h\in\Dom\theta$ and $a\in\A$, we define $\mu(ha)\in\Delta\Ob$ by}\Co{i}

$$\mu(ha):=\Prj^{\Abs{h}+1}\mu(ha)$$

For each $n\in\Nats$, $\IS_n$ is finite-dimensional and therefore has a natural topology. $\IS_\omega$ also becomes a topological space by equipping it with the inverse limit topology. Since the $\IS_n$ are closed and bounded they are compact, and therefore $\IS_\omega$ is also compact by Tychonoff's theorem. In the special case $\Abs{\A}=1$, $\IS_\omega\cong\Delta\Ob^\omega$ and the inverse limit topology is the weak topology for probability measures, defined w.r.t. the product topology on $\Ob^\omega$.

We can now give the first definition of an imperceptible reward function: a continuous affine function $\R: \IS_\omega \rightarrow \Reals$. Why affine? A convex linear combination of instrumental states is empirically indistinguishable from a probabilistic lottery. If we assign expected values to probabilistic lotteries (as we should by the VNM theorem), then we must also assign them to convex linear combinations of instrumental states: otherwise our reward again depends on entirely unobservable parameters of our model.

An alternative approach is to consider the notion of "experiment" explicitly.

We will use the notation $\mathcal{X}^{\leq\omega}:=\mathcal{X}^*\sqcup\mathcal{X}^\omega$. Given a logical condition $\phi$, the symbol $\boldsymbol{1}_\phi$ will denote 1 when $\phi$ is true and 0 when $\phi$ is false.

\textbf{Definition 10}\Co{b}

\textit{Given $\pi: \AP{\A\times\Ob}^*\K\A\sqcup\{\bot\}$ and $\mu\in\IS_\omega$, we define $\mu\pi\in\Delta\AP{\A\times\Ob}^{\leq\omega}$ by}\Co{i}

$$\Pa{ao\sim\mu\pi}{a_n=b}=\Ea{ao\sim\mu\pi}{\boldsymbol{1}_{\Abs{ao}\geq n}\pi\APM{b}{ao_{:n}}}$$
$$\Pa{ao\sim\mu\pi}{o_n=p}=\Ea{ao\sim\mu\pi}{\boldsymbol{1}_{\Abs{ao}> n}\mu\APM{p}{ao_{:n}a_n}}$$

\textit{$\pi$ is said to be a}\Co{i} terminable policy \textit{when for any $\mu\in\IS_\omega$}\Co{i}

$$\Pa{h\sim\mu\pi}{\Abs{h}<\infty} = 1$$

That is, a terminable policy is allowed to produce a special token $\bot$ which terminates the "experiment" (instead of choosing an action), and we require that, for any environment, this token will be eventually produced with probability 1.

\textbf{Definition 11}\Co{b}

\textit{Given a terminable policy $\pi$ and a function $r:\AP{\A\times\Ob}^*\rightarrow[0,1]$, we define the function $\R_{\pi r}:\IS_\omega\rightarrow[0,1]$ by
}\Co{i}
$$\R_{\pi r}(\mu):=\Ea{h\sim\mu\pi}{r(h)}$$

This gives us a second definition of imperceptible reward functions. In fact, the two definitions are equivalent:

\textbf{Proposition 1}\Co{b}

\textit{For any terminable policy $\pi$ and function $r:\AP{\A\times\Ob}^*\rightarrow[0,1]$, $\R_{\pi r}$ is continuous and affine.}\Co{i}

\textbf{Theorem 1}\Co{b}

\textit{For any continuous affine $\R:\IS_\omega\rightarrow[0,1]$, there exists a terminable policy $\pi$ and $r:\AP{\A\times\Ob}^*\rightarrow[0,1]$ s.t. $\R = \R_{\pi r}$.}\Co{i}

In other words, for any imperceptible reward function (in the sense of the first definition) there is some experiment the agent can do which yields an \textit{unbiased estimate}\Co{i} of the reward for the instrumental state that existed at the beginning of the experiment.

To illustrate, imagine that Clippy has access to a black box with a collection of levers on its outside. Pulling the levers produces some sounds that hint at what happens inside the box, but are not enough to determine it with certainty. The box has a shuttered glass window, whose shutter can be opened from the outside. Through the window, Clippy can see a jumble of scrap metal, mechanical manipulators that are controlled by the levers (and can be used to shape the scrap metal), and also a few mice running around the box and wreaking havoc. However, it is not possible to control the manipulators while the shutter is open. Worse, while opening the shutter allows seeing a snapshot of the shape of the metal, it also causes the manipulators to move chaotically, ruining this shape. So, Clippy can experiment with the levers and occasionally open the shutter to test the result. However, in order to produce and maintain paperclips inside the box, the shutter has to be kept closed (and the paperclips hidden) most of the time.

It is also possible to consider reward functions of the more general form $\R:\AP{\A\times\Ob}^*\times\IS_\omega\rightarrow\Reals$, required to be continuous and affine in the second argument. Such a reward function depends both on the current (unknown) instrumental state of the environment and the observable history so far...

% description via terminable polies

% Lipschitz and finite expected time

\begin{Huge}Discussion\end{Huge}

TBD
% Kamikaze...
% Specification a la de Blanc...

\begin{Huge}Proofs\end{Huge}

\textbf{Proposition A.1}\Co{b}

\textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.}\Co{i}

$$A=\int_M{\left(B+CD^E-\sum_{n=0}^\infty\frac{X_n\left[Q^\gamma+R^{\int_0^1\boldsymbol{x}(t)\cdot A_n\left(v_n\otimes w_n\right) dt}\right]}{\alpha_n+\beta_n-\theta_n^\gamma F\left(B+\sin\phi\right)}\land\frac{\int_\mathcal{X}\rho_n}{\frac{df}{dx}\left(x_n\right)-v_n\mathrm{D}F}\right)}d\omega$$

\textbf{Proof of Proposition A.1}\Co{b}

Moo! $\blacksquare$

\end{document}


