% --------------------------------------------------------------------------- %
% Created with Brian Amberg's LaTeX Poster Template.                          %
% --------------------------------------------------------------------------- %
\documentclass[a0paper,portrait]{baposter}

\usepackage{relsize}            % For \smaller
\usepackage{url}                        % For \url
% \usepackage{epstopdf} % Included EPS files automatically converted to PDF to include with pdflatex
\usepackage{csquotes}
\usepackage[boxruled]{algorithm2e}

%%% Macros %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\Comment}[1]{}

%%% Global Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\graphicspath{{pix/}}   % Root directory of the pictures 
% \tracingstats=2                       % Enabled LaTeX logging with conditionals

%%% Color Definitions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{bordercol}{RGB}{40,40,40}
\definecolor{headercol1}{RGB}{186,215,230}
\definecolor{headercol2}{RGB}{80,80,80}
\definecolor{headerfontcol}{RGB}{0,0,0}
\definecolor{boxcolor}{RGB}{186,215,230}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Utility functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Save space in lists. Use this after the opening of the list %%%%%%%%%%%%%%%%
\newcommand{\compresslist}{
        \setlength{\itemsep}{1pt}
        \setlength{\parskip}{0pt}
        \setlength{\parsep}{0pt}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Document Start %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\typeout{Poster rendering started}

%%% Setting Background Image %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\background{
        \begin{tikzpicture}[remember picture,overlay]%
        %\draw (current page.north west)+(-2em,2em) node[anchor=north west]
        %{\includegraphics[height=1.1\textheight]{background}};
        \end{tikzpicture}
}

%%% General Poster Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Eye Catcher, Title, Authors and University Images %%%%%%%%%%%%%%%%%%%%%%
\begin{poster}{
        grid=false,
        % Option is left on true though the eyecatcher is not used. The reason is
        % that we have a bit nicer looking title and author formatting in the headercol
        % this way
        %eyecatcher=false, 
        borderColor=bordercol,
        headerColorOne=headercol1,
        headerColorTwo=headercol2,
        headerFontColor=headerfontcol,
        % Only simple background color used, no shading, so boxColorTwo isn't necessary
        boxColorOne=boxcolor,
        headershape=roundedright,
        headerfont=\Large\sf\bf,
        textborder=rectangle,
        background=user,
        headerborder=open,
  boxshade=plain
}
%%% Eye Cacther %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
        Eye Catcher, empty if option eyecatcher=false - unused
}
%%% Title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\sf\bf
        Delegative Reinforcement Learning: learning to avoid traps with a little help
}
%%% Authors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
        \vspace{1em} Vanessa Kosoy\\
        {\smaller vanessa.kosoy@intelligence.org}
}

%%% Photo %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
        \fbox{
        \begin{minipage}{14em}
                %\includegraphics[width=10em,height=4em]{colbud_logo}
                %\includegraphics[width=4em,height=4em]{elte_logo} \\
                %\includegraphics[width=10em,height=4em]{dynanets_logo}
                %\includegraphics[width=4em,height=4em]{aitia_logo}
        \end{minipage}
}
}

\headerbox{Overview}{name=overview,column=0,row=0,span=1}{
Most known theoretical regret bounds for reinforcement learning are either episodic or assume an environment without traps. Such regret bounds are inadequate for applications in which critical failures are too costly to be an acceptable side effect of learning, and it is impossible to train the algorithm sufficiently well using simulations only. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We call this setting \enquote{delegative reinforcement learning} (DRL). An equivalent setting was introduced in \cite{Clouse1997} under the name "ask for help", but they proved no regret bound.}

\headerbox{Traps}{name=traps,column=0,below=overview}{
Traps are actions that incur irrecoverable long-term loss of reward. For example, in the MDP below, action $b$ in state $s2$ is a trap (it reduces the maximal achievable long-term average reward from $\frac{1}{3}$ to $\frac{1}{5}$) whereas action $a$ in state $s0$ is suboptimal but isn't a trap.

\includegraphics[width=\linewidth]{trap}
}

\headerbox{Sanity}{name=sanity,column=0,below=traps}{
Our regret bound is applicable when the external advisor satisfies conditions we call \enquote{$\epsilon$-sanity} ($\epsilon$ is a parameter in $(0,1)$ that appears in the regret bound coefficient). These conditions are:
\begin{enumerate}
\item Never take trap actions.
\item In any state, there is an optimal action which is taken with probability more than $\epsilon$.
\end{enumerate}
Such a policy doesn't have to be especially good: in our example, its average long-term reward can be $O(\epsilon)$.
}

\headerbox{References}{name=references,column=0,below=sanity}{
\smaller                                                                                                        % Make the whole text smaller
\vspace{-0.4em}                                                                                 % Save some space at the beginning
\bibliographystyle{plain}                                                       % Use plain style
\renewcommand{\section}[2]{\vskip 0.05em}               % Omit "References" title
\begin{thebibliography}{1}                                                      % Simple bibliography with widest label of 1
\itemsep=-0.01em                                                                                % Save space between the separation
\setlength{\baselineskip}{0.4em}                                        % Save space with longer lines
\bibitem{Clouse1997} J. Clouse: \emph{On Integrating Apprentice Learning and Reinforcement Learning} (1997)
\bibitem{Osband2013} Ian Osband, Benjamin Van Roy and Daniel Russo: \emph{(More) Efficient Reinforcement Learning via Posterior Sampling}, 26th International Conference on Neural Information Processing Systems (2013)
\end{thebibliography}
}

\headerbox{Acknowledgements}{name=acknowledgements,column=0,below=references, above=bottom}{
\smaller                                                % Make the whole text smaller
\vspace{-0.4em}                 % Save some space at the beginning
This work was supported by the Machine Intelligence Research Institute (Berkeley, California, USA).
} 

\headerbox{Setting}{name=setting,span=2,column=1,row=0}{
On every round, the DRL agent takes either an action from the MDP action set $\mathcal{A}$, or the \enquote{delegate} action $\bot$. Upon delegation, the external advisor is asked to select an action from $\mathcal{A}$. In either case, the MDP environment is acted upon by an action from $\mathcal{A}$, and changes its state accordingly, in a way visible both to the DRL agent and the advisor. 
\begin{center}
\includegraphics[angle=0,width=0.35\linewidth]{setting}
\end{center}
}

\headerbox{Algorithm}
{name=algorithm,span=2,column=1,below=setting}{
The algorithm used to prove the regret bound is based on PSRL \cite{Osband2013} with artificial \enquote{episodes} of length $T$. The main differences are:
\begin{enumerate}
\item We only take an action if it is known to be safe (not trap) according to current belief state, otherwise we delegate.
\item Hypotheses whose probability drops below some constant $\eta$ are discarded from the belief state.
\end{enumerate}

The regret bound can be expressed in terms of $T$ and $\eta$, which also allows setting them to values that give optimal asymptotics (see below).

\RestyleAlgo{boxed}
\LinesNumbered
\DontPrintSemicolon
\SetKwFor{Loop}{InfiniteLoopBegin}{}{InfiniteLoopEnd}
\SetKwData{Z}{belief}
\SetKwData{J}{hypothesis}
\SetKwData{S}{state}
\SetKwData{RA}{agentAction}

\begin{algorithm}[H]

\SetKwData{NS}{newState}
\SetKwData{AA}{advisorAction}
\SetKwData{GA}{isSafeAction}

\Z$\leftarrow$ uniform distribution over $[N]$\;
\Loop{}{\label{ln:loop_begin}
        \J$\leftarrow$ sample the distribution \Z\;\label{ln:sample_hypothesis}
        \For{$m=0$ \KwTo $T-1$}{
                \eIf{$\Z(\J) > 0$}{
                        \RA$\leftarrow$ optimal action for \S given \J\;
                        \If{\RA \normalfont{might be unsafe}}{\label{ln:delegate_cond}
                                \RA$\leftarrow\bot$\label{ln:delegate_primary}
                        }
                                             
                }{
                        \RA$\leftarrow$ some certainly safe action or $\bot$ if there are none
                }
                take action \RA\;
                update \Z according to observations\;
                discard all hypotheses in \Z with probability below $\eta$\;               
        }
}

\end{algorithm}

}

\headerbox{Regret Bound}
{name=regret,span=2,column=1,below=algorithm,above=bottom}
{
We derive a Bayesian regret bound assuming a uniform prior over a finite number of hypotheses and geometric time discount. Denoting $\mathcal{R}_n$ the reward received on time step $n$, we define:

$$\mathrm{Reg}:=\mathrm{E}^\star\left[(1-\gamma)\sum_{n=0}^\infty {\gamma^n \mathcal{R}_n}\right]-\mathrm{E}\left[(1-\gamma)\sum_{n=0}^\infty {\gamma^n \mathcal{R}_n}\right]$$

Here, $\mathrm{E}^\star$ assumes the optimal policy for the true environment is followed, whereas $\mathrm{E}$ assumes that the algorithm above is followed. The regret bound is:

$$\mathrm{Reg}=O\left((1-\gamma)^{\frac{1}{4}}\right)$$

Denoting $\mathrm{D}$ the total number of delegations, we also prove:

$$\Pr\left[\mathrm{D}>K\right]=O\left((1-\gamma)^{\frac{1}{4}}\right)+\frac{1}{K}O\left((1-\gamma)^{-\frac{1}{4}}\right)$$

In the paper, we also give detailed expressions for the coefficients implicit in the $O$-notation.
}

\end{poster}
\end{document}

