%&latex
\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{csquotes}

\begin{document}

%+Title
\title{Learning to avoid traps with a little help}
\author{Vadim Kosoy \\ \href{mailto:vadim.kosoy@intelligence.org}{vadim.kosoy@intelligence.org}}
\date{}
\maketitle
%-Title

%+Abstract
\begin{abstract}
    Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Thompson sampling supplemented by a subroutine that decides which actions should be delegated. Importantly, the algorithm is not anytime, since the confidence threshold for acting without delegating must be adjusted according to the target time discount. We also demonstrate that this setting can handle situations in which the reward signal and the advisor become unreliable in particular environment states (assuming these states can be avoided.) Currently, our analysis is limited to Markov decision processes with a finite number of states. 
\end{abstract}
%-Abstract

%+Contents
%\tableofcontents
%-Contents

\section{Introduction}

A \emph{reinforcement learning agent} is a system that interacts with an unknown environment in a manner that is designed to maximize the expectation of a utility function that can be written as a sum rewards over time (sometimes weighted by a time-discount function.) A standard metric for evaluating the performance of such an agent is the \emph{regret}: the difference between the expected utility of the agent in a given environment, and the expected utility of an optimal policy for the same environment. This metric allows formalizing the notion of \enquote{the agent \emph{learns} the environment} by requiring that the regret has sublinear growth in the planning horizon (usually assuming the utility function is a finite, undiscounted, sum of rewards.) For example, if we consider stateless environments, reinforcement learning reduces to a multi-armed bandit for which algorithms with guaranteed sublinear regret bounds are well-known \cite{TBD}.

However, the desideratum of sublinear regret is impossible to achieve even for a finite class of environments without making further assumptions, and this is because of the possible presence of \enquote{traps}. A trap is a state which, once reached, forces a linear lower bound on regret. For example, see figure~\ref{TBD}. The agent starts at state $s_1$, and as long as it takes action $a$, it receives a reward of $1$. However, if it ever takes action $b$, it will reach state $s_2$ and remain there, receiving a reward of $0$ forever, whatever it does. Thus, $s_2$ is a trap. On the other hand, it is impossible to design an algorithm which guarantees never entering traps for an arbitrary environment. For example, the environment on figure~\ref{TBD} has the same structure except that taking action $a$ leads into the trap. Thus, if the transition matrix is not known a priori, no algorithm can learn the correct behavior, and every algorithm will have linear regret in at least one of the two environments.

There are two widespread approaches to deriving regret bounds which circumvent this problem. One is simply assuming that the environment contains no traps in some formal sense \cite{TBD}. The other is \enquote{episodic learning \cite{TBD}.} In episodic learning, the timeline is divided into intervals (\enquote{episodes}) and, regret is defined s.t. the contribution of each episode is the difference between following the given policy and following the given policy \emph{during previous episodes} but the optimal policy in the current episode. Such a metric consider entering a trap as a fatal event since in following episodes this even will be considered as \enquote{given.} That is, a policy that enters trap can still achieve sublinear regret in this sense. In fact, algorithms designed to achieve sublinear regret for sufficiently general classes of environments have the property that they eventually enter \emph{every trap they encounter} (such algorithms have a random exploration phase, like e.g. $\epsilon$-exploration in Q-learning.)

In terms of practical applications, it means that most known approaches to reinforcement learning that have theoretical performance guarantees either assume that no mistake is \enquote{fatal}, or that numerous \enquote{fatal} mistakes in the training process are acceptable. These assumptions are unacceptable in applications such as controlling a very expensive, breakable piece of machinery (e.g. spaceship) or performing a task that involves significant risk to human lives (e.g. surgery or rescue,) assuming that the algorithm \emph{cannot} be reliably trained in a simulation since the simulation doesn't reflect all the intricacies of the physical world.

This problem clearly cannot be overcome without using prior knowledge about the environment. In itself, prior knowledge is not such a strong assumption, since at least for any task that can be accomplished by a person, this prior knowledge is already available to us. The challenge is then transferring this knowledge to algorithm. This transfer can be accomplished either by manually transforming the knowledge into a formal mathematical specification, or by establishing a learning protocol that involves a human in the loop. Since human knowledge is often complex, difficult to formalise and partly intuitive, the latter option seems especially attractive.

TBD

\section{Document Class Options}
The typesetting specification selected by this document template
uses the default class options. There are a number of class options
supported by this document class. The available options include 
setting the paper size, the point size of the font used in the 
document body and others.

\subsection{Customizing Class Options}
Select `Insert', `Document Properties ...', the `Generic' tab
and then modify desired class options in appeared dialog.
Changes will be applied after pressing the 'OK' button.

%+Bibliography
\begin{thebibliography}{99}
\bibitem{Label1} ...
\bibitem{Label2} ...
\end{thebibliography}
%-Bibliography

\end{document}


