%&latex
\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{csquotes}

\begin{document}

%+Title
\title{Learning to avoid traps with a little help}
\author{Vadim Kosoy \\ \href{mailto:vadim.kosoy@intelligence.org}{vadim.kosoy@intelligence.org}}
\date{}
\maketitle
%-Title

%+Abstract
\begin{abstract}
    Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Thompson sampling supplemented by a subroutine that decides which actions should be delegated. Importantly, the algorithm is not anytime, since the confidence threshold for acting without delegating must be adjusted according to the target time discount. We also demonstrate that this setting can handle situations in which the reward signal and the advisor become unreliable in particular environment states (assuming these states can be avoided.) Currently, our analysis is limited to Markov decision processes with a finite number of states. 
\end{abstract}
%-Abstract

%+Contents
%\tableofcontents
%-Contents

\section{Introduction}

A \emph{reinforcement learning agent} is a system that interacts with an unknown environment in a manner that is designed to maximize the expectation of a utility function that can be written as a sum rewards over time (sometimes weighted by a time-discount function.) A standard metric for evaluating the performance of such an agent is the \emph{regret}: the difference between the expected utility of the agent in a given environment, and the expected utility of an optimal policy for the same environment. This metric allows formalizing the notion of \enquote{the agent \emph{learns} the environment} by requiring that the regret has sublinear growth in the planning horizon (usually assuming the utility function is a finite, undiscounted, sum of rewards.) For example, if we consider stateless environments, reinforcement learning reduces to a multi-armed bandit for which algorithms with guaranteed sublinear regret bounds are well-known \cite{TBD}.

However, the desideratum of sublinear regret is impossible to achieve even for a finite class of environments without making further assumptions, and this is because of the possible presence of \enquote{traps}. A trap is a state which, once reached, forces a linear lower bound on regret. For example, see figure~\ref{TBD}. The agent starts at state $s_1$, and as long as it takes action $a$, it receives a reward of $1$. However, if it ever takes action $b$, it will reach state $s_2$ and remain there, receiving a reward of $0$ forever, whatever it does. Thus, $s_2$ is a trap. On the other hand, it is impossible to design an algorithm which guarantees never entering traps for an arbitrary environment. For example, the environment on figure~\ref{TBD} has the same structure except that taking action $a$ leads into the trap. Thus, if the transition matrix is not known a priori, no algorithm can learn the correct behavior, and every algorithm will have linear regret in at least one of the two environments.

TBD

\section{Document Class Options}
The typesetting specification selected by this document template
uses the default class options. There are a number of class options
supported by this document class. The available options include 
setting the paper size, the point size of the font used in the 
document body and others.

\subsection{Customizing Class Options}
Select `Insert', `Document Properties ...', the `Generic' tab
and then modify desired class options in appeared dialog.
Changes will be applied after pressing the 'OK' button.

%+Bibliography
\begin{thebibliography}{99}
\bibitem{Label1} ...
\bibitem{Label2} ...
\end{thebibliography}
%-Bibliography

\end{document}


