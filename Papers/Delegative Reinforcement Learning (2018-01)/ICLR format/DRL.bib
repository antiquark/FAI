@Book{Feinberg2002,
	author = {},
	editor = {Feinberg, Eugene A. and Shwartz, Adam},
	title = {Handbook of Markov Decision Processes},
	publisher = {Springer},
	year = 2002
}
@Article{Garcia2015,
	author = {Garc\'{\i}a, Javier and Fern\'{a}ndez, Fernando},
	title = {A Comprehensive Survey on Safe Reinforcement Learning},
	journal = {J. Mach. Learn. Res.},
	issue_date = {January 2015},
	volume = 16,
	number = 1,
	month = jan,
	year = 2015,
	issn = {1532-4435},
	pages = {1437--1480},
	numpages = 44,
	url = {http://dl.acm.org/citation.cfm?id=2789272.2886795},
	acmid = 2886795,
	publisher = {JMLR.org},
	keywords = {reinforcement learning, risk sensitivity, safe exploration, teacher advice}
}
@TechReport{Clouse1997,
	author = {Clouse, J.},
	title = {On Integrating Apprentice Learning and Reinforcement Learning},
	year = 1997,
	source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1997-026},
	publisher = {University of Massachusetts},
	address = {Amherst, MA, USA}
}
@article{Bubeck2012,
url = {http://dx.doi.org/10.1561/2200000024},
year = {2012},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
doi = {10.1561/2200000024},
issn = {1935-8237},
number = {1},
pages = {1-122},
author = {Sébastien Bubeck and Nicolò Cesa-Bianchi}
}
@InProceedings{Nguyen2013,
  title = 	 {Competing with an Infinite Set of Models in Reinforcement Learning},
  author = 	 {Phuong Nguyen and Odalric-Ambrym Maillard and Daniil Ryabko and Ronald Ortner},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {463--471},
  year = 	 {2013},
  editor = 	 {Carlos M. Carvalho and Pradeep Ravikumar},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 {29 Apr--01 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/nguyen13a.pdf},
  url = 	 {http://proceedings.mlr.press/v31/nguyen13a.html},
  abstract = 	 {We consider a reinforcement learning setting where the learner also has to deal with the problem of finding a suitable state-representation function from a given set of models. This has to be done while interacting with the environment in an online fashion (no resets), and the goal is to have small regret with respect to any Markov model in the set. For this setting, recently the BLB algorithm has been proposed, which achieves regret of order T^2/3, provided that the given set of models is finite. Our first contribution is to extend this result to a countably infinite set of models. Moreover, the BLB regret bound suffers from an additive term that can be exponential in the diameter of the MDP involved, since the diameter has to be guessed. The algorithm we propose avoids  guessing the diameter, thus improving the regret bound.}
}
@incollection{Osband2014,
title = {Model-based Reinforcement Learning and the Eluder Dimension},
author = {Osband, Ian and Van Roy, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {1466--1474},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension.pdf}
}
@article{Russo2016,
 author = {Russo, Daniel and Van Roy, Benjamin},
 title = {An Information-theoretic Analysis of Thompson Sampling},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 month = jan,
 year = {2016},
 issn = {1532-4435},
 pages = {2442--2471},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=2946645.3007021},
 acmid = {3007021},
 publisher = {JMLR.org},
 keywords = {Thompson sampling, information theory, mutli-armed bandit, online optimization, regret bounds},
}
@INPROCEEDINGS{Bartlett2009,
    author = {Peter L. Bartlett},
    title = {REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs},
    booktitle = {In Proceedings of the 25th Annual Conference on Uncertainty in Artificial Intelligence},
    year = {2009}
}
@inproceedings{Osband2013,
 author = {Osband, Ian and Van Roy, Benjamin and Russo, Daniel},
 title = {(More) Efficient Reinforcement Learning via Posterior Sampling},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {3003--3011},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999947},
 acmid = {2999947},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 