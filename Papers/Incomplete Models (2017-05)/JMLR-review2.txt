Review report on "Forecasting using incomplete models" by Vanessa Kosoy.

In this paper the problem of forecasting an infinite sequence of future observations based on past observations is considered. The forecaster does not know the exact mechanism that generates the observed sequence. He can only observe a certain initial fragment of it of sufficiently large size. Some weak assumption is used: it is assumed that the observed sequence is generated by an unknown probability measure (probabilistic source) belonging to some convex set in the space of all probability measures. More precisely, each such class represents a property inherent in sequences generated by the sources from a given class.
The exact class of such measures is also not known to the predictor - only a countable sequence of such classes is given. Author calls such classes incomplete models.
This approach looks as a generalization of Ray Solomonoff method of universal forecasting (see Li and Vitanyi book) and Levin - Gacs neutral measure (see http://www.cs.bu.edu/~gacs/papers/ait-notes.pdf).

An example of such a problem is presented by the author: "The environment might be very complex, but at the same time it might possess some simple features, and it is these features that the forecast must capture." For example, a sequence of binary observations can be considered, such that whatever is the probabilistic behavior of each even observation the following odd observation coincides with it. The forecaster should asymptotically assign high probability to this property.  

A forecaster is a function which assign to each finite initial fragment x^n of infinite sequence x a probability distribution F(x^n) in the set of all infinite continuations of x^n. Also, the corresponding approximations M_n(x^n) of any model M are defined.

The main result is Theorem 1 which says that for any sequence of incomplete models (convex classes of probability measures) "a universal forecaster" F(x^n) exists such that for any incomplete model M (from a given countable class), for almost all sequences generated by any measure from M_n the distance between F(x^n) and the class of measures M_n(x^n) tends to 0 as n tends to infinity, i.e, F_n asymptotically predicts a feature of infinite sequences presented by the class M. 

The proof of Theorem 1 occupies 20 of 29 pages of the work. The proof consists of two parts. In the first part, for any countable set of gamblers a dominant forecaster is constructed (Theorem 2). The construction is based on Ky-Fan mininimax theorem and has analogs with Vovk-Shafer 
(Besides [6], see (i) Alexey Chernov and Vladimir Vovk. Prediction with Advice of Unknown Number of
Experts. arXiv:1006.0475v1 [cs.LG] 2 Jun 2010 (Section 3.2); 
(ii) Vovk V., Shafer G. Good randomized sequential probability forecasting is always possible. J. Royal Stat. Soc. B. 2005. V. 67. P. 747–763) and other approaches for construction the universal forecasting strategies. 

In the second part, for any incomplete model a "prudent" and "savvy" gambling strategy is constructed using a theorem from [9]. This gambling strategy controls the convergens to incomplete model. The universal forecaster from the first part dominates this strategy that implies 
the needed result.

Through main steps of the proof seems correct, motivations of the notions of prudent 
and savvy gambling strategies did not presented, so, the idea of the second part of the proof can be unclear for a reader. 

The approach and main result of this work are novel and of a fundamental nature and certainly deserves publication after essential clarification and explanation of result and proof.

There are two issues with this paper. 
The first one is that reading this paper requires a lot of efforts - the results are presented in the most general abstract form, a lot of notations and definitions are introduced during the presentation. Even, the examples given are unrealistic and of a general nature. 

Some explanation should be useful. For example, maybe to present results and some proofs for Cantor space of infinite binary sequences with standard topology 
and for some natural sequence of incomplete models. 

The second issue with this paper is that it does not fit well with the JMLR topics. This paper is intended more for specialists on the theory of measure than for specialists in the field of machine learning.
All the results have a highly asymptotic character, no estimates of the rate of convergence are presented. 
Moreover, taking into account the methods used, such 
estimates do not exist at all. The pure theorems of the existence of forecastig systems are given, no corresponding efficient methods or algorithms are presented. The problem of efficiency of the methods does not discussed.
In this sense, the methods and the results of the work relate to measure theory rather than to machine learning.

I am sure that with this manner of presentation most of the evidence will be inaccessible to readers of JMLR. I recommend to add many explanations of proposed definitions and ideas, perhaps by increasing the size of the paper. 
It would be useful to expand the introduction and to add history and references to the relevant work. JMLR allows you to submit articles of a larger size. 
It would be useful to present theorems with explanation of main ideas in the separate section and move all technical proofs to the appendix. 

Guided solely by these two issues, I recommend making the necessary clarifications and simplifications and to resubmit this paper.

Minor remarks:

p.4. lines 9-14 from the top.
These definitions can confuse the reader. Operation * is defined for Kernel, but further it is used for simple mappinng. The definition of conditional probability more easy to learn in other textbooks. 

p.9. Explain details of application of the Kuratowski-Rill-Nardzewski measurable selection theorem.

p.10. lines=6-7 from the top.
This important point of the proof should be expained in more details.

p.10. The set (31) and the gambler (33) have the same notation.

p.11. Notation \mu;\mu in (34) was not explained earlier (I think that this is a concatination)

p.21. Item (ii) of Lemma 6. Here $E_\mu$ should be $E_{\mu_0}$.

p.22. lines=15 (from the top). For better understanding add: for all $\epsilon>0$.

