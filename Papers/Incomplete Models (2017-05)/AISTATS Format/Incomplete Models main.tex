%
\documentclass[twoside]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}
\usepackage{bm}
\usepackage{chngcntr}
\usepackage{apptools}

\usepackage{aistats2020}

%\numberwithin{equation}{section}

\theoremstyle{definition}
\newtheorem{definition}{Definition}%[section]
\newtheorem{example}{Example}%[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%[section]
%\newtheorem{lemma}{Lemma}%[section]
%\newtheorem{proposition}{Proposition}%[section]
%\newtheorem{corollary}{Corollary}%[section]
%\newtheorem{conjecture}{Conjecture}[section]

%\theoremstyle{remark}
%\newtheorem{note}{Note}[section]

%\AtAppendix{\counterwithin{theorem}{section}}
%\AtAppendix{\counterwithin{proposition}{section}}
%\AtAppendix{\counterwithin{lemma}{section}}
%\AtAppendix{\counterwithin{corollary}{section}}
%\AtAppendix{\counterwithin{definition}{section}}
%\AtAppendix{\counterwithin{example}{section}}

\newcommand{\Comment}[1]{}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\A}[1]{\lvert #1 \rvert}
\newcommand{\N}[1]{\lVert #1 \rVert}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\I}{\operatorname{id}}
\newcommand{\D}{\operatorname{diag}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\B}{\operatorname{B}}

\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}

\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\DeclareMathOperator{\Gr}{graph}

\newcommand{\PM}{\mathcal{P}}
\newcommand{\Lp}{{\operatorname{Lip}}}

\DeclareMathOperator{\Sp}{supp}

\newcommand{\DTV}{\operatorname{d}_{\textnormal{TV}}}
\newcommand{\DKR}{\operatorname{d}_{\textnormal{KR}}}

\newcommand{\R}[1]{\N{#1}_{\text{R}}}

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\OO}{\Ob^\omega}
\newcommand{\PO}{\pi^\Ob}
\newcommand{\PMO}{\PM(\OO)}
\newcommand{\MC}{\mathcal{H}}
\newcommand{\Gm}{\mathcal{B}}
\newcommand{\GMO}{\Gm(\OO)}
\newcommand{\CO}{C(\OO)}
\newcommand{\GC}{\mathfrak{G}}
\DeclareMathOperator{\V}{V}
\DeclareMathOperator{\SV}{\Sigma V}
\DeclareMathOperator{\SVM}{\Sigma V_{\min}}
\DeclareMathOperator{\SVX}{\Sigma V_{\max}}
\DeclareMathOperator{\Ab}{A}
\DeclareMathOperator{\Nr}{N}
\newcommand{\Bd}{\Lambda}
\DeclareMathOperator{\PG}{\Gamma}
\newcommand{\BM}{\bm{\mu}}
\newcommand{\F}{\mathcal{F}}

% If your paper is accepted, change the options for the package
% aistats2020 as follows:
%
% \usepackage[accepted]{aistats2020}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Forecasting using incomplete models}

]

\begin{abstract}
  We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is \enquote{suspected} to satisfy one or more of a set of \emph{incomplete} models, i.e., convex sets in the space of probability measures. This setting is in some sense intermediate between the \emph{realizable} setting where the probability measure comes from some known set of probability measures (which can be addressed using, e.g., Bayesian inference) and the \emph{unrealizable} setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.
\end{abstract}

\section{Introduction}

Forecasting future observations based on past observations is one of the fundamental problems in machine learning, and more broadly is one of the fundamental components of rational reasoning in general. This problem received a great deal of attention, using different methods (see e.g. \cite{Cesa-Bianchi_2006} or Chapter 21 in \cite{Shalev-Shwartz_2014}). Most of those methods assume fixing a class $\MC$ of models or \enquote{hypotheses}, each of which defines a probability measure on sequences of observations (and also conditional probability measures), and produce a forecast $F^\MC$ which satisfies at least one of two kinds of guarantees:

\begin{itemize}
\item 
In the \emph{realizable} setting, the guarantee is that if the observations are sampled from some $\mu \in \MC$, then $F^\MC$ converges to an \enquote{ideal} forecast in some sense.
\item
In the \emph{unrealizable} setting, the guarantee is that for \emph{any} sequence of observations, $F^\MC$ is asymptotically as good as the forecast produced by any $\mu \in \MC$.
\end{itemize}

The realizable setting is often unrealistic, in particular because it requires that the environment under observation is simpler than the observer itself. Indeed, even though we avoid analyzing computational complexity in this work, it should be noted that the computational (e.g., time) complexity of a forecaster is always greater than the complexities of all $\mu \in \MC$. On the other hand, the unrealizable setting usually only provides guarantees for short-term forecasts (since otherwise the training data is insufficient). The latter is in contrast to, e.g., Bayesian inference where the time to learn the model depends on its prior probability, but once \enquote{learned} (i.e., once $F^\MC$ has converged to a given total variation distance from the true probability measure), arbitrarily long-term forecasts become reliable.

The spirit of our approach is that the environment might be very complex, but at the same time it might possess some simple features, and it is these features that the forecast must capture. For example, if we consider a sequence of observations $\{o_n \in \{0,1\}\}_{n \in \Nats}$ s.t. $o_{2k+1}=o_{2k}$, then whatever is the behavior of the even observations $o_{2k}$, the property $o_{2k+1}=o_{2k}$ should asymptotically be assigned high probability by the forecast (this idea was discussed in \cite{Hutter_2009} as \enquote{open problem 4j}).

Formally, we introduce the notion of an \emph{incomplete model}, which is a convex set $M$ in the space $\PMO$ of probability measures on the space of sequences $\OO$. Such an incomplete model may be regarded as a hybrid of probabilistic and \emph{Knightian} uncertainty. We then consider a countable\footnote{This work only deals with the \emph{nonparametric} setting in which $\MC$ is discrete.} set $\MC$ of incomplete models. For any $M \in \MC$ and $\mu \in M$, our forecasts will converge to $M$ in an appropriate sense with $\mu$-probability 1. This convergence theorem can be regarded as an analogue for incomplete models of Bayesian merging of opinions (see \cite{Blackwell_1962}), and is our main result. Our setting can be considered to be in between realizable and unrealizable: it is \enquote{partially realizable} since we require the environment to conform to some $M \in \MC$, it is \enquote{partially unrealizable} since $\mu \in M$ can be chosen adversarially (we can even allow non-oblivious choice, i.e., dependence on the forecast itself).

The forecasting method we demonstrate is based on the principles introduced in \cite{Garrabrant_2016} (similar ideas appeared in \cite{Vovk_2005} in a somewhat simpler setting). The forecast may be regarded as the pricing of a certain combinatorial prediction market, with a countable set of gamblers making bets. The market pricing is then defined by the requirement that the aggregate of all gamblers doesn't make a net profit. The existence of such a pricing follows from the Kakutani-Glicksberg-Fan fixed point theorem and the Kuratowski-Rill-Nardzewski measurable selection theorem (the latter in order to show that the dependence on observation history can be made measurable). The fact that the aggregate of all gamblers cannot make a net profit implies that each individual gambler can only make a bounded profit.

The above method is fairly general, but for our purposes, we associate with each incomplete model $M \in \MC$ a set of gamblers that make bets which are guaranteed to be profitable assuming the true environment $\mu$ satisfies the incomplete model (i.e. $\mu \in M$). The existence of these gamblers follows from the Hahn-Banach separation theorem (where the incomplete model defines the convex set in question) and selection theorems used to ensure that the choice of separating functional depends in a sufficiently \enquote{regular} way on the market pricing. In order to show that the forecaster we described satisfies the desired convergence property, we use tools from martingale theory.

\section{Results}%{Learning Incomplete Models}
\label{sec:learning}

$\Nats$ will denote the set of natural numbers $\{0, 1, 2 \ldots\}$.

Let $\Sqn{\Ob_n}$ be a sequence of compact Polish spaces. $\Ob_n$ represents the space of possible observations at time $n$. Denote $\Ob^n := \prod_{m < n} \Ob_m$  and $\Ob^\omega:=\prod_{n \in \Nats} \Ob_n$. $\PO_n: \Ob^\omega \rightarrow \Ob^n$ is the projection mapping and $x_{:n}:=\PO_n\left(x\right)$. Given $y \in \Ob^n$, $y\OO := \left(\PO_n\right)^{-1}\left(y\right)$ is a closed subspace of $\OO$. Given $A\subseteq\Ob^n$, we denote $A\Ob^\omega:=\left(\PO_n\right)^{-1}(A)$. $\Ob^n$ will be regarded as a topological space using the product topology and as a measurable space with the $\sigma$-algebra of \emph{universally measurable} sets\footnote{The fact we use universally measurable sets rather than Borel sets will play an important role in the proof: see supplementary material, Lemma~C.1.}. For any measurable space $X$, $\PM\left(X\right)$ will denote the space of probability measures on $X$. When $X$ is a Polish space with the Borel $\sigma$-algebra, $\PM\left(X\right)$ will be regarded as a topological space using the weak topology and as a measurable space using the $\sigma$-algebra of Borel sets. Given $\mu \in \PM\left(X\right)$, we denote $\Sp \mu \subseteq X$ the support of $\mu$.

\begin{samepage}
\begin{definition}

A \emph{forecaster} $F$ is a family of measurable mappings

\[\Sqn{F_n: \Ob^n \rightarrow \PMO}\]

s.t. $\Sp {F_n\left(y\right)} \subseteq y\OO$.

\end{definition}
\end{samepage}

Given a forecaster $F$ and $y \in \Ob^n$, $F_n\left(y\right)$ represents the forecast corresponding to observation history $y$. The condition $\Sp {F_n\left(y\right)} \subseteq y\OO$ reflects the obvious requirement of consistency with past observations.

\begin{samepage}
\begin{example}

Consider any $\mu\in\PMO$. Then we can take $F_n(y)$ to be a regular conditional probability of $\mu$ (where the condition is $x \in y\OO$). This corresponds to Bayesian forecasting with prior $\mu$.

\end{example}
\end{samepage}

In order to formulate a claim about forecast convergence, we will need a metric on $\PMO$. Consider $\rho: \OO \times \OO \rightarrow \Reals$ a metrization of $\OO$. Let $\Lp\left(\OO,\rho\right)$ be the Banach space of $\rho$-Lipschitz functions on $\OO$, equipped with the norm

\begin{equation}
\N{f}_\rho:=\max_{x}{\A{f\left(x\right)}} + \sup_{x \ne y} \frac{\A{f\left(x\right)-f\left(y\right)}}{\rho\left(x,y\right)}
\end{equation}

$\PMO$ can be regarded as a compact subset of the dual space $\Lp\left(\OO,\rho\right)'$, yielding the following metrization of $\PMO$:

\begin{equation}
\DKR^\rho\left(\mu,\nu\right):=\sup_{\N{f}_\rho \leq 1}{\left(\E_\mu\left[f\right] - \E_\nu\left[f\right]\right)}
\end{equation}

We call $\DKR^\rho$ the \emph{Kantorovich-Rubinstein metric}\footnote{This is slightly different from the conventional definition but strongly equivalent (i.e. each metric is bounded by a constant multiple of the other). Other names used in the literature for the strongly equivalent metric are \enquote{1st Wasserstein metric} and \enquote{earth mover's distance.}}.

Fix any $x \in \OO$. It is easy to see that

\begin{equation}
\lim_{n \rightarrow \infty} \max_{x' \in x_{:n}\OO} \rho\left(x', x\right) = 0
\end{equation}

Denote $\delta_x \in \PMO$ the unique probability measure s.t. $\delta_x\left(\{x\}\right)=1$. It follows that for any sequence $\Sqn{\mu_n \in \PMO}$ s.t. $\Sp{\mu_n} \subseteq x_{:n}\OO$

\begin{equation}
\lim_{n \rightarrow \infty} \DKR^\rho\left(\mu_n, \delta_x\right) = 0
\end{equation}

Therefore, formulating a non-vacuous convergence theorem requires \enquote{renormalizing} $\DKR$ for each $n \in \Nats$. To this end, we consider a \emph{sequence} of metrizations of $\OO$: $\Sqn{\rho_n: \OO \times \OO \rightarrow \Reals}$. We denote $\DKR^n:=\DKR^{\rho_n}$. In the special case when $\Ob_n=\Ob$ for all $n \in \Nats$ and some fixed space $\Ob$, there is a natural class of metrizations with the property that $\rho_n(yx,yx')=\rho(x,x')$ for some fixed metric $\rho$ on $\OO$ and any $y \in \Ob^n$, $x,x' \in \OO$\footnote{In this special case, the need for renormalization is a side effect of our choice of notation where the forecaster produces a probability measure over the entire sequence rather than over future observations only. Also, in this case the choice of $\rho$ determines everything, since we only use Kantorovich-Rubinstein distance between measures with support in $y\OO$ for the same $y \in \Ob^n$.}. However, in general we can choose any sequence.

Another ingredient we will need is a notion of regular conditional probability for incomplete models. Given measurable spaces $X$ and $Y$, we will use the notation $K: X \M Y$ to denote a Markov kernel with source $X$ and target $Y$. Given $x \in X$, we will use the notation $K\left(x\right) \in \PM\left(Y\right)$. Given $\mu \in \PM\left(X\right)$, $\mu \ltimes K \in \PM\left(X \times Y\right)$ denotes the semidirect product of $\mu$ and $K$, that is, the unique measure satisfying

\begin{equation}
(\mu \ltimes K)(A \times B) = \int_A K(x)(B)\, \mu(dx)
\end{equation} 

Given $\pi:X\rightarrow Y$ measurable, $\pi_*\mu\in\PM(Y)$ denotes the pushforward of $\mu$ by $\pi$. $K_* \mu \in \PM\left(Y\right)$ denotes the pushforward of $\mu$ by $K$ (i.e. the pushforward of $\mu \ltimes K$ by the projection to $Y$). Of course $\pi$ can be regarded as a \enquote{deterministic} Markov kernel, so the notation $\pi_*\mu$ is truly a special case of $K_*\mu$. When $X,Y$ are Polish and $\pi: X \rightarrow Y$ is Borel measurable, $\mu \mid \pi: Y \M X$ is defined to be s.t. $\pi_* \mu \ltimes \left(\mu \mid \pi\right)$ is supported on the graph of $\pi$ and $\left(\mu \mid \pi\right)_* \pi_* \mu = \mu$ (i.e. $\mu \mid \pi$ is a regular conditional probability). By the disintegration theorem, $\mu \mid \pi$ exists and is defined up to coincidence $\pi_* \mu$-almost everywhere.

\begin{samepage}
\begin{definition}
\label{def:update_incomplete}

Let $X,Y$ be compact Polish spaces, $\pi: X \rightarrow Y$ continuous and $M \subseteq \PM(X)$. We say that $N: Y \rightarrow 2^{\PM(X)}$ is a \emph{regular upper bound for $M \mid \pi$} when 
%
\begin{enumerate}[i.]
\item\label{con:def__update_incomplete__clos} The set $\Gr{N}:=\{(y,\mu)\in Y\times\PM(X) \mid \mu \in N(y)\}$ is closed.
\item\label{con:def__update_incomplete__conv} For every $y \in \Ob^n$, $N(y)$ is convex.
\item\label{con:def__update_incomplete__cond} For every $\mu \in M$ and $\pi_*\mu$-almost every $y \in Y$, $(\mu \mid \pi)(y) \in N(y)$.
\end{enumerate}

\end{definition}
\end{samepage}

The reason we call that \enquote{regular upper bound for $M\mid\pi$} rather than just \enquote{$M\mid\pi$} is that, roughly, our conditions guarantee $N$ is \enquote{big enough} but don't guarantee it is not \enquote{too big}. For example, setting $N(y):=\PM(X)$ would trivially satisfy all conditions. Also note that, although technically we haven't assumed $M$ is convex, we might as well have assumed it: it is not hard to see that, if $N$ is a regular upper bound for $M \mid \pi$ and $M'$ is the convex hull of $M$, then $N$ is a regular upper bound for $M' \mid \pi$. The same remark applies to Theorem~\ref{thm:main} below.

We give a few examples for Definition~\ref{def:update_incomplete}. The proofs that these examples are valid are in the supplementary material, Appendix~E.

\begin{samepage}
\begin{example}
\label{exm:update_incomplete_finite}

Suppose that $M$ is convex and $Y$ is a finite set (in particular, this example is applicable when the $\Ob_n$ are finite sets, $X=\OO$, $Y=\Ob^n$ and $\pi=\PO_n$). Then, there is a unique $N: Y \rightarrow 2^{\PM(X)}$ which is a \emph{minimal} (w.r.t. set inclusion) regular upper bound for $M \mid \pi$ and we have

\begin{equation}
\label{eqn:exm__update_incomplete_finite}
N(y) = \overline{\{\left(\mu \mid \pi^{-1}(y)\right) \mid \mu \in M,\, \mu\left(\pi^{-1}(y)\right) > 0\}}
\end{equation}

Here, $\mu \mid \pi^{-1}(y)$ is the conditional probability measure and the overline stands for topological closure.

\end{example}
\end{samepage}
%
\begin{samepage}
\begin{example}
\label{exm:update_incomplete_kernels}

Consider some $I \subseteq \Nats$ and suppose that for each $n \in I$, we are given $K_n: \Ob^n \M \Ob_n$. Assume that each $K_n$ is \emph{Feller continuous}, that is, that the induced mapping $\Ob^n \rightarrow \PM\left(\Ob_n\right)$ is continuous. Define $M^K$ by

\begin{multline}
M^K:=\{\mu \in \PMO \mid\\ \forall n \in I: \PO_{n+1*}\mu = \PO_{n*}\mu \ltimes K_n\}
\end{multline}

For each $n \in \Nats$, define $M^K_n: \Ob^n \rightarrow 2^{\PMO}$ by

\begin{multline}
M^K_n(y) = \{\mu \in \PMO \mid \Sp{\mu} \subseteq y\OO,\\ \forall m \in I: m \geq n \implies \PO_{m+1*}\mu = \PO_{m*}\mu \ltimes K_m\}
\end{multline}

Then, $M^K_n$ is a regular upper bound for $M^K \mid \PO_n$.

\end{example}
\end{samepage}

\begin{samepage}
\begin{example}
\label{exm:update_incomplete_euclid}

Suppose that $Y$ is a compact subset of $\Reals^{d}$ for some $d \in \Nats$ (in particular, this example is applicable when each $\Ob_n$ is a compact subset of $\Reals^{d_n}$, $X=\OO$, $Y=\Ob^n$ and $\pi=\PO_n$; in that case $d=\sum_{m < n} d_n$). We regard $Y$ as a metric space using the Euclidean metric. For any $y \in Y$ and $r > 0$, $\B_r\left(y\right)$ will denote the open ball of radius $r$ with center at $y$. 

Fix any $M \subseteq \PM(X)$. Then, there is a unique $N: Y \rightarrow 2^{\PM(X)}$ which is minimal among mappings which both satisfy conditions \ref{con:def__update_incomplete__clos} and \ref{con:def__update_incomplete__conv} of Definition~\ref{def:update_incomplete} and are s.t. for any $y \in Y$, $\nu \in \PM(X)$ and $\mu \in M$, if $y \in \Sp \pi_*\mu$ and $\nu = \lim_{r \rightarrow 0}{\left(\mu \mid \pi^{-1}\left(\B_r\left(y\right)\right)\right)}$, then $\nu \in N(y)$ (the limit is defined using the the weak topology). Moreover, $N$ is a regular upper bound for $M \mid \pi$.

\end{example}
\end{samepage}

We are now ready to formulate the main result.

Given a metric space $X$ with metric $\rho: X \times X \rightarrow \Reals$, $x \in X$ and $A \subseteq X$, we will use the notation

\begin{equation}
\rho\left(x,A\right):=\inf_{y \in A} \rho\left(x,y\right)
\end{equation}

\begin{theorem}
\label{thm:main}

Fix any $\MC \subseteq 2^{\PMO}$ countable. For every $M \in \MC$ and $n \in \Nats$, let $M_n$ be a regular upper bound for $M \mid \PO_n$. Then, there exists a forecaster $F^\MC$ s.t. for any $M \in \MC$, $\mu \in M$ and $\mu$-almost any $x \in \OO$

\begin{equation}
\label{eqn:thm_main}
\lim_{n \rightarrow \infty} \DKR^n\left(F^\MC_n\left(x_{:n}\right),M_n\left(x_{:n}\right)\right) = 0
\end{equation}

\end{theorem}

That is, the forecaster $F^\MC$ ensures that, for any incomplete model $M$ satisfied by the true environment $\mu$, the forecast will (almost surely) converge to the model (as opposed to complete models, there may be several incomplete models satisfied by the same environment).

Note that $F^\MC$ as above exists for any choice of $\Sqn{\rho_n}$ but it depends on the choice. Informally, we can think of this choice as determining the duration of the future time period over which we need our forecast to be reliable. It also might depend on the choice of regular upper bounds (of course, if there are \emph{minimal} regular upper bounds, these yield a forecaster than works for any other regular upper bounds).

The example from the Introduction section can now be realized as follows. Take $\Ob_n=\Ob=\{0,1\}$ and let $M^K \subseteq \PMO$ be as in Example~\ref{exm:update_incomplete_kernels}, where $I = 2\Nats+1$ and $\Pr_{o\sim K_{2n+1}(y)}\left[o=y_{2n}\right]=1$. Define $\rho_n$  by

\begin{equation}
\rho_n(x,x'):=\begin{cases} \max\{2^{n-m} \mid x_{m} \ne x'_{m}\} \text{ if } x \ne x' \\ 0 \text{ if } x=x'\end{cases}
\end{equation}

If $M^K \in \MC$, and $x\in\OO$ is s.t. $x_{2n+1}=x_{2n}$, then for any $k \in \Nats$, the forecaster $F^\MC$ of Theorem~\ref{thm:main} satisfies

\begin{align}
\lim_{n\rightarrow\infty} \Pr_{x' \sim F_{2n}\left(x_{:2n}\right)}&\left[x'_{2(n+k)+1}=x'_{2(n+k)}\right] = 1\\
\lim_{n\rightarrow\infty} \Pr_{x' \sim F_{2n+1}\left(x_{:2n+1}\right)}&\left[x'_{2(n+k)+1}=x'_{2(n+k)}\right] = 1
\end{align}

If we only cared about predicting the next observation (as opposed to producing a probability measure over the entire sequence), this example (and any other instance of Example~\ref{exm:update_incomplete_kernels}, at least in the case $\Ob_n=\Ob$ finite) would be a special case of the \enquote{sleeping experts} setting in online learning (see \cite{Freund_1997}). However, our formalism is much more general, even for predicting the next observation. For instance, we can consider $\Ob=\{0,1,2,3\}$ and have an incomplete model specifying that the next observation is an odd number (thus, the \enquote{expert} specializes by predicting specific \emph{properties} of the observation rather than only making predictions at specific times). As another example, we can consider $\Ob=\{0,1,2\}$ and an incomplete model specifying that the probability distribution $\left(p_0,p_1,p_2\right)$ of each observation satisfies $p_0,p_1,p_2 \geq 0.1$. This model would capture any environment that can be regarded as a process observed through a noisy sensor that has a probability of 0.3 to output a uniformly random observation instead of the real state of the process.

The proof of Theorem~\ref{thm:main} is in the supplementary material, Appendices A-C.

\bibliographystyle{unsrt}
\bibliography{Incomplete_Models}

\end{document}
