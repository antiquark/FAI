%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}

\DeclareMathOperator{\Prj}{pr}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}

\newcommand{\Prob}{\mathcal{P}}

% Paper specific

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\UM}{\mathcal{U}}
\newcommand{\W}{\operatorname{W}}
\newcommand{\SW}{\operatorname{\Sigma W}}
\newcommand{\I}{\operatorname{id}}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\NormL}[1]{\Norm{#1}_{\operatorname{Lip}}}
\newcommand{\Dkr}{\operatorname{d}_{\text{KR}}}
\newcommand{\Ball}{\operatorname{B}}
\newcommand{\F}{\mathcal{F}}

\begin{document}

*This post is formal treatment of the idea outlined [here](https://agentfoundations.org/item?id=1197).*

Given a countable set of [incomplete models], we define a forecasting function that converges in the Kantorovich-Rubinstein metric with probability 1 to every one of the models which is satisfied by the true environment. This is analogous to Blackwell-Dubins [merging of opinions](https://projecteuclid.org/euclid.aoms/1177704456), for complete models, except that Kantorovich-Rubinstein convergence is weaker than convergence in total variation. The forecasting function is a [dominant market](https://agentfoundations.org/item?id=1214) for a suitably constructed set of traders. We prove this result in a fairly general setting (for every finite time $t \in \Nats$, the space of observations is an arbitrary compact subset of $\Reals^{D(t)}$).%, but only assuming projective determinacy. In simpler situations (e.g. when observations are strings over a finite alphabet), ZFC is sufficient.

Appendix A contains the proofs of key lemmas and theorems. Appendix B contains the proofs of technical propositions used in Appendix A, which are mostly straightforward. Appendix C contains the statement of a version of the optional stopping theorem from [Durret](?).

***

\section{Notation}

Given ${X}$ a metric space and ${x \in X}$, ${\Ball_r(x):=\{y \in X \mid d(x,y) <\ r\}}$.

Given ${X}$ a topological space:

* ${\I_X: X \rightarrow X}$ is the identity mapping.

* ${\Prob(X)}$ is the space of Borel probability measures on ${X}$ equipped with the weak* topology.

* ${C(X)}$ is the Banach space of continuous functions ${X \rightarrow \Reals}$ with uniform norm.

* ${\mathcal{B}}(X)$ is the Borel ${\sigma}$-algebra on ${X}$.

* ${\UM(X)}$ is the ${\sigma}$-algebra of universally measurable sets on ${X}$.

* Given ${\mu \in \Prob(X)}$, ${\Supp \mu}$ denotes the support of ${\mu}$. 

Given ${X}$ and ${Y}$ measurable spaces, ${K: X \Markov\ Y}$ is a Markov kernel from ${X}$ to ${Y}$. For any ${x \in X}$, we have ${K(x) \in \Prob(Y)}$. Given ${\mu \in \Prob(X)}$, ${\mu \ltimes K \in \Prob(X \times Y)}$ is the semidirect product of ${\mu}$ and ${K}$ and ${K_*\mu \in \Prob(Y)}$ is the pushforward of ${\mu}$ by ${K}$.

Given ${X}$, ${Y}$ Polish spaces, ${\pi: X \rightarrow Y}$ Borel measurable and ${\mu \in \Prob(X)}$, we denote ${\mu \mid \pi}$ the set of Markov kernels ${K: Y \Markov X}$ s.t. ${\pi_* \mu \ltimes K}$ is supported on the graph of ${\pi}$ and ${K_*\pi_* \mu = \mu}$. By the disintegration theorem, ${\mu \mid \pi}$ is always non-empty and any two kernels in ${\mu \mid \pi}$ coincide ${\pi_*\mu}$-almost everywhere.

\section{Dominant Stochastic Markets}

The way we previously laid out the dominant market formalism, the sequence of observations (represented by the sets $\{X_i\}$) was fixed. To study forecasting, we instead need to assume this sequence is sampled from some probability measure (the true environment).

For each ${n \in \Nats}$, let ${\Ob_n}$ be a compact Polish space. ${\Ob_n}$ represents the space of possible observations at time ${n}$. Denote 

$${Y_n:=\prod_{m < n} \Ob_m}$$

Given ${n \leq m}$, ${\pi_{nm}: Y_m \rightarrow Y_n}$ denotes the projection mapping and ${\pi_n:=\pi_{n,n+1}}$. Denote 

$${X = \prod_{n = 0}^\infty \Ob_n}$$

${X}$ is a compact Polish space. For each ${n \in \Nats}$ we denote ${\pi_{n\omega}: X \rightarrow Y_n}$ the projection mapping. Given ${y \in Y_n}$, we denote ${X_y:=\pi_{n\omega}^{-1}(y)}$, a closed subspace of ${X}$.

\#Definition N1

A *market* is a sequence of mappings ${\{M_n: Y_n \rightarrow \Prob(X)\}}_{n \in \Nats}$ s.t.

* Each ${M_n}$ is measurable w.r.t. ${\UM(Y_n)}$ and ${\B(\Prob(X))}$.

* For any ${y \in Y_n}$, ${\Supp M_n(y) \subseteq X_y}$.

***

As before, we define the space of trading strategies ${\T(X):=C(\Prob(X)\times X)}$, but this time we regard it as a Banach space. 

\#Definition N2

A *trader* is a sequence of mappings ${\{T_n: Y_n \times \Prob(X)^n \rightarrow \T(X)\}}_{n \in \Nats}$ which are measurable w.r.t. ${\UM(Y_n) \otimes \B(\Prob(X)^n)}$ and ${\B(\T(X))}$.

***

Given a trader ${T}$ and a market ${M}$, we define the mappings ${\{T^M_n: Y_n \rightarrow \T(X)\}_{n \in \Nats}}$ (measurable w.r.t. ${\UM(Y_n)}$ and ${\B(\T(X))}$) and ${\{\bar{T}^M_n: Y_n \rightarrow C(X)\}_{n \in \Nats}}$ (measurable w.r.t. ${\UM(Y_n)}$ and ${\B(C(X))}$) as follows:

$$T^M_n(y):= T_n(y, M_0(\pi_{0n}(y)),M_1(\pi_{1n}(y)) \ldots M_{n-1}(\pi_{n-1,n}(y)))$$

$$\bar{T}^M_n(y):= T^M_n(y,M_n(y))$$

The "market maker" lemma now requires some additional work due to the measurability requirement:

\#Lemma N1

Consider any trader ${T}$. Then, there is a market ${M}$ s.t. for all ${n \in \Nats}$ and ${y \in Y_n}$

$$\Supp M_n(y) \subseteq \Argmax{X_y} \bar{T}^M_n(y)$$

***

As before, we have the operator ${\W: \T(X) \rightarrow \T(X)}$ defined by 

$$\W \tau(\mu,x):= \tau(\mu,x) - \E_{z \sim \mu}[\tau(\mu,z)]$$

We also introduce the notation ${\{\W \bar{T}^M_n: Y_n \rightarrow C(X)\}_{n \in \Nats}}$ and ${\{\SW T^M_n: Y_n \rightarrow C(X)\}_{n \in \Nats}}$ which are measurable mappings defined by

$$\W \bar{T}^M_n(y) = \W T^M_n(y, M_n(y))$$

$$\SW T^M_n(y) = \sum_{m \leq n} \W \bar{T}^M_m(\pi_{mn}(y))$$

\#Definition N3

A market ${M}$ is said to dominate a trader ${T}$ when for any ${x \in X}$, if

$$\inf_{n \in \Nats} \min_{z \in \pi_{n\omega}^{-1}(\pi_{n\omega}(x))} \SW T^M_n(\pi_{n\omega}(x),z) > -\infty$$

then

$$\sup_{n \in \Nats} \max_{z \in \pi_{n\omega}^{-1}(\pi_{n\omega}(x))} \SW T^M_n(\pi_{n\omega}(x),z) < +\infty$$

\#Theorem N2

Given any countable set of traders $R$, there is a market ${M}$ s.t. ${M}$ dominates all ${T \in R}$.

***

Theorem N2 is proved exactly as [before](https://agentfoundations.org/item?id=1214) (modulo Lemma N1), and we omit the details.

\section{Profitable Metastrategies}

We now describe a class of traders associated with a fixed environment ${\mu^* \in \Prob(X)}$ s.t. if a market dominates a trader from this class, a certain function of the pricing converges to 0 with ${\mu^*}$-probability 1. Afterwards, we will apply this result to a trader associated with an incomplete models ${\Phi \subseteq \Prob(X)}$ by observing that the trader is in the class for any ${\mu^* \in \Phi}$.

\#Definition N4

A *trading metastrategy* is a uniformly bounded family of measurable mappings ${\{\upsilon_n: Y_n \rightarrow \T(X)\}_{n \in \Nats}}$. Given ${\mu^* \in \Prob(X)}$, ${\upsilon}$ is said said to be *profitable for ${\mu^*}$*, when there are ${c > 0}$ and ${\{K_n \in \mu^* \mid \pi_{n\omega}\}_{n \in \Nats}}$ s.t. for any ${n \in \Nats}$, ${\pi_{n\omega*}\mu^*}$-almost any ${y \in Y_n}$ and any ${\mu \in \Prob(X_y)}$:

$$\E_{K_n(y)}[\upsilon(y,\mu)] - \E_{\mu}[\upsilon(y,\mu)] \geq c \Norm{\upsilon(y,\mu)}$$

***

Even if a metastrategy is profitable, it doesn't mean that a smart trader should use this metastrategy all the time: in order to avoid running out of budget, a trader shouldn't place too many bets simultaneously. The following construction defines a trader that employs a metastrategy only when all previous bets are closed to being resolved.

\#Definition N5

Fix a metastrategy ${\upsilon}$. We define the trader ${T_\upsilon}$ and the measurable mappings ${\{U_{\upsilon  n}: Y_n \times P(X)^n \rightarrow C(X)\}_{n \in \Nats}}$ recursively as follows:
 
 $$U_0 := 0$$

$$T_{\upsilon0} := \upsilon$$

$$U_{\upsilon,n+1}(y, \{\mu_m\}_{m \leq n}) := U_{\upsilon n}(\pi_n(y), \{\mu_m\}_{m < n}) + T_{\upsilon n}(y, \{\mu_m\}_{m \leq n})$$

$$T_{\upsilon,n+1}(y, \{\mu_m\}_{m \leq n}) := \begin{cases}\upsilon(y) \text{ if } \max_{X_y} U_{\upsilon,n+1}(y,\{\mu_m\}_{m \leq n}) - \min_{X_y} U_{\upsilon,n+1}(y,\{\mu_m\}_{m \leq n})\leq 1\\0 \text{ otherwise}\end{cases}$$

\#Lemma N3

Consider ${\mu^* \in P(X)}$, ${\{K_n \in \mu^* \mid \pi_{n\omega}\}_{n \in \Nats}}$, ${\upsilon}$ a metastrategy profitable for ${\mu^*}$ and ${M}$ a market. Assume ${M}$ dominates ${T_\upsilon}$. Then, for ${\mu^*}$-almost any ${x \in X}$, denoting ${y_n:=\pi_{n\omega}(x)}$:

$$\lim_{n \rightarrow \infty} (\E_{K_n(y_n)}[\upsilon(y_n,M_n(y_n))]-\E_{M_n(y_n)}[\upsilon(y_n,M_n(y_n))])= 0$$

\section{Profiting from Incomplete Models}

Consider ${X}$ any compact Polish metric space and ${d: X \times X \rightarrow \Reals}$ its metric. We denote ${\Lip(X)}$ the Banach space of Lipschitz continuous functions ${X \rightarrow \Reals}$ with the norm

$$\NormL{f}:=\max_{x} \Abs{f(x)} + \sup_{x \ne y}\frac{\Abs{f(x)-f(y)}}{d(x,y)}$$

${\Prob(X)}$ (as before, with the weak topology) can be regarded as a compact subset of ${\Lip(X)'}$ (with the strong topology), yielding a metrization of ${\Prob(X)}$ which we will call the Kantorovich-Rubinstein metric ${\Dkr}$:

$$\Dkr(\mu,\nu):=\sup_{\NormL{f} \leq 1} \Abs{\E_\mu[f] - \E_\nu[f]}$$

In fact, the above differs from the standard definition of the Kantorovich-Rubinstein metric (a.k.a. 1st Wasserstein metric, a.k.a. earth's mover metric), but this abuse of terminology is mild since the two are strongly equivalent.

Now consider ${\Phi \subseteq \Prob(X)}$ convex. We will describe a class of trading strategies that are designed to exploit any ${\mu \in \Phi}$.

\#Definition N6

${\tau \in \T(X)}$ is said to be *profitable for ${\Phi}$* when for all ${\mu \in \Prob(X)}$, ${\nu \in \Phi}$ we have

$$\E_\nu[\tau(\mu)] \geq \E_\mu[\tau(\mu)] + \Dkr(\mu,\Phi)$$

\#Lemma N4

Consider ${Y}$ another compact Polish space and ${\Phi \subseteq Y \times \Prob(X)}$ closed. For any ${y \in Y}$, define ${\Phi_y \subseteq \Prob(X)}$ by

$${\Phi_y := \{\mu \in \Prob(X) \mid (y,\mu) \in \Phi\}}$$ 

Assume that for any ${y \in Y}$, ${\Phi_y}$ is convex. Then, there exists ${\upsilon: Y \rightarrow \T(X)}$ measurable w.r.t. ${\UM(Y)}$ and ${\B(\T(X))}$ s.t. for all ${y \in Y}$, ${\Norm{\upsilon(y)} \leq 2}$ and ${\upsilon(y)}$ is profitable for ${\Phi_y}$ (instead of 2, the norm can be bounded by ${1+\epsilon}$ for any ${\epsilon > 0}$, but for our purposes any uniform bound is sufficient).

***

Now consider ${\Ob_n}$, ${\{Y_n\}_{n \in \Nats}}$ and $X$ as before, but assume that for each ${n \in \Nats}$, ${\Ob_n}$ is a compact subset of ${\Reals^{D(n)}}$ where ${D: \Nats \rightarrow \Nats}$ is an arbitrary function. Thus, ${Y_n}$ is a compact subset of ${\Reals^{\sum_{m < n} D(m)}}$. We will regard it as equipped with the Euclidean metric. The following definition provides a notion of updating an incomplete model by observations:

\#Definition N7

Consider ${\Phi \subseteq \Prob(X)}$. For any ${n \in \Nats}$ and ${y \in Y_n}$, we define ${\Phi_y'' \subseteq \Prob(X)}$ by

$$\Phi_y'':=\{\lim_{r \rightarrow 0} (\mu \mid \pi_{n\omega}^{-1}(\Ball_r(y))) \mid \mu \in \Phi\}$$

Note that the limit in the definition above need not exist for every ${\mu \in \Phi}$.

Denote ${\Phi_y'}$ the convex hull of ${\Phi_y''}$ and define ${\Phi_n' \subseteq Y_n \times \Prob(X)}$ by

$$\Phi_n':=\{(y,\mu) \in Y_n \times \Prob(X) \mid \mu \in \Phi_y'\}$$

Finally, we define ${\Phi_n \subseteq Y_n \times \Prob(X)}$ to be the closure of ${\Phi_n'}$. Given ${y \in Y_n}$, we define ${\Phi_y \subseteq \Prob(X)}$ by

$$\Phi_y:=\{\mu \in \Prob(X) \mid (y,\mu) \in \Phi_n\}$$

***

Note that the above definition uses the Euclidean metric on ${Y_n}$. This is the only place through which the assumption that ${\Ob_n}$ is a compact subset of ${R^{D(n)}}$ enters. This is used the in the proof of Lemma N5 below, via the Lebesgue differentiation theorem.

Fix a family of metrizations of ${X}$: ${\{d_n: X \times X \rightarrow \Reals\}_{n \in \Nats}}$. The reason we need a family rather than a single metric is that convergence in ${\Dkr}$ is trivial unless we renormalize the metric for each ${n}$. On the other hand, completely arbitrary renormalization is allowed. For example, if ${X=Y^\omega}$ and ${d}$ is a metrization of ${Y}$ (e.g. the Euclidean metric) we can take 

$$d_n(x^1,x^2)= \max_{m \in \Nats} c_{nm} d(x^1_m,x^2_m)$$

Here, ${\{c_{nm} > 0\}_{n,m \in \Nats}}$ are required to satisfy ${\lim_{m \rightarrow \infty} c_{nm} = 0}$. To get a non-trivial result, we need the ${c_{nm}}$ to fall slower with ${m}$ as ${n}$ increases. The stronger we make this trend, the stronger conclusion we get (although it always remains weaker than convergence in total variation).

\#Lemma N5

Consider ${\Phi \subseteq \Prob(X)}$ and ${\epsilon > 0}$. For every ${n \in \Nats}$, denote ${\Dkr^n}$ the Kantorovich-Rubinstein metric associated with ${d_n}$. Define ${\Phi_n^\epsilon \subseteq Y_n \times \Prob(X)}$ by 

$$\Phi_n^\epsilon:=\{(y,\mu) \in Y_n \times \Prob(X) \mid \Dkr^n(\mu,\Phi_y) \leq \epsilon\}$$

For every ${y \in Y_n}$, we define ${\Phi^\epsilon_y \subseteq \Prob(X)}$ by

$$\Phi^\epsilon_y:=\{\mu \in \Prob(X) \mid (y,\mu) \in \Phi^\epsilon_n\}$$

Let ${\upsilon}$ be a metastrategy s.t. for each ${n \in \Nats}$ and ${y \in Y_n}$, ${\upsilon_n(y)}$ is profitable for ${\Phi^\epsilon_y}$. Then, ${\upsilon}$ is profitable for any ${\mu^* \in \Phi}$.

***

Combining Lemma N3 and Lemma N5, it is easy to get the following:

\#Corollary N6

Consider ${\Phi \subseteq \Prob(X)}$, ${\epsilon > 0}$ and ${\upsilon}$ a metastrategy s.t. for each ${n \in \Nats}$ and ${y \in Y_n}$, ${\upsilon_n(y)}$ is profitable for ${\Phi^\epsilon_y}$. Let ${M}$ be a market which dominates ${T_\upsilon}$. Then, for any ${\mu^* \in \Phi}$ and ${\mu^*}$-almost any ${x \in X}$, denoting ${y_n:=\pi_{n\omega}(x)}$:

$$\limsup_{n \rightarrow \infty} \Dkr^n(M_n(y_n),\Phi_{y_n}) \leq \epsilon$$

***

Thus, if we construct a set of traders ${T_{\upsilon_k}}$ as in Corollary N6 where ${\upsilon_k}$ corresponds to ${\epsilon=\frac{1}{k}}$, then a market dominating all of these traders would have to converge to ${\Phi_{y_n}}$ with ${\mu^*}$-probability 1. Putting this together with Lemma N4 (so that ${\upsilon_k}$ as above actually exists) and Theorem N2 we finally get

\#Theorem N7

Consider a ${H}$ countable set of subsets of ${\Prob(X)}$. Then, there exists a market ${M^H}$ s.t. for any ${\Phi \in H}$, ${\mu^* \in \Phi}$ and ${\mu^*}$-almost any ${x \in X}$, denoting ${y_n:=\pi_{n\omega}(x)}$:

$$\lim_{n \rightarrow \infty} \Dkr^n(M^H_n(y_n),\Phi_{y_n}) = 0$$

***

This leaves an interesting open question, namely whether the counterpart of Theorem N7 with ${\Dkr}$ replaced by total variation distance is true.

\section{Appendix A}

\#Proposition N19

Fix ${Y,Y'}$ compact Polish spaces. Denote ${X:=Y \times Y'}$. Then, there exists ${\mathcal{M}: Y \times \T(X) \rightarrow \Prob(X)}$ measurable w.r.t. ${\B(Y \times \T(X))}$ and ${\B(\Prob(X))}$ s.t. for any ${y \in Y}$ and ${\tau \in \T(X)}$:


$$\Supp \mathcal{M}(y,\tau) \subseteq \Argmax{y \times Y'} \tau(\mathcal{M}(y,\tau))$$

\#Proof of Proposition N19

Define ${Z_1, Z_2, Z \subseteq Y \times \T(X) \times \Prob(X)}$ by

$${Z_1:=\{(y,\tau,\mu) \in Y \times \T(X) \times \Prob(X) \mid \Supp \mu \subseteq X_y\}}$$

$${Z_2:=\{(y,\tau,\mu) \in Y \times \T(X) \times \Prob(X) \mid \E_\mu[\tau(\mu)] = \max_{y \in Y'} \tau(\mu,y,y')\}}$$

$${Z:=\{(y,\tau,\mu) \in Y \times \T(X) \times \Prob(X) \mid \Supp \mu \subseteq \Argmax{y \times Y'} \tau(\mu)\}}$$

We can view ${Z}$ as the graph of a *multivalued* mapping from ${Y_n \times \T(X)}$ to ${\Prob(X)}$. We will now show this multivalued mapping has a *selection*, i.e. a single-valued measurable mapping whose graph is a subset. Obviously, the selection is the desired ${\mathcal{M}}$.

${Z_1}$ is closed by Proposition N16. ${Z_2}$ is closed by Proposition N9. ${Z = Z_1 \cap Z_2}$ by Proposition N14 and hence also closed. In particular, the fiber ${Z_{y\tau}}$ of ${Z}$ over any ${(y,\tau) \in Y \times \T(X)}$ is also closed. ${Z_{y\tau}}$ is also non-empty (see "Proposition 1" from [before](https://agentfoundations.org/item?id=1214)).

Consider any ${U \subseteq \Prob(X)}$ open. Then, ${A_U:=(Y \times \T(X) \times U) \cap Z}$ is locally closed and in particular ${F_\sigma}$. Therefore, the image of ${A_U}$ under the projection to ${Y \times \T(X)}$ is also ${F_\sigma}$ and in particular Borel. 

Applying the Kuratowski-Ryll-Nardzewski measurable selection theorem, we get the desired result.

\#Proof of Lemma N1

For any ${n \in \Nats}$, let ${\mathcal{M}_n: Y_n \times \T(X) \rightarrow \Prob(X)}$ be as in Proposition N19. We define ${M_n}$ recursively by:

$$M_n(y):=\mathcal{M}_n(y,T_n^M(y))$$

\#Proposition N20

Consider ${X}$ a probability space, ${\{\F_n \subseteq 2^X\}_{n \in \Nats}}$ a filtration of ${X}$, ${\{S_n:X \rightarrow \Reals\}_{n \in \Nats}}$ and ${\{T_n:X \rightarrow [0,1]\}_{n \in \Nats}}$ stochastic processes adapted to ${\F}$. Assume that there is ${c > 0}$ s.t.:

$$\E[\Abs{S_0}] < \infty$$

$$\Abs{S_{n+1}-S_n} \leq T_n$$

$$\E[S_{n+1} - S_n \mid \F_n] \geq c T_n$$

Then, ${\inf_{n} S_n > -\infty}$ with probability 1.

***

The proof will use the following definition:

\#Definition N8

Consider ${X}$ a probability space and ${\{T_n:X \rightarrow \Reals\}_{n \in \Nats}}$ a stochastic process. The *accumulation times* of ${T}$ are ${\{N_k: X \rightarrow \Nats \sqcup \{\infty\}\}_{k \in \Nats}}$ defined recursively by

$$N_0 \equiv 0$$

$$N_{k+1}(x) = \begin{cases}\inf \{n \in \Nats \mid \sum_{m=N_k}^{n-1} T_m(x) \geq 1\} \text{ if } N_k(x) < \infty\\\infty \text{ if } N_k(x) = \infty\end{cases}$$

\#Proof of Proposition N20

Define ${\Sqn{S^0_n: X \rightarrow \Reals}}$ by

$$S^0_n := S_n - c \sum_{m=0}^{n-1} T_m$$

By Proposition N21, ${S^0}$ is a submartingale.

TBD

\section{Appendix B}

\#Proposition N11

If ${X,Y}$ are compact Polish spaces and ${f: X \times Y \rightarrow \Reals}$ is continuous, then ${F: X \rightarrow C(Y)}$ defined by ${F(x)(y):=f(x,y)}$ is continuous.

***

We omit the proof of Proposition N11, since it appeared as "Proposition A.2" [before](https://agentfoundations.org/item?id=1214).

\#Proposition N8

Fix ${X,Y}$ compact Polish spaces. Define ${e: C(Y \times X) \times Y \rightarrow C(X)}$ by ${e(f,y)(x):=f(y,x)}$. Then, ${e}$ is continuous. In particular, we can apply this to ${Y = \Prob(X)}$ in which case ${e: \T(X) \times \Prob(X) \rightarrow C(X)}$.

\#Proof of Proposition N8

Consider ${f_k \rightarrow f}$ and ${y_k \rightarrow y}$. We have

$$\max_{x \in X} \Abs{f_k(y_k,x)-f(y_k,x)} \leq \Norm{f_k - f} \rightarrow 0$$

By Proposition N11

$$\max_{x \in X} \Abs{f(y_k,x)-f(y,x)} \rightarrow 0$$

Combining, we get

$$\max_{x \in X} \Abs{f_k(y_k,x)-f(y,x)} \rightarrow 0$$

\#Proposition N17

Fix ${Y,Y'}$ compact Polish spaces and denote ${X:=Y \times Y'}$. Define ${F: Y \times C(X) \rightarrow \Reals}$ by 

$${F(y,f):=\max_{y' \in Y'} f(y,y')}$$

Then, ${F}$ is continuous.

\#Proof of Proposition N17

Consider ${y_k \rightarrow y}$, ${f_k \rightarrow f}$. By Proposition N11, ${y_k \rightarrow y}$ implies that

$${\lim_{k \rightarrow \infty} \max_{y' \in Y'} f(y_k,y') = \max_{y' \in Y'} f(y,y')}$$

Since ${f_k \rightarrow f}$, we get

$${\lim_{k \rightarrow \infty} \max_{y' \in Y'} f_k(y_k,y') = \max_{y' \in Y'} f(y,y')}$$

\#Proposition N18

Fix ${Y,Y'}$ compact Polish spaces. Denote ${X:=Y \times Y'}$. Define ${Z \subseteq Y \times \Prob(X) \times C(X)}$ by

$$Z:=\{(y,\mu,f) \in Y \times \Prob(X) \times C(X) \mid \E_\mu[f] = \max_{y' \in Y'} f(y,y')\}$$

Then, ${Z}$ is closed.

\#Proof of Proposition N18

Consider ${y_k \rightarrow y}$, ${\mu_k \rightarrow \mu}$, ${f_k \rightarrow f}$, ${(y_k,\mu_k,f_k) \in Z}$. By Proposition N17, we get

$$\max_{y' \in Y'} f(y,y') = \lim_{k \rightarrow \infty} \max_{y' \in Y'} f_k(y_k,y')= \lim_{k \rightarrow \infty} \E_{\mu_k}[f_k] = \E_{\mu}[f]$$

Hence, ${(y,\mu,f) \in Z}$.

\#Proposition N9

Fix ${Y,Y'}$ compact Polish spaces. Denote ${X:=Y \times Y'}$. Define ${Z \subseteq Y \times \Prob(X) \times \T(X)}$ by

$$Z:=\{(y,\mu,\tau) \in Y \times \Prob(X) \times \T(X) \mid \E_\mu[\tau(\mu)] = \max_{y' \in Y'} \tau(\mu,y,y')\}$$

Then, ${Z}$ is closed.

\#Proof of Proposition N9

By Proposition N8, ${Z}$ is the continuous inverse image of a subset of ${Y \times \Prob(X) \times C(X)}$ which is closed by Proposition N18.

\#Proposition N14

Fix ${X}$ a compact Polish space. Consider ${f \in C(X)}$ and ${\mu \in \Prob(X)}$ and denote ${M := \max f}$. Then, ${\Supp \mu \subseteq f^{-1}(M)}$ iff ${\E_\mu[f] = M}$.

\#Proof of Proposition N14

If ${\Supp \mu \subseteq f^{-1}(M)}$ then $\Prb_{x\sim \mu}[f(x) \ne M] = 0$ and therefore ${\E_\mu[f] = M}$.

Now, assume ${\E_\mu[f] = M}$. For any ${k \in \Nats}$, Markov's inequality yields 

$$\Prb_{x\sim \mu}[M - f(x) \geq \frac{1}{k}] \leq k\E_{x \sim \mu}[M - f(x)] = 0$$

Taking $k \rightarrow \infty$, we get ${\Prb_{x\sim \mu}[M > f(x)] = 0}$ and hence ${\Supp \mu \subseteq f^{-1}(M)}$.

\#Proposition N16

Consider ${Y,Y'}$ compact Polish spaces. Denote ${X:=Y \times Y'}$. Define ${\Delta \subseteq Y \times \Prob(X)}$ by

$$\Delta:=\{(y,\mu) \in Y \times \Prob(X) \mid \Supp \mu \subseteq y \times Y'\}$$

Then, ${\Delta}$ is closed.

\#Proof of Proposition N16

We fix metrizations for ${Y}$ and ${Y'}$ and metrize ${X}$ by 

$${d_X((y_1,y_1'),(y_2,y_2')):=\max(d_Y(y_1,y_2),d_{Y'}(y_1',y_2'))}$$

For each ${y \in Y}$, denote ${d_{y}:=d_{y \times Y'}}$.

Consider ${y_k \rightarrow y}$, ${\mu_k \rightarrow \mu}$, ${(y_k, \mu_k) \in \Delta}$. We have ${\E_{\mu_k}[d_{y_k}]=0}$. By Proposition N11, ${d_{y_k} \rightarrow d_y}$, therefore ${\E_\mu[d_y] = 0}$. By Proposition N14, ${\Supp \mu \subseteq y \times Y'}$ and hence ${(y,\mu) \in \Delta}$.

\#Proposition N21

Consider ${X}$ a probability space, ${\{\F_n \subseteq 2^X\}_{n \in \Nats}}$ a filtration of ${X}$, ${\{S_n:X \rightarrow \Reals\}_{n \in \Nats}}$ and ${\{T_n:X \rightarrow [0,1]\}_{n \in \Nats}}$ stochastic processes adapted to ${\F}$. Assume that there is ${c > 0}$ s.t.:

$$\E[\Abs{S_0}] < \infty$$

$$\Abs{S_{n+1}-S_n} \leq T_n$$

$$\E[S_{n+1} - S_n \mid \F_n] \geq c T_n$$

Define ${\Sqn{S^0_n: X \rightarrow \Reals}}$ by

$$S^0_n := S_n - c \sum_{m=0}^{n-1} T_m$$
 
Then, ${S^0}$ is a submartingale.

\#Proof of Proposition N21

We prove by induction that ${\E[\Abs{S^0_n}] < \infty}$:

$${\E[\Abs{S^0_0}] = \E[\Abs{S_0}] < \infty}$$

$$\E[\Abs{S^0_{n+1}}] \leq \E[\Abs{S^0_n}] + \E[\Abs{S^0_{n+1}-S^0_n}] \leq \E[\Abs{S^0_n}] + \E[T_n] \leq \E[\Abs{S^0_n}] + 1$$

Obviously, ${S^0}$ is adapted to ${\F}$. Finally, we have

$$\E[S^0_{n+1} \mid \F_n] = \E[S_{n+1} \mid \F_n] - c \sum_{m = 0}^{n} T_m$$

$$\E[S^0_{n+1} \mid \F_n] \geq \E[S_{n} \mid \F_n] + c T_n - c \sum_{m = 0}^{n} T_m$$

$$\E[S^0_{n+1} \mid \F_n] \geq \E[S_{n} \mid \F_n] - c \sum_{m = 0}^{n - 1} T_m$$

$$\E[S^0_{n+1} \mid \F_n] \geq \E[S^0_{n} \mid \F_n]$$

\#Proposition N23

Let ${X}$ be a probability space, ${\Sqn{\F_n \subseteq 2^X}}$ a filtration on ${X}$, ${\Sqn{S_n: X \rightarrow \Reals}}$ a stochastic process adapted to ${\F}$ and ${N: X \rightarrow \Nats \sqcup \{\infty\}}$ a stopping time (w.r.t. ${\F}$). Suppose ${S}$ is submartingale. Then, ${\Sqn{S_{\min(n,N)}}}$ is also submartingale.

\#Proof of Proposition N23

Clearly, ${\Sqn{S_{\min(n,N)}}}$ is adapted to ${\F}$. We have

$$\Abs{S_{\min(n,N)}} \leq \sum_{m \leq n} \Abs{S_m}$$

$$\E[\Abs{S_{\min(n,N)}}] \leq \E[\sum_{m \leq n} \Abs{S_m}] = \sum_{m \leq n} \E[\Abs{S_m}] < \infty$$

For any ${n \in \Nats}$, define ${A_n \subseteq X}$ by

$$A_n:=\{x \in X \mid N(x)\ > n\}$$

${N}$ is a stopping time, therefore ${A_n \in \F_n}$. We have 

$${S_{\min(n+1,N)} - S_{\min(n,N)} = \chi_{A_n} (S_{n+1} - S_n)}$$

$${\E[S_{\min(n+1,N)} - S_{\min(n,N)} \mid \F_n] = \E[\chi_{A_n} (S_{n+1} - S_n) \mid \F_n]}$$

$${\E[S_{\min(n+1,N)} - S_{\min(n,N)} \mid \F_n] = \chi_{A_n} \E[ S_{n+1} - S_n \mid \F_n] \geq 0}$$

\#Proposition N25

Consider ${X}$ a probability space and ${\{T_n:X \rightarrow [0,1]\}_{n \in \Nats}}$ a stochastic process. Let ${\{N_k: X \rightarrow \Nats \sqcup \{\infty\}\}_{k \in \Nats}}$ be the accumulation times of ${T}$. Then:

$$k \leq \sum_{n = 0}^{N_k - 1} T_n \leq k + 1$$

\#Proof of Proposition N25

We prove by induction on ${k}$. For ${k = 0}$, the claim is obvious. ${N_{k+1} \geq N_k}$, therefore

$$\sum_{n = 0}^{N_{k+1} - 1} T_n = \sum_{n = 0}^{N_{k} - 1} T_n + \sum_{n = N_k}^{N_{k + 1} - 1} T_n$$

TBD

\#Proposition N24

Consider ${X}$ a probability space, ${\{\F_n \subseteq 2^X\}_{n \in \Nats}}$ a filtration of ${X}$, ${\{S_n:X \rightarrow \Reals\}_{n \in \Nats}}$ and ${\{T_n:X \rightarrow [0,1]\}_{n \in \Nats}}$ stochastic processes adapted to ${\F}$. Assume that

$$\E[\Abs{S_0}] < \infty$$

$$\Abs{S_{n+1}-S_n} \leq T_n$$

Let ${\{N_k: X \rightarrow \Nats \sqcup \{\infty\}\}_{k \in \Nats}}$ be the accumulation times of ${T}$. Define ${\Sqn{U_n: X \rightarrow \Reals}}$ by

$$U_n := \sum_{m=0}^{n-1} T_m$$

Then, ${\Sqn{S_{\min(n,N)}}}$ and ${\Sqn{U_{\min(n,N)}}}$ are uniformly integrable.

\#Proof of Proposition N24

TBD

\section{Appendix C}

The following version of the optional stopping theorem appears in [Durret](?) as Theorem 5.4.7:

\#Theorem N22

Let ${X}$ be a probability space, ${\Sqn{\F_n \subseteq 2^X}}$ a filtration on ${X}$, ${\Sqn{S_n: X \rightarrow \Reals}}$ a stochastic process adapted to ${\F}$ and ${M,N: X \rightarrow \Nats \sqcup \{\infty\}}$ stopping times (w.r.t. ${\F}$) s.t. ${M \leq N}$. Assume that ${\Sqn{S_{\min(n,N)}}}$ is a uniformly integrable submartingale. Using Doob's martingale convergence theorem, we can define ${S_N : X \rightarrow \Reals}$ by

$$S_N(x):=\lim_{n \rightarrow \infty} S_{\min(n,N)}(x)=\begin{cases}S_{N(x)}(x) \text{ if } N(x) < \infty\\\lim_{n \rightarrow \infty} S_n(x) \text{ if } N(x) = \infty\end{cases}$$

(the above is well-defined *almost* everywhere, which is sufficient for our purpose) We define ${S_M: X \rightarrow \Reals}$ is an analogous way (this time we can't use Doob's martingale convergence theorem, but whenever ${M(x) = \infty}$ we also have ${N(x) = \infty}$, therefore the limit almost surely converges). We also define ${\F_M \subseteq 2^X}$ by

$$\F_M:=\{A \subseteq X \text{ measurable} \mid \forall n \in \Nats: A \cap M^{-1}(n) \in \F_n\}$$

Then:

$$\E[S_N \mid \F_M] \geq S_M$$

\end{document}




