%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{bm}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\newcommand{\PP}[2]{\operatorname{Pr}_{\substack{#1 \\ #2}}}
\newcommand{\PPP}[3]{\operatorname{Pr}_{\substack{#1 \\ #2 \\ #3}}}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}

\DeclareMathOperator{\Prj}{pr}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\textnormal{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Estr}{\bm{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}

\newcommand{\Prob}{\mathcal{P}}

% Paper specific

\newcommand{\Act}{\mathcal{A}}
\newcommand{\Obs}{\mathcal{O}}
\newcommand{\ObsO}{\Obs^\omega}
\newcommand{\Pol}{\Obs^* \rightarrow \Act}
\newcommand{\CC}{\mathcal{P}_{\operatorname{C}}}

\begin{document}

*This post continues the research programme of attacking the grain of truth problem by departure from the Bayesian paradigm. In the [previous post](https://agentfoundations.org/item?id=1046), I suggested using Savage's minimax regret decision rule, but here I fall back to the simple minimax decision rule. This is because the mathematics is considerably simpler, and minimax should be sufficient to get IUD play in general games and Nash equilibrium in zero-sum two-player games. I hope to build on these results to get analogous results for minimax regret in the future.*

We consider "semi-Bayesian" agents following the minimax expected utility decision rule, in oblivious environments with full monitoring (a setting that we will refer to as "forecasting"). This setting is considered in order to avoid the need to enforce exploration, as a preparation for analysis of general environments. We show that such agents satisfy a certain asymptotic optimality theorem. Intuitively, this theorem means that whenever the environment satisfies an incomplete model that is included in the prior, the agent will eventually learn this model i.e. extract at least as much utility as can be guaranteed for this model.

Appendix A contains the proofs of all results. Appendix B contains some theorems from the literature together with a corollary used in Appendix A.

\section{Notation}

Given ${X}$ a topological space, ${\Prob(X)}$ will denote the space of Borel probability measures on ${X}$. We regard it as a topological space using the weak${^*}$ topology. Given ${x \in X}$, ${\delta_x \in \Prob(X)}$ is defined by ${\delta_x(A):=[[x \in A]]}$. Given ${\mu, \nu \in \Prob(X)}$, ${\Dtv(\mu, \nu)}$ stands for the total variation distance between ${\mu}$ and ${\nu}$. We denote ${\CC(X)}$ the set of non-empty convex compact subsets of ${\Prob(X)}$.

Given ${\mathcal{X}}$ a finite set, ${\mathcal{X}^*}$ denotes the set of finite strings in alphabet ${\mathcal{X}}$, i.e. ${\mathcal{X}^*:=\bigsqcup_{n \in \Nats} \mathcal{X}^n}$. ${\mathcal{X}^\omega}$ denotes the set of infinite strings in alphabet ${\mathcal{X}}$. Given ${x \in \mathcal{X}^* \sqcup \mathcal{X}^\omega}$, ${\Abs{x} \in \Nats \sqcup \{\infty\}}$ is the length of string. Given ${0 \leq n < \Abs{x}}$, ${x_n \in \mathcal{X}}$ is the ${n}$-th symbol in ${x}$. Given ${0 \leq n \leq \Abs{x}}$, ${x_{<n}}$ is the prefix of ${x}$ of length ${n}$. Given ${x,y \in \mathcal{X}^* \sqcup \mathcal{X}^\omega}$, the notations ${x \sqsubset y}$, ${x \sqsubseteq y}$, ${x \not\sqsubset y}$ and ${x \not\sqsubseteq y}$ mean "${x}$ is a proper prefix of ${y}$", "${x}$ is a prefix of ${y}$" and their negations. ${\Estr_\mathcal{X} \in \mathcal{X}^*}$ is the empty string and ${\mathcal{X}^{+}:=\mathcal{X}^* \setminus \Estr_\mathcal{X}}$. 

\section{Elementary properties of minimax}

Fix ${S}$ and ${E}$ compact Polish spaces, ${S}$ representing the agent's pure policies and ${E}$ representing the pure environments. Let ${u: S \times E \rightarrow \Reals}$ be a continuous utility function.

\subsection{Proposition 1}

Consider ${\Phi \subseteq \Prob(E)}$. There exists 

$${\pi^* \in \Argmax{\pi \in \Prob(S)} \inf_{\mu \in \Phi} \E_{\pi \times \mu}[u]}$$

Moreover, let ${\bar{\Phi}}$ be the closure of the convex hull of ${\Phi}$. Then, the same ${\pi^*}$ satisfies

$${\pi^* \in \Argmax{\pi \in \Prob(S)} \min_{\mu \in \bar{\Phi}} \E_{\pi \times \mu}[u]}$$

We will say that such a ${\pi^*}$ is a *minimax policy for ${\Phi}$*.

\subsection{Proposition 2}

Consider ${\Phi, \Phi' \in \CC(E)}$ and ${p \in [0,1]}$. Then, there is ${\nu^* \in \Phi'}$ s.t. any minimax policy for ${p\Phi+(1-p)\Phi'}$ is a minimax policy for ${p\Phi+(1-p)\nu^*}$.

***

In particular, Proposition 2 implies that a minimax policy for ${\Phi}$ is the optimal policy for some ${\mu^* \in \Phi}$.

Now we ask what policy is implemented by "subagents" created by a minimax policy. Suppose ${S = S_1 \times S_2}$, where ${S_2}$ represents the pure policies of the subagent. Assume that there is a Borel set ${A \subseteq E}$ (the condition for the creation of the subagent), a finite set ${T}$ (the actions taken before the creation of the subagent), a Borel measurable mapping ${\alpha: S_1 \rightarrow T}$ and continuous functions ${u_1: S_1 \times E \rightarrow \Reals}$ and ${u_2: S_2 \times T \times E \rightarrow \Reals}$ s.t. 

$${u(s_1,s_2,e)=u_1(s_1,e)+\chi_A(e) u_2(s_2,\alpha(s_1),e)}$$

Consider ${\Phi \in \CC(E)}$ and ${\pi^* \in \Prob(S)}$ minimax for ${\Phi}$. Denote ${\Prj_{1,2}: S \rightarrow S_{1,2}}$ the projection mappings. Define ${\pi_1^* \in \Prob(S_1)}$ by

$$\pi_1^* := \Prj_{1*}\pi^*$$

Define ${\pi_2^*: T \rightarrow \Prob(S_2)}$ by

$$\pi_2^*(t) := \Prj_{2*} (\pi^* \mid \alpha^{-1}(t) \times S_2)$$

If for some ${t_0 \in T}$ we have ${\pi^*(\alpha^{-1}(t_0) \times S_2)=0}$, then ${\pi^*_2(t_0)}$ can be arbitrary.

\subsection{Proposition 3}

$$\pi_2^* \in \Argmax{\pi_2: T \rightarrow \Prob(S_2)} \min_{\mu \in \Phi} (\E_{\pi_1^* \times \mu}[u_1] + \mu(A) \E_{t \sim \alpha_*\pi_1^*}[\E_{\pi_2(t) \times \mu \mid A}[u_2(t)]])$$

***

At this point, it should be possible to make do without ${T}$ and the associated decomposition of ${u}$ by instead decomposing ${\pi^*}$ into ${\pi_1^*}$ and a Markov kernel from ${S_1}$ to ${S_2}$. However, we won't need the general case.

In general, ${\pi_2^*}$ is not a minimax policy for any natural model i.e. the minimax decision rule is not "dynamically consistent". However, if we assume a certain factorization condition, it is. Specifically, assume ${E_1, E_2}$ are compact Polish, ${f: E_1 \times E_2 \rightarrow E}$ is Borel measurable, ${\bar{u}_1: S_1 \times E_1 \rightarrow \Reals}$ and ${\bar{u}_2: S_2 \times T \times E_2 \rightarrow \Reals}$ are continuous s.t. 

$${u_1(s_1,f(e_1,e_2))=\bar{u}_1(s_1,e_1)}$$

$${u_2(s_2,t,f(e_1,e_2))=\bar{u}_1(s_2,t,e_2)}$$

Further assume that ${A_1 \subseteq E_1}$ is Borel s.t. ${f^{-1}(A) = A_1 \times E_2}$ and ${\Phi_{1,2} \in \CC(E_{1,2})}$ are s.t. ${\Phi}$ is the closure of the convex hull of ${f_*(\Phi_1 \times \Phi_2)}$.

\subsection{Proposition 4}

Assume that ${\min_{\mu \in \Phi_1} \mu_1(A_1) > 0}$. Then,

$$\pi_2^* \in \Argmax{\pi_2: T \rightarrow \Prob(S_2)} \min_{\mu \in \Phi_2} \E_{t \sim \alpha_*\pi_1^*}[\E_{\pi_2(t) \times \mu}[\bar{u}_2(t)]]$$

Moreover, define ${\bar{S}_2:=T \rightarrow S_2}$ equipped with the product topology, define ${\bar{\pi}_2^* \in \Prob(\bar{S}_2)}$ by

$${\bar{\pi}_2^* := \prod_{t \in T} \pi_2^*(t)}$$

Define ${\bar{E}_2:=T \times E_2}$ and define ${\bar{\Phi}_2 \in \CC(\bar{E}_2)}$ by

$$\bar{\Phi}_2:=\{\alpha_* \pi_1^* \times \mu \mid \mu \in \Phi_2\}$$

Finally, define ${\hat{u}_2: \bar{S}_2 \times \bar{E}_2 \rightarrow \Reals}$ by

$$\hat{u}_2(s,t,e):=\bar{u}_2(s(t),t,e)$$

Then, ${\bar{\pi}_2^*}$ is a minimax policy for ${\bar{\Phi}_2}$ w.r.t. the utility function ${\hat{u}_2}$.

\section{Asymptotic optimality}

Fix finite sets ${\Act}$ (actions) and ${\Obs}$ (observations). We now assume that ${E=\ObsO}$ and ${S=\Pol}$ with the product topology. We fix ${\gamma: \Nats \rightarrow \Reals^{\geq 0}}$ a time discount function satisfying ${\sum_n \gamma(n) < \infty}$ and ${r: (\Act \times \Obs)^* \rightarrow \Reals}$ a bounded reward function. Given ${e \in \Obs^*}$ and ${s: \Pol}$, we define ${e^s \in (\Act \times \Obs)^\omega}$ by

$${e^s_n:=(s(e_{<n}),e_n)}$$

The utility function is then given by

$$u(s,e)=\sum_{n \in \Nats} \gamma(n) r(e^s_{<n})$$

We will need a notation for a combination of two policies where the agent switches from one to another when observing some ${x \in \Obs^*}$. Given ${s_1: \Pol}$ and ${s_2: \Pol}$, we define ${s_1 \times_x s_2: \Pol}$ by

$$(s_1 \times_x s_2)(y):=\begin{cases}s_1(y) \text{ if } y \not\sqsupseteq x \\s_2(z) \text{ if } y=xz\end{cases}$$

Consider any ${\pi \in \Prob(\Pol)}$ and ${\rho: \Act^{\Abs{x}} \rightarrow \Prob(\Pol)}$. We define ${\pi \otimes_x \rho \in \Prob(\Obs^* \rightarrow \Act)}$ as follows. For any Borel measurable ${B \subseteq \Pol}$:

$$(\pi \otimes_x \rho)(B):=\sum_{t \in \Act^{\Abs{x}}} \PP{s_1 \sim \pi}{s_2 \sim \rho(t)}[s_1 \times_x s_2 \in B \land \forall n < \Abs{x}: t_n=s_1(x_{<n})]$$ 

It is easy to see the above indeed defines a probability measure.

Note that changing the policy after a certain event cannot change utility more than ${O(1)}$ times the probability of the event times the corresponding time discount factor. That is, we have the following:

\subsection{Proposition 5}

$$\Abs{\E_{(\pi \otimes_x \rho) \times \mu}[u]-\E_{\pi \times \mu}[u]} \leq (\sup r - \inf r) \Prb_{e \sim \mu}[x \sqsubset e] \sum_{n > \Abs{x}} \gamma(n)$$

***

We are now ready to formulate the optimality theorem. Consider ${\Phi,\Phi' \in \CC(\ObsO)}$ and ${p \in (0,1]}$. Denote ${\Psi = p \Phi + (1-p) \Phi'}$. We think of ${\Psi}$ as the prior, ${\Phi}$ as one of the models in the prior (assigned probability ${p}$) and ${\Phi'}$ as the convex combination of all other models. Let ${\pi^* \in \Prob(\Pol)}$ be a minimax policy for ${\Psi}$. Define ${v: \Obs^* \rightarrow \Reals}$ by

$$v(x):=\max_{\rho \in \Act^{\Abs{x}} \rightarrow \Prob(\Pol)} \min_{\mu \in \Phi} \E_{(\pi^* \otimes_x \rho) \times \mu}[u]$$

That is, ${v}$ is the expected utility that can be guaranteed by changing the policy following ${x}$, assuming that the true environment is in ${\Phi}$. We claim that if the true environment is in ${\Phi}$, then after sufficient time ${\pi^*}$ will achieve at least as much utility as can be guaranteed for ${\Phi}$ (guaranteed under the constraint of following ${\pi^*}$ until the point of making the current observations). That is, ${v}$ can only be greater than ${\E_{\pi^* \times \mu}[u]}$ by a quantity that goes to 0 with probability 1, even after normalizing according to Proposition~5:

\subsection{Theorem 1}

$$\forall \mu \in \Phi: \Prb_{e \sim \mu}[\lim_{n \rightarrow \infty} \max(\frac{v(e_{<n})-\E_{\pi^* \times \mu}[u]}{\Prb_{e' \sim \mu}[e_{<n} \sqsubset e'] \sum_{m > n} \gamma(m)},0)=0] = 1$$

***

Compared to the Bayesian case, ${v(x)}$ has the strange property that its definition depends on rewards other than those received after observing ${x}$. This is a direct consequence of the dynamic inconsistency of minimax. In order to get an asymptotic optimality property without this dependence, we need the model to satisfy a factorization condition similar to Proposition 4.

\subsection{Definition 1}

Consider ${\Phi \in \CC(\ObsO)}$ and ${x \in \Obs^*}$. Define ${f^x: \ObsO \times \ObsO \rightarrow \ObsO}$ by

$${f^x(e_1,e_2):=\begin{cases}e_1 \text{ if } x \not\sqsubset e_1 \\xe_2 \text{ if } x \sqsubset e_1\end{cases}}$$

${\Phi}$ is said to *factorize at ${x}$* when there are ${\Phi_1, \Phi_2 \subseteq \Prob(\ObsO)}$ s.t. ${\Phi}$ is the closure of the convex hull of ${f^x_*(\Phi_1 \times \Phi_2)}$.

Given ${e \in \ObsO}$, ${\Phi}$ is said to *factorize over ${e}$* when there are infinitely many ${n \in \Nats}$ s.t. ${\Phi}$ factorizes at ${e_{<n}}$.

Given a time discount function ${\gamma}$, ${\Phi}$ is said to *factorize frequently over ${e}$* (w.r.t. ${\gamma}$) when there is an increasing sequence ${\{n_k \in \Nats\}_{k \in \Nats}}$ s.t.

i. ${\Phi}$ factorizes at ${e_{<n_k}}$ for all ${k}$.

ii. There is ${\epsilon > 0}$ s.t. for all ${k}$

$$\frac{\sum_{n = n_{k+1}-1}^\infty \gamma(n)}{\sum_{n = n_{k}}^\infty \gamma(n)} > \epsilon$$

For example, for geometric discount (${\gamma(n)=\beta^n)}$, condition ii means that ${n_{k+1}-n_k}$  is bounded.

${\Phi}$ is said to *factorize everywhere* when 

$${\forall \mu \in \Phi: \Prb_{e \sim \mu}[\Phi \text{ factorizes over } e] = 1}$$

${\Phi}$ is said to *factorize frequently everywhere* (w.r.t. ${\gamma}$) when

$${\forall \mu \in \Phi: \Prb_{e \sim \mu}[\Phi \text{ factorizes frequently over } e] = 1}$$

***

Note that any singleton ${\{\mu\}}$ factorizes frequently everywhere w.r.t. any time discount function. More generally, we can give the following explicit description of models that factorize everywhere:

\subsection{Construction}

Consider some ${F: \Obs^* \rightarrow \CC(\Obs^+)}$ (${\Obs^+}$ is considered to be a discrete space). Assume that for any ${x \in \Obs}$, there is ${A_x \subseteq \Obs^+}$ prefix-free s.t. any ${\mu \in F(x)}$ is supported on ${A_x}$.

We define ${X_F \subseteq \Obs^*}$ as the minimal set with the following properties:

i. ${\Estr_\Obs \in X_F}$

ii. If ${x \in X_F}$, ${\mu \in F(x)}$ and ${y \in \Obs^+}$ is s.t. ${\mu(y) > 0}$, then ${xy \in X_F}$.

We define ${\Phi_F \subseteq \Prob(\ObsO)}$ as follows. ${\mu \in \Phi_F}$ iff for any ${x \in X_F}$, there is ${\mu_x \in F(x)}$ s.t. for any ${y \in \Obs^+}$, if ${\mu_x(y) > 0}$ then

$$\Prb_{e \sim \mu}[xy \sqsubset e] = \Prb_{e \sim \mu}[x \sqsubset e] \mu_x(y)$$

\subsection{Proposition 6}

For any ${F}$ as above, ${\Phi_F \in \CC(\ObsO)}$ and factorizes everywhere.

***

It is also easy to use the above construction to get ${\Phi}$ that factorizes frequently everywhere w.r.t. given ${\gamma}$ (for example, if we require that for every ${x \in X_F}$, ${F(x)}$ is supported on strings of uniformly bounded length, ${\Phi_F}$ will factorize frequently everywhere w.r.t. geometric discount; this condition on ${F}$ is sufficient but not necessary).

Now consider again ${\Phi}$, ${\Psi}$ and ${\pi^*}$ as before. Define ${u_{\geq n}: \ObsO \times (\Pol) \rightarrow \Reals}$ to be normalized sum of rewards from time ${n}$, i.e.

$$u_{> n}(s,e):=\frac{\sum_{m > n} \gamma(m)r(e^s_{<m})}{\sum_{m > n} \gamma(m)}$$

Define ${w: \Obs^* \rightarrow \Reals}$ by

$$w(x):=\max_{\rho \in \Act^{\Abs{x}} \rightarrow \Prob(\Pol)} \min_{\mu \in \Phi} \EE{s \sim \pi^* \otimes_x \rho}{e \sim \mu}[u_{> \Abs{x}}(s,e) \mid x \sqsubset e]$$

\subsection{Corollary}

Assume ${\Phi}$ factorizes everywhere. Then,

$$\forall \mu \in \Phi: \Prb_{e \sim \mu}[\liminf_{n \rightarrow \infty} \max(w(e_{<n})-\EE{s \sim \pi^*}{e' \sim \mu}[u_{> n}(s,e') \mid e_{<n} \sqsubset e'] ,0)=0] = 1$$

Moreover, if ${\Phi}$ factorizes frequently everywhere, then

$$\forall \mu \in \Phi: \Prb_{e \sim \mu}[\lim_{n \rightarrow \infty} \max(w(e_{<n})-\EE{s \sim \pi^*}{e' \sim \mu}[u_{> n}(s,e') \mid e_{<n} \sqsubset e'] ,0)=0] = 1$$

%##Appendix
\section{Appendix A}

We will repeatedly use the standard fact that, given ${X}$ a compact Polish space, ${\Prob(X)}$ is also a compact Polish space (it is metrizable by the Levy-Prokhorov metric, compact as a consequence of the Banach-Alaoglu theorem and the fact that any probability measure on a Polish space is Radon, and separability is also not hard to prove).

\subsection{Proposition A.1}

Consider ${X, Y}$ compact Polish spaces, ${f: X \times Y \rightarrow \Reals}$ continuous, ${x_\infty \in X}$ and a sequence ${x_n \rightarrow x_\infty}$. Define ${g_n: Y \rightarrow \Reals}$ by ${g_n(y):=f(x_n,y)}$ and ${g_\infty: Y \rightarrow \Reals}$ by ${g_\infty(y):=f(x_\infty,y)}$. Then, ${g_n}$ converges to ${g_\infty}$ uniformly.

\subsection{Proof of Proposition A.1}

Assume to the contrary that there is ${\epsilon > 0}$ and a subsequence ${\{x'_n \in X\}_{n \in \Nats}}$ of ${\{x_n\}}$ s.t. 

$${\max_{y \in Y} \Abs{f(x'_n,y)-f(x_\infty,y)} > \epsilon}$$

This implies we can choose ${\{y_n \in Y\}_{n \in \Nats}}$ s.t. ${\Abs{f(x'_n,y_n)-f(x_\infty,y_n)} > \epsilon}$. Let ${\{n_k \in \Nats\}_{k \in \Nats}}$ be an increasing sequence s.t. ${y_{n_k} \rightarrow y_\infty}$ for some ${y_\infty \in Y}$. We get ${\Abs{f(x'_{n_k},y_{n_k})-f(x_\infty,y_{n_k})} > \epsilon}$, which is a contradiction because ${f(x'_{n_k},y_{n_k}) \rightarrow f(x_\infty,y_\infty)}$ and ${f(x_\infty,y_{n_k}) \rightarrow f(x_\infty,y_\infty)}$.

\subsection{Proposition A.2}

Consider ${X,Y}$ compact Polish spaces and ${f: X \times Y \rightarrow \Reals}$ continuous. Define ${\phi: X \times \Prob(Y) \rightarrow \Reals}$ by 

$${\phi(x,\nu):=\E_{y \sim \nu}[f(x,y)]}$$ 

Then, ${\phi}$ is continuous.

\subsection{Proof of Proposition A.2}

Consider any ${x_\infty \in X}$, ${\nu_\infty \in \Prob(Y)}$ and sequences ${x_n \rightarrow x_\infty}$, ${\nu_n \rightarrow \nu_\infty}$. We have

$$\phi(x_n,\nu_n)- \phi(x_\infty,\nu_\infty)=\E_{y \sim \nu_n}[f(x_n,y)-f(x_\infty,y)]+E_{y \sim \nu_n}[f(x_\infty,y)]-E_{y \sim \nu_\infty}[f(x_\infty,y)]$$

On the right hand side, the first term goes to 0 by Proposition A.1 and the second term goes to 0 by definition of the weak* topology.

\subsection{Proposition A.3}

Consider any ${X,Y}$ compact Polish and ${f: X \times Y \rightarrow \Reals}$ continuous. Define ${F: \Prob(X) \times \Prob(Y) \rightarrow \Reals}$ by 

$${F(\mu,\nu):=\E_{\mu \times \nu}[f]}$$ 

Then, ${F}$ is continuous.

\subsection{Proof of Proposition A.3}

Follows by applying Fubini's theorem and using Proposition A.2 twice.

\subsection{Proposition A.4}

Consider any ${X,Y}$ compact Polish, ${A \subseteq Y}$ and ${f: X \times Y \rightarrow \Reals}$ continuous. Define ${f_A: X \rightarrow \Reals}$ by ${f_A(x):= \inf_{y \in A} f(x,y)}$. Then, ${f_A}$ is continuous.

\subsection{Proof of Proposition A.4}

Consider any ${x_\infty \in X}$ and a sequence ${x_n \rightarrow x_\infty}$

$$\Abs{\inf_{y \in A} f(x_n,y) - \inf_{y \in A} f(x_\infty,y)} \leq \sup_{y \in A} \Abs{f(x_n,y) - f(x_\infty,y)}$$

By Proposition A.1, the right hand converges to 0, therefore the left hand side also converges to 0.

***

In the following, we will use the notation ${U: \Prob(S) \times \Prob(E) \rightarrow \Reals}$, defined by ${U(\pi,\mu):=\E_{\pi \times \mu}[u]}$. By Proposition A.3, ${U}$ is continuous. 

\subsection{Proof of Proposition 1}

Define ${U_\Phi: \Prob(S) \rightarrow \Reals}$ by ${U_\Phi(\pi):= \inf_{\mu \in \Phi} U(\pi,\mu)}$. By Proposition A.4, ${U_{\Phi}}$ is continuous and therefore, ${\pi^*}$ exists.

${U}$ is continuous and affine in the 2nd argument (i.e. it sends convex linear combinations to convex linear combinations), and therefore ${\inf_{\mu \in \Phi} U(\pi,\mu)=\min_{\mu \in \bar{\Phi}} U(\pi,\mu)}$, implying the second part of the proposition.

\subsection{Proof of Proposition 2}

Define ${V: \Prob(E) \rightarrow \Reals}$ by ${V(\mu):=\max_{\pi \in \Prob(S)} U(\pi,\mu)=\max_{s \in S} U(\delta_s,\mu)}$. Denote ${\Psi:=p\Phi + (1-p)\Phi'}$. By Proposition A.4, ${V}$ is continuous and therefore, there exists

$$\xi^* \in \Argmin{\mu \in \Psi} V(\mu)$$

Choose ${\mu^* \in \Phi}$, ${\nu^* \in \Phi'}$ s.t. ${\xi^* = p \mu^* + (1-p) \nu^*}$.

${\Prob(S)}$ and ${\Psi}$ are compact convex sets in the spaces of finite signed Borel measures on ${S}$ and ${E}$ respectively. Therefore, we can apply the minimax theorem to get

$$\max_{\pi \in \Prob(S)} \min_{\xi \in \Psi} U(\pi,\xi) = \min_{\xi \in \Psi} \max_{s \in S} U(\delta_s,\xi)$$

Denote the above number ${u^*}$. We have

$$\min_{\mu \in \Phi} U(\pi^*,p\mu + (1-p)\nu^*) \geq \min_{\xi \in \Psi} U(\pi^*,\xi)=u^*$$

On the other hand

$$\max_{\pi \in \Prob(S)} \min_{\mu \in \Phi} U(\pi,p\mu + (1-p)\nu^*) \leq \max_{\pi \in \Prob(S)}  U(\pi,\xi^*)=u^*$$

Combining the inequalities, we get the desired result.

\subsection{Definition A.1}

In the setting of Proposition 3, consider any ${\pi_1 \in \Prob(S_1)}$ and ${\pi_2: T \rightarrow \Prob(S_2)}$. We define ${\pi_1 \otimes_\alpha \pi_2 \in \Prob(S_1 \times S_2)}$ as follows. For any Borel measurable ${B \subseteq S_1 \times S_2}$:

$$(\pi_1 \otimes_\alpha \pi_2)(B):=\sum_{t \in T} (\pi_1 \times \pi_2(t))(B \cap (\alpha^{-1}(t) \times S_2))$$ 

It is easy to see the above indeed defines a probability measure.

\subsection{Proposition A.5}

Consider any ${\pi_1 \in \Prob(S_1)}$ and ${\pi_2: T \rightarrow \Prob(S_2)}$ and ${f: S_1 \times S_2 \rightarrow \Reals}$ Borel measurable and bounded. Then:

$$\E_{\pi_1 \otimes_\alpha \pi_2}[f] = \E_{s_1 \sim \pi_1}[\E_{s_2 \sim \pi_2(\alpha(s_1))}[f(s_1, s_2)]]$$

\subsection{Proof of Proposition A.5}

$$\E_{\pi_1 \otimes_\alpha \pi_2}[f] = \sum_{t \in T} \int_{\alpha^{-1}(t) \times S_2} f(s_1, s_2) d(\pi_1 \times \pi_2(t))(s_2)$$

$$\E_{\pi_1 \otimes_\alpha \pi_2}[f] = \sum_{t \in T} \int_{\alpha^{-1}(t)} \int_{S_2} f(s_1, s_2) d\pi_2(t)(s_2) d\pi_1(s_1)$$

$$\E_{\pi_1 \otimes_\alpha \pi_2}[f] = \sum_{t \in T} \int_{\alpha^{-1}(t)} \E_{s_2 \sim \pi_2(t)}[f(s_1,s_2)] d\pi_1(s_1)$$

$$\E_{\pi_1 \otimes_\alpha \pi_2}[f] = \sum_{t \in T} \int_{\alpha^{-1}(t)} \E_{s_2 \sim \pi_2(\alpha(s_1))}[f(s_1,s_2)] d\pi_1(s_1)$$

$$\E_{\pi_1 \otimes_\alpha \pi_2}[f] = \int_{S_1} \E_{s_2 \sim \pi_2(\alpha(s_1))}[f(s_1,s_2)] d\pi_1(s_1)$$

$$\E_{\pi_1 \otimes_\alpha \pi_2}[f] = \E_{s_1 \sim \pi_1}[\E_{s_2 \sim \pi_2(\alpha(s_1))}[f(s_1, s_2)]]$$

\subsection{Proposition A.6}

Given ${\pi_1 \in \Prob(S_1)}$, ${\pi_2: T \rightarrow \Prob(S_2)}$ and ${\mu \in \Prob(E)}$

$$U(\pi_1 \otimes_\alpha \pi_2, \mu) = \E_{\pi_1 \times \mu}[u_1] + \mu(A) \E_{t \sim \alpha_*\pi_1}[\E_{\pi_2(t) \times \mu \mid A}[u_2(t)]]$$

\subsection{Proof of Proposition A.6}

Applying Proposition A.5:

$$U(\pi_1 \otimes_\alpha \pi_2, \mu) = \E_{s_1 \sim \pi_1}[\E_{s_2 \sim \pi_2(\alpha(s_1))}[\E_{e \sim \mu}[u_1(s_1,e)+\chi_A(e) u_2(s_2,\alpha(s_1),e)]]]$$

$$U(\pi_1 \otimes_\alpha \pi_2, \mu) = \E_{\pi_1 \times \mu}[u_1] + \E_{s_1 \sim \pi_1}[\E_{s_2 \sim \pi_2(\alpha(s_1))}[\mu(A) \E_{e \sim \mu \mid A}[u_2(s_2,\alpha(s_1),e)]]]$$

$$U(\pi_1 \otimes_\alpha \pi_2, \mu) = \E_{\pi_1 \times \mu}[u_1] + \mu(A) \E_{t \sim \alpha_*\pi_1}[\E_{\pi_2(t) \times \mu \mid A}[u_2(t)]]$$

\subsection{Proposition A.7}

Consider any ${\pi \in \Prob(S_1 \times S_2)}$. Define ${\pi_1 \in \Prob(S_1)}$ and ${\pi_2: T \rightarrow \Prob(S_2)}$ by

$$\pi_1 := \Prj_{1*}\pi$$

$$\pi_2(t) := \Prj_{2*} (\pi \mid \alpha^{-1}(t) \times S_2)$$

If for some ${t_0 \in T}$ we have ${\pi(\alpha^{-1}(t_0) \times S_2)=0}$, then ${\pi_2(t_0)}$ can be arbitrary.

Then,

$$\forall \mu \in \Prob(E):U(\pi,\mu)=U(\pi_1 \otimes_\alpha \pi_2, \mu)$$

\subsection{Proof of Proposition A.7}

$$U(\pi, \mu) = \EE{(s_1,s_2) \sim \pi}{e \sim \mu}[u_1(s_1,e) + \chi_A(e) u_2(s_2, \alpha(s_1), e)]$$

$$U(\pi, \mu) = \E_{\pi_1 \times \mu}[u_1] + \mu(A) \EE{(s_1,s_2) \sim \pi}{e \sim \mu \mid A}[u_2(s_2, \alpha(s_1), e)]$$

$$U(\pi, \mu) = \E_{\pi_1 \times \mu}[u_1] + \mu(A) \sum_{t \in T} \Prb_{(s_1, s_2) \sim \pi}[\alpha(s_1)=t] \EE{(s_1,s_2) \sim \pi \mid \alpha^{-1}(t) \times S_2}{e \sim \mu \mid A}[u_2(s_2, t, e)]$$

$$U(\pi, \mu) = \E_{\pi_1 \times \mu}[u_1] + \mu(A) \E_{t \sim \alpha_*\pi_1}[\E_{\pi_2(t) \times \mu \mid A}[u_2(t)]]$$

Applying Proposition A.6, we get the desired result.

\subsection{Proof of Proposition 3}

Consider any ${\pi_2: T \rightarrow \Prob(S_2)}$. By definition of ${\pi^*}$ 

$${\min_{\mu \in \Phi} U(\pi_1^* \otimes_\alpha \pi_2, \mu) \leq \min_{\mu \in \Phi} U(\pi^*, \mu)}$$

Applying Proposition A.7 to the right hand side

$${\min_{\mu \in \Phi} U(\pi_1^* \otimes_\alpha \pi_2, \mu) \leq \min_{\mu \in \Phi} U(\pi_1^* \otimes_\alpha \pi_2^*, \mu)}$$

Applying Proposition A.6 to both sides, we get the desired result.

\subsection{Proof of Proposition 4}

Using Proposition 1 and Proposition 3, we have

$$\pi_2^* \in \Argmax{\pi_2: T \rightarrow \Prob(S_2)} \min_{\substack{\mu_1 \in \Phi_1 \\ \mu_2 \in \Phi_2}} (\E_{\pi_1^* \times f_*(\mu_1 \times \mu_2)}[u_1] + (\mu_1 \times \mu_2)(f^{-1}(A)) \E_{t \sim \alpha_*\pi_1^*}[\E_{\pi_2(t) \times f_*(\mu_1 \times \mu_2) \mid A}[u_2(t)]])$$

$$\pi_2^* \in \Argmax{\pi_2: T \rightarrow \Prob(S_2)} \min_{\substack{\mu_1 \in \Phi_1 \\ \mu_2 \in \Phi_2}} (\E_{\pi_1^* \times \mu_1}[\bar{u}_1] + \mu_1(A_1) \E_{t \sim \alpha_*\pi_1^*}[\E_{\pi_2(t) \times \mu_2}[\bar{u}_2(t)]])$$

$$\pi_2^* \in \Argmax{\pi_2: T \rightarrow \Prob(S_2)} \min_{\mu_1 \in \Phi_1} (\E_{\pi_1^* \times \mu_1}[\bar{u}_1] + \mu_1(A_1) \min_{\mu_2 \in \Phi_2} \E_{t \sim \alpha_*\pi_1^*}[\E_{\pi_2(t) \times \mu_2}[\bar{u}_2(t)]])$$

Using the assumption ${\min_{\mu_1 \in \Phi_1} \mu_1(A_1) > 0}$, we get the desired result.

Now consider any ${\bar{\pi}_2 \in \Prob(\bar{S}_2)}$. Define ${\pi_2: T \rightarrow \Prob(S_2)}$ by ${(\pi_2(t))(B):=\Prb_{\sigma \sim \bar{\pi}_2}[\sigma(t) \in B]}$. We have

$$\E_{t \sim \alpha_*\pi_1^*}[\EE{s \sim \pi_2(t)}{e \sim \mu_2}[\bar{u}_2(s,t,e)]] = \E_{t \sim \alpha_*\pi_1^*}[\EE{\sigma \sim \bar{\pi}_2}{e \sim \mu_2}[\bar{u}_2(\sigma(t),t,e)]]$$

$$\E_{t \sim \alpha_*\pi_1^*}[\EE{s \sim \pi_2(t)}{e \sim \mu_2}[\bar{u}_2(s,t,e)]] = \E_{\bar{\pi}_2 \times \alpha_* \pi^*_1 \times \mu}[\hat{u}_2]$$

For the special case ${\bar{\pi}_2=\bar{\pi}^*_2=\prod_{t \in T} \pi_2^*(t)}$, we get ${\pi_2=\pi^*_2}$ and therefore

$$\E_{t \sim \alpha_*\pi_1^*}[\EE{s \sim \pi^*_2(t)}{e \sim \mu_2}[\bar{u}_2(s,t,e)]] = \E_{\bar{\pi}^*_2 \times \alpha_* \pi^*_1 \times \mu}[\hat{u}_2]$$

Using the property of ${\pi^*_2}$, it follows that

$$\forall \mu \in \Phi_2: \E_{\bar{\pi}_2 \times \alpha_* \pi^*_1 \times \mu}[\hat{u}_2] \leq \E_{\bar{\pi}^*_2 \times \alpha_* \pi^*_1 \times \mu}[\hat{u}_2]$$

$$\forall \bar{\mu} \in \bar{\Phi}_2: \E_{\bar{\pi}_2 \times \bar{\mu}}[\hat{u}_2] \leq \E_{\bar{\pi}^*_2 \times \bar{\mu}}[\hat{u}_2]$$

***

Given any ${x \in \Obs^*}$, we denote ${x\ObsO:=\{xy \mid y \in \ObsO\}}$ and ${\bar{x}\ObsO:=\ObsO \setminus x\ObsO}$.

\subsection{Proof of Proposition 5}

We have

$$\E_{\pi \times \mu}[u] = \mu(x\ObsO) \E_{\pi \times \mu \mid x \ObsO}[u] + (1 - \mu(x\ObsO)) \E_{\pi \times \mu \mid \bar{x} \Obs^*}[u]$$

$$\E_{(\pi \otimes_x \rho) \times \mu}[u] = \mu(x\ObsO) \E_{(\pi \otimes_x \rho) \times \mu \mid x \ObsO}[u] + (1 - \mu(x\ObsO)) \E_{\pi \times \mu \mid \bar{x} \ObsO}[u]$$

$$\E_{(\pi \otimes_x \rho) \times \mu}[u]-\E_{\pi \times \mu}[u] = \mu(x\ObsO)(\E_{(\pi \otimes_x \rho) \times \mu \mid x \ObsO}[u]-\E_{\pi \times \mu \mid x \ObsO}[u])$$

Denote ${u_1(s,e):=\sum_{n \leq \Abs{x}} \gamma(n) r(e^s_{<n})}$, ${u_2(s,e):=\sum_{n > \Abs{x}} \gamma(n) r(e^s_{<n})}$. We have

$$\E_{\pi \times \mu \mid x \ObsO}[u] = \E_{\pi \times \mu \mid x \ObsO}[u_1] + \E_{\pi \times \mu \mid x \Obs^*}[u_2]$$

$$\E_{(\pi \otimes_x \rho) \times \mu \mid x \ObsO}[u] = \E_{\pi \times \mu \mid x \ObsO}[u_1] + \E_{(\pi \otimes_x \rho) \times \mu \mid x \ObsO}[u_2]$$

$$\E_{(\pi \otimes_x \rho) \times \mu}[u]-\E_{\pi \times \mu}[u] = \mu(x\ObsO)(\E_{(\pi \otimes_x \rho) \times \mu \mid x \ObsO}[u_2]-\E_{\pi \times \mu \mid x \ObsO}[u_2])$$

The desired result now follows from ${(\inf r) \sum_{n > \Abs{x}} \gamma(n) \leq u_2 \leq (\sup r) \sum_{n > \Abs{x}} \gamma(n)}$.

\subsection{Definition A.2}

A *splitting* of ${(S,E,u)}$ is ${(S_1,S_2,A,T,\alpha,u_1,u_2)}$ s.t. ${S \cong S_1 \times S_2}$, ${A \subseteq E}$ is Borel, ${T}$ is a finite set, ${\alpha: S_1 \rightarrow T}$ is Borel measurable, ${u_1: S_1 \times E \rightarrow \Reals}$ and ${u_2: S_2 \times T \times E \rightarrow \Reals}$ are continuous and

$$u(s_1,s_2,e) = u_1(s_1,e) + \chi_A(e) u_2(s_2, \alpha(s_1), e)$$

\subsection{Proposition A.8}

Consider ${\Phi \in \CC(E)}$, ${\nu^* \in \Prob(E)}$ and ${p \in [0,1]}$, and denote ${\Psi:=p\Phi+(1-p)\nu^*}$. Suppose that ${(S_1,S_2,A,T,\alpha,u_1,u_2)}$ is a splitting of ${(S,E,u)}$. Assume ${\nu^*(A) > 0}$. Fix ${\pi \in \Prob(S_1)}$.  Let ${\rho^*: T \rightarrow \Prob(S_2)}$ satisfy

$$\rho^* \in \Argmax{\rho: T \rightarrow \Prob(S_2)} \min_{\xi \in \Psi} U(\pi \otimes_\alpha \rho, \xi)$$

Denote ${\Gamma := \sup u_2 - \inf u_2}$. Then, for any ${\mu_0 \in \Phi}$ s.t. ${\mu_0(A) > 0}$

$$U(\pi \otimes_\alpha \rho^*,\mu_0) \geq \max_{\rho: T \rightarrow \Prob(S_2)} \min_{\mu \in \Phi} U(\pi \otimes_\alpha \rho, \mu)-2\Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)$$

\subsection{Proof of Proposition A.8}

Let ${\tau^*: T \rightarrow \Prob(S_2)}$ satisfy

$$\tau^* \in \Argmax{\rho: T \rightarrow \Prob(S_2)} \min_{\mu \in \Phi} U(\pi \otimes_\alpha \rho, \mu)$$

Denote ${u^*:= \min_{\mu \in \Phi} U(\pi \otimes_\alpha \tau^*, \mu)}$. Consider any ${\mu \in \Phi}$ and denote ${\xi:=p\mu+(1-p)\nu^*}$. We have

$$U(\pi \otimes_\alpha \tau^*, \xi) = p U(\pi \otimes_\alpha \tau^*, \mu') + (1-p) U(\pi \otimes_\alpha \tau^*, \nu^*)$$

$$U(\pi \otimes_\alpha \tau^*, \xi) \geq pu^* + (1-p) U(\pi \otimes_\alpha \tau^*, \nu^*)$$

Applying Proposition A.6 to the right hand side

$$U(\pi \otimes_\alpha \tau^*, \xi) \geq pu^* + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)\E_{t \sim \alpha_* \pi}[\E_{\tau^*(t) \times \nu^*}[u_2 \mid A]])$$

$$U(\pi \otimes_\alpha \tau^*, \xi) \geq pu^* + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)(\E_{t \sim \alpha_* \pi}[\E_{\tau^*(t) \times \mu_0}[u_2 \mid A]]- \Gamma\Dtv(\mu_0 \mid A, \nu^* \mid A)))$$

Also, we have

$$U(\pi \otimes_\alpha \tau^*, \mu_0) \geq u^*$$

Applying Proposition A.6 again

$$\E_{\pi \times \mu_0}[u_1] + \mu_0(A)\E_{t \sim \alpha_* \pi}[\E_{\tau^*(t) \times \mu_0}[u_2 \mid A]] \geq u^*$$

$$\E_{t \sim \alpha_* \pi}[\E_{\tau^*(t) \times \mu_0}[u_2 \mid A]] \geq \frac{u^* - \E_{\pi \times \mu_0}[u_1]}{\mu_0(A)}$$

Combining the inequalities

$$U(\pi \otimes_\alpha \tau^*, \xi) \geq pu^* + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)(\frac{u^* - \E_{\pi \times \mu_0}[u_1]}{\mu_0(A)}- \Gamma\Dtv(\mu_0 \mid A, \nu^* \mid A)))$$

$$U(\pi \otimes_\alpha \tau^*, \xi) \geq pu^* + (1-p) (\E_{\pi \times \nu^*}[u_1] + \frac{\nu^*(A)}{\mu_0(A)}(u^* - \E_{\pi \times \mu_0}[u_1]- \Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)))$$

$$U(\pi \otimes_\alpha \tau^*, \xi) \geq (p + (1-p) \frac{\nu^*(A)}{\mu_0(A)})u^* + (1-p) (\E_{\pi \times \nu^*}[u_1] - \frac{\nu^*(A)}{\mu_0(A)}(\E_{\pi \times \mu_0}[u_1]+ \Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)))$$

The above inequality holds for any ${\xi}$, therefore

$$\min_{\xi \in \Psi} U(\pi \otimes_\alpha \tau^*, \xi) \geq (p + (1-p) \frac{\nu^*(A)}{\mu_0(A)})u^* + (1-p) (\E_{\pi \times \nu^*}[u_1] - \frac{\nu^*(A)}{\mu_0(A)}(\E_{\pi \times \mu_0}[u_1]+ \Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)))$$

By definition of ${\rho^*}$, it follows that

$$\min_{\xi \in \Psi} U(\pi \otimes_\alpha \rho^*, \xi) \geq (p + (1-p) \frac{\nu^*(A)}{\mu_0(A)})u^* + (1-p) (\E_{\pi \times \nu^*}[u_1] - \frac{\nu^*(A)}{\mu_0(A)}(\E_{\pi \times \mu_0}[u_1]+ \Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)))$$

Denote ${\xi_0:= p \mu_0 + (1-p) \nu^*}$ and ${u:=U(\pi \otimes_\alpha \rho^*, \mu_0)}$. We have

$$U(\pi \otimes_\alpha \rho^*, \xi_0) = pu + (1-p) U(\pi \otimes_\alpha \rho^*, \nu^*)$$

Applying Proposition A.6 to the right hand side

$$U(\pi \otimes_\alpha \rho^*, \xi_0) = pu + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)\E_{t \sim \alpha_* \pi}[\E_{\rho^*(t) \times \nu^*}[u_2 \mid A]])$$

$$U(\pi \otimes_\alpha \rho^*, \xi_0) \leq pu + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)(\E_{t \sim \alpha_* \pi}[\E_{\rho^*(t) \times \mu_0}[u_2 \mid A]]+\Gamma \Dtv(\mu_0 \mid A, \nu^* \mid A))$$

On the other hand, Proposition A.6 implies that

$$\E_{\pi \times \mu_0}[u_1] + \mu_0(A)\E_{t \sim \alpha_* \pi}[\E_{\rho^*(t) \times \mu_0}[u_2 \mid A]] = u$$

$$\E_{t \sim \alpha_* \pi}[\E_{\rho^*(t) \times \mu_0}[u_2 \mid A]] = \frac{u-\E_{\pi \times \mu_0}[u_1]}{\mu_0(A)}$$

Substituting in the inequality, we get

$$U(\pi \otimes_\alpha \rho^*, \xi_0) \leq pu + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)(\frac{u-\E_{\pi \times \mu_0}[u_1]}{\mu_0(A)}+\Gamma \Dtv(\mu_0 \mid A, \nu^* \mid A))$$

$$U(\pi \otimes_\alpha \rho^*, \xi_0) \leq pu + (1-p) (\E_{\pi \times \nu^*}[u_1] + \frac{\nu^*(A)}{\mu_0(A)}(u-\E_{\pi \times \mu_0}[u_1]+\Gamma \mu_0(A) \Dtv(\mu_0 \mid A, \nu^* \mid A))$$

$$U(\pi \otimes_\alpha \rho^*, \xi_0) \leq (p + (1-p)\frac{\nu^*(A)}{\mu_0(A)})u + (1-p) (\E_{\pi \times \nu^*}[u_1] - \frac{\nu^*(A)}{\mu_0(A)}(\E_{\pi \times \mu_0}[u_1]-\Gamma \mu_0(A) \Dtv(\mu_0 \mid A, \nu^* \mid A))$$

Observing that ${U(\pi \otimes_\alpha \rho^*, \xi_0) \geq \min_{\xi \in \Psi} U(\pi \otimes_\alpha \rho^*, \xi)}$ we can combine the inequalities, we get

$$(p + (1-p)\frac{\nu^*(A)}{\mu_0(A)})(u-u^*)\geq 2 (1-p) ( - \frac{\nu^*(A)}{\mu_0(A)}\Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A))$$

If ${p = 1}$, we get ${u \geq u^*}$, as desired. If ${p < 1}$, we divide both sides by ${(1-p)\frac{\nu^*(A)}{\mu_0(A)}}$ and get

$$(\frac{p}{1-p}\frac{\mu_0(A)}{\nu^*(A)}+1)(u - u^*) \geq - 2\Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)$$

$$u \geq u^* - \frac{2\Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)}{\frac{p}{1-p}\frac{\mu_0(A)}{\nu^*(A)}+1}$$

$$u \geq u^* - 2\Gamma\mu_0(A)\Dtv(\mu_0 \mid A, \nu^* \mid A)$$

\subsection{Proposition A.9}

In the setting of Proposition A.8, assuming ${p > 0}$ but sans the assumption ${\nu^*(A) > 0}$:

$$U(\pi \otimes_\alpha \rho^*,\mu_0) \geq \max_{\rho: T \rightarrow \Prob(S_2)} \min_{\mu \in \Phi} U(\pi \otimes_\alpha \rho, \mu)-\frac{1-p}{p}\Gamma\nu^*(A)$$

\subsection{Proof of Proposition A.9}

We use the same notation as in the proof of Proposition A.8. As before, we have

$$\min_{\xi \in \Psi} U(\pi \otimes_\alpha \tau^*, \xi) \geq pu^* + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)\E_{t \sim \alpha_* \pi}[\E_{\tau^*(t) \times \nu^*}[u_2 \mid A]])$$

$$U(\pi \otimes_\alpha \rho^*, \xi_0) = pu + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)\E_{t \sim \alpha_* \pi}[\E_{\rho^*(t) \times \nu^*}[u_2 \mid A]])$$

Combining, we get

$$pu + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)\E_{t \sim \alpha_* \pi}[\E_{\rho^*(t) \times \nu^*}[u_2 \mid A]]) \geq pu^* + (1-p) (\E_{\pi \times \nu^*}[u_1] + \nu^*(A)\E_{t \sim \alpha_* \pi}[\E_{\tau^*(t) \times \nu^*}[u_2 \mid A]])$$

$$u  \geq u^* + \frac{1-p}{p} \nu^*(A)\E_{t \sim \alpha_* \pi}[\E_{\tau^*(t) \times \nu^*}[u_2 \mid A]-\E_{\rho^*(t) \times \nu^*}[u_2 \mid A]]$$

$$u  \geq u^* + \frac{1-p}{p} \Gamma \nu^*(A)$$

\subsection{Definition A.3}

Denote ${x\Obs^*:=\{xy \mid y \in \Obs^*\}}$, $\bar{x}\Obs^*:=\Obs^* \setminus x\Obs^*$. Given any ${x \in \Obs^*}$, the *splitting of ${(\Pol,\ObsO,u)}$ at ${x}$* is defined as follows.

$$S_1 := \bar{x}\Obs^* \rightarrow \Act$$

$$S_2 := x\Obs^* \rightarrow \Act$$

$$A := x\ObsO$$

$$T := \Act^{\Abs{x}}$$

$$\alpha(s)_n:=s(x_{<n})$$

Given ${s: \bar{x}\Obs^* \rightarrow \Act}$ and ${e \in x\ObsO}$, define ${e^s \in (\Act \times \Obs)^{\Abs{x}}}$ by ${e_n^s:=(s(e_{<n}),e_n)}$. Given ${e \in \bar{x}\ObsO}$, define ${e^s \in (\Act \times \Obs)^\omega}$ by ${e_n^s:=(s(e_{<n}),e_n)}$. Define ${u_1}$ by

$$u_1(s,e):=\sum_{n = 0}^{\Abs{e^s}} \gamma(n) r(e^s_{<n})$$

Given ${t \in \Act^{\Abs{x}}}$, ${s: x\Obs^* \rightarrow \Act}$ and ${e \in x\ObsO}$, define ${e^{ts} \in (\Act \times \Obs)^\omega}$ by

$$e^{ts}_n:=\begin{cases}(t_n,e_n) \text{ if } n < \Abs{x}\\(s(e_{<n},e_n) \text{ if } n \geq \Abs{x}\end{cases}$$

Define ${u_2}$ by

$$u_2(s,t,e):=\begin{cases}\sum_{n = \Abs{x}+1}^{\infty} \gamma(n) r(e^{ts}_{<n}) \text{ if } x \sqsubset e\\0 \text{ otherwise}\end{cases}$$

It is easy to see that this is indeed a splitting.

\subsection{Proof of Theorem 1}

Fix ${\mu_0 \in \Phi}$. Let ${\nu^* \in \Phi'}$ be as in Proposition 2. Define ${M,N \subseteq \ObsO}$ by

$$M:=\{e \in \ObsO \mid \lim_{n \rightarrow \infty} \Dtv(\mu_0 \mid e_{<n}\ObsO, \nu^* \mid e_{<n}\ObsO) = 0\}$$

$$N:=\{e \in \ObsO \mid \lim_{n \rightarrow \infty} \frac{\nu^*(e_{<n}\ObsO)}{\mu_0(e_{<n}\ObsO)} = 0\}$$

By Corollary B, ${\mu_0(M \cup N) = 1}$.

Consider any ${e \in M}$. By definition of ${M}$, for any ${n \in \Nats}$ we have ${\mu_0(e_{<n}\ObsO) > 0}$ and ${\nu^*(e_{<n}\ObsO) > 0}$. Therefore, we can apply Proposition A.8 to the splitting of ${(\Pol,\ObsO,u)}$ at ${e_{<n}}$, with ${\pi = \pi^*}$. This gives us

$$U(\pi^*,\mu_0) \geq v(e_{<n})-2\Gamma_n \mu_0(e_{<n}\ObsO)\Dtv(\mu_0 \mid e_{<n}\ObsO, \nu^* \mid e_{<n}\ObsO)$$

Here, ${\Gamma_n \leq (\sup r - \inf r) \sum_{m > n} \gamma(n)}$. 

$$\frac{v(e_{<n}) - U(\pi^*,\mu_0)}{\Gamma_n \mu_0(e_{<n}\ObsO)} \leq 2\Dtv(\mu_0 \mid e_{<n}\ObsO, \nu^* \mid e_{<n}\ObsO)$$

By definition of ${M}$, this implies

$$\lim_{n \rightarrow \infty} \max(\frac{v(e_{<n}) - U(\pi^*,\mu_0)}{\Gamma_n \mu_0(e_{<n}\ObsO)},0) = 0 $$

Now, consider any ${e \in N}$. By definition of ${N}$, for any ${n \in N}$ we have ${\mu_0(e_{<n}\ObsO) > 0}$. Therefore, we can apply Proposition A.9 to the splitting of ${(\Pol,\ObsO,u)}$ at ${e_{<n}}$, with ${\pi = \pi^*}$. This gives us

$$U(\pi^*,\mu_0) \geq v(e_{<n})-\frac{1-p}{p} \Gamma_n \nu^*(e_{<n}\ObsO)$$

$$\frac{v(e_{<n}) - U(\pi^*,\mu_0)}{\Gamma_n\mu_0(e_{<n}\ObsO)} \leq \frac{1-p}{p} \frac{\nu^*(e_{<n}\ObsO)}{\mu_0(e_{<n}\ObsO)}$$

By definition of ${N}$, this also implies

$$\lim_{n \rightarrow \infty} \max(\frac{v(e_{<n}) - U(\pi^*,\mu_0)}{\Gamma_n \mu_0(e_{<n}\ObsO)},0) = 0 $$

\subsection{Proposition A.10}

In the setting of the Construction, consider any ${\mu \in \Phi_F}$. Then:

$$\forall x \in X_F \exists \mu_x \in F(x) \forall y \in A_x: \mu(xy\ObsO) = \mu(x\ObsO) \mu_x(y)$$

\subsection{Proof of Proposition A.10}

Given ${x \in X_F}$, we know there is ${\mu_x \in F(x)}$ s.t. the identity holds whenever ${\mu_x(y) > 0}$. Summing these identities over y, we get

$$\sum_{\mu_x(y) > 0} \mu(xy\ObsO) = \sum_{\mu_x(y) > 0} \mu(x\ObsO) \mu_x(y)$$

$$\sum_{\mu_x(y) > 0} \mu(xy\ObsO) = \mu(x\ObsO)$$

Since ${A_x}$ is prefix-free, for any ${y,y' \in A_x}$ we have ${xy\ObsO \cap xy'\ObsO = \varnothing}$. Also, obviously, ${xy\ObsO \subseteq x\ObsO}$. It follows that

$$\sum_{y \in A_x} \mu(xy\ObsO) = \mu(\bigcup_{y \in A_x} xy\ObsO) \leq \mu(x\ObsO) = \sum_{\mu_x(y) > 0} \mu(xy\ObsO)$$

Since ${\mu_x(y) > 0}$ implies ${y \in A_x}$, we conclude that for any ${y \in A_x}$, if ${\mu_x(y) = 0}$ then ${\mu(xy\ObsO)=0}$. This, together with the property of ${\mu_x}$, gives the desired result.

\subsection{Proposition A.11}

In the setting of the Construction, ${\Phi_F}$ is convex.

\subsection{Proof of Proposition A.11}

Consider ${\mu,\nu \in \Phi_F}$ and ${p \in (0,1)}$ and define ${\xi = p \mu + (1-p) \nu}$. Consider some ${x \in X_F}$, and let ${A_x \subseteq \Obs^+}$ be as in the Construction. If ${\xi(x\ObsO)=0}$, we can choose any ${\xi_x \in F(x)}$ to satisfy the desired condition. Therefore, we can assume ${\xi(x\ObsO) > 0}$. By Proposition A.10, there are ${\mu_x, \nu_x \in F(x)}$ s.t. for all ${y \in A_x}$:

$$\mu(xy\ObsO) = \mu(x\ObsO) \mu_x(y)$$

$$\nu(xy\ObsO) = \nu(x\ObsO) \nu_x(y)$$

Taking a convex linear combination of these equations, we get

$$\xi(xy\ObsO) = p \mu(x\ObsO) \mu_x(y) + (1-p) \nu(x\ObsO) \nu_x(y)$$

$$\xi(xy\ObsO) = \xi(x\ObsO) \frac{p \mu(x\ObsO) \mu_x(y) + (1-p) \nu(x\ObsO) \nu_x(y)}{\xi(x\ObsO)}$$

Define ${\xi_x \in \Prob(\Obs^+)}$ by 

$$\xi_x := p\frac{\mu(x\ObsO)}{\xi(x\ObsO)}\mu_x+(1-p)\frac{\nu(x\ObsO)}{\xi(x\ObsO)}\nu_x$$

${\xi_x}$ is a convex linear combination of ${\mu_x}$ and ${\nu_x}$, therefore ${\xi_x \in F(x)}$. We get

$$\xi(xy\ObsO) = \xi(x\ObsO) \xi_x(y)$$

Therefore, ${\xi \in \Phi_F}$.

\subsection{Proposition A.12}

In the setting of the Construction, ${\Phi_F}$ is closed.

\subsection{Proof of Proposition A.12}

Consider any sequence ${\{\mu_n \in \Phi_F\}}_{n \in \Nats}$ and s.t. ${\mu_n \rightarrow \mu}$ for some ${\mu \in \Prob(\ObsO)}$. Consider some ${x \in X_F}$, and let ${A_x \subseteq \Obs^+}$ be as in the Construction. By Proposition A.10, for any ${n \in \Nats}$, there is ${\mu_{nx} \in F(x)}$ s.t. for any ${y \in A_x}$:

$$\mu_n(xy\ObsO) = \mu_n(x\ObsO) \mu_{nx}(y)$$

Note that ${\Prob(\Obs^+)}$ is not compact but is still Polish (since ${\Obs^+}$ is such), and therefore ${F(x)}$ is compact and Polish. Let ${\{n_k \in \Nats\}_{k \in \Nats}}$ be an increasing sequence s.t. ${\mu_{n_kx} \rightarrow \mu_x}$ for some ${\mu_x \in F(x)}$. For any ${z \in \Obs^*}$, ${\chi_{z\ObsO}: \ObsO \rightarrow \{0,1\}}$ is a continuous function, and therefore ${\mu_{n}(z\Obs^*) \rightarrow \mu(z\Obs^*)}$. For any ${y \in \Obs^+}$, ${\chi_{\{y\}}: \Obs^+ \rightarrow \{0,1\}}$ is also continuous, and therefore ${\mu_{n_k x}(y) \rightarrow \mu_x(y)}$. Putting these together, we get

$$\mu(xy\ObsO) = \mu(x\ObsO) \mu_{x}(y)$$

Therefore ${\mu \in \Phi_F}$.

\subsection{Proposition A.13}

In the setting of the Construction, consider any ${x,z \in X_F}$, ${\mu \in F(z)}$ and ${y \in \Obs^+}$ s.t. ${\mu(y) > 0}$. Assume that ${x \sqsubset zy}$. Then, ${x \sqsubseteq z}$.

\subsection{Proof of Proposition A.13}

We prove by induction on ${\Abs{x}}$. If ${\Abs{x}=0}$, then obviously ${x \sqsubseteq z}$. If ${\Abs{x} > 0}$, then, by definition of ${X_F}$, there is ${x' \in X_F}$, ${y' \in \Obs^+}$ and ${\nu \in F(x')}$ s.t. ${\nu(y') > 0}$ and ${x = x'y'}$. ${x' \sqsubset x \sqsubset zy}$, therefore, by the induction hypothesis, ${x' \sqsubseteq z}$. Assume to the contrary that ${x \not\sqsubseteq z}$. Then, since ${x \sqsubset zy}$, ${z \sqsubset x = x'y'}$. By the induction hypothesis, this implies ${z \sqsubseteq x'}$. We got ${z = x'}$. It follows that ${zy' = x \sqsubset zy}$, and therefore ${y' \sqsubset y}$. However, ${y,y' \in A_z}$ and ${A_z}$ is prefix-free, which is a contradiction.

\subsection{Proposition A.14}

In the setting of the Construction, ${\Phi_F}$ is non-empty.

\subsection{Proof Proposition A.14}

Choose ${\mu_x \in F(x)}$ for each ${x \in X_F}$. Given any ${z \in \Obs^+}$, define ${p(z) \in \Obs^*}$ as the longest proper prefix of ${z}$ s.t. ${p(z) \in X_F}$. Define ${\theta: \Obs^* \rightarrow [0,1]}$ recursively by

$$\theta(\Estr_\Obs):=1$$

$$\forall z \in \Obs^+: \theta(z):=\theta(p(z)) \Prb_{y \sim \mu_{p(z)}}[z \sqsubseteq p(z)y]$$

Consider any ${z \not\in X_F}$. For any ${o \in \Obs}$, ${p(zo)=p(z)}$, therefore:

$$\theta(zo)=\theta(p(z))\Prb_{y \sim \mu_{p(z)}}[zo \sqsubseteq p(z)y]$$

$$\sum_{o \in \Obs} \theta(zo)=\theta(p(z)) \sum_{o \in \Obs} \Prb_{y \sim \mu_{p(z)}}[zo \sqsubseteq p(z)y]$$

$$\sum_{o \in \Obs} \theta(zo)=\theta(p(z)) \Prb_{y \sim \mu_{p(z)}}[z \sqsubset p(z)y]$$

For any ${y}$ s.t. ${\mu_{p(z)}(y) > 0}$, ${p(z)y \in X_F}$ and therefore ${z \ne p(z)y}$. It follows that ${z \sqsubset p(z)y}$ iff ${z \sqsubseteq p(z)y}$, and we get

$$\sum_{o \in \Obs} \theta(zo)=\theta(z)$$

Now, consider ${z \in X_F}$. For any ${o \in \Obs}$, ${p(zo)=z}$, therefore:

$$\theta(zo)=\theta(z)\Prb_{y \sim \mu_z}[zo \sqsubseteq zy]$$

$$\sum_{o \in \Obs} \theta(zo)=\theta(z) \sum_{o \in \Obs} \Prb_{y \sim \mu_z}[zo \sqsubseteq zy]$$

Again we get 

$$\sum_{o \in \Obs} \theta(zo)=\theta(z)$$

It follows that there is ${\mu \in \Prob(\ObsO)}$ s.t. for any ${z \in \Obs^*}$, ${\mu(z\ObsO)=\theta(z)}$.

Consider any ${x \in X_F}$ and ${y_0 \in \Obs^+}$ s.t. ${\mu_x(y_0) > 0}$. ${p(xy_0) \sqsubset xy_0}$ and by Proposition\ A.13, this implies ${p(xy_0) \sqsubseteq x}$. By definition of ${p}$, it follows that ${p(xy_0)=x}$. We get

$$\theta(xy_0)=\theta(p(xy_0)) \Prb_{y \sim \mu_{p(xy_0)}}[xy_0 \sqsubseteq p(xy_0)y]$$

$$\theta(xy_0)=\theta(x) \Prb_{y \sim \mu_x}[xy_0 \sqsubseteq xy]$$

$$\theta(xy_0)=\theta(x) \Prb_{y \sim \mu_x}[y_0 \sqsubseteq y]$$

${y_0 \in A_x}$ and any ${y \in \Obs^+}$ s.t. ${\mu_x(y) > 0}$ is also in ${A_x}$. ${A_x}$ is prefix-free, so ${y_0 \sqsubseteq y}$ iff ${y_0 = y}$. We get

$$\theta(xy_0)=\theta(x) \mu_x(y_0)$$

$$\mu(xy_0 \ObsO)=\mu(x\ObsO) \mu_x(y_0)$$

Therefore, ${\mu \in \Phi_F}$.

\subsection{Proposition A.15}

Consider ${x \in \Obs^*}$ and let ${f^x: \ObsO \times \ObsO \rightarrow \ObsO}$ be defined as in Definition 1. Then, for any ${y \in \Obs^*}$:

$${(f^x)^{-1}(xy\ObsO) = x\ObsO \times y\ObsO}$$

\subsection{Proof of Proposition A.15}

Given any ${e_1,e_2 \in \ObsO}$, ${f^x(xe_1,ye_2)=xye_2}$. On the other hand, if ${e_1,e_2,e_3 \in \ObsO}$ are s.t. ${f^x(e_1,e_2)=xye_3}$ then ${x \sqsubset e_1}$ and ${e_2=ye_3}$.

\subsection{Proposition A.16}

Consider ${x \in \Obs^*}$ and let ${f^x: \ObsO \times \ObsO \rightarrow \ObsO}$ be defined as in Definition 1. Then, for any ${z \in \Obs^*}$, if ${x \not\sqsubset z}$, then:

$${(f^x)^{-1}(z\ObsO) = z\ObsO \times \ObsO}$$

\subsection{Proof Proposition A.16}

Assume ${x = zw}$ for some ${w \in \Obs^*}$. For any ${e_1,e_2 \in \ObsO}$, ${f^x(ze_1,e_2)}$ is either ${ze_1}$ (if $w \not\sqsubset e_1$) or ${xe_2=zwe_2}$ (if ${w \sqsubset e_1}$). Either way, ${z \sqsubset f^x(ze_1,e_2)}$. On the other hand, if ${e_1,e_2,e_3 \in \ObsO}$ are s.t. ${f^x(e_1,e_2)=ze_3}$ then either ${x \not\sqsubset e_1}$ and ${e_1 = ze_3}$ or ${zw = x \sqsubset e_1}$. Either way, ${z \sqsubset e_1}$.

Now, assume ${x \not\sqsupseteq z}$. For any ${e_1,e_2 \in \ObsO}$, ${f^x(ze_1,e_2)=ze_1}$ (since ${x \not\sqsubset ze_1}$). On the other hand, if ${e_1,e_2,e_3 \in \ObsO}$ are s.t. ${f^x(e_1,e_2)=ze_3}$ then ${e_1 = ze_3}$ (since ${x \not\sqsubset ze_3}$).

\subsection{Proposition A.17}

In the setting of the Construction, consider some ${x \in X_F}$ and define ${G: \Obs^* \rightarrow \CC(\Obs^+)}$ by ${G(z):=F(xz)}$. Assume that ${w \in \Obs^*}$ is s.t. ${xw \in X_F}$. Then, ${w \in X_G}$.

\subsection{Proof of Proposition A.17}

We prove by induction on ${\Abs{w}}$. For ${\Abs{w}=0}$, the proposition is obvious. Assume ${\Abs{w} > 0}$. ${xw \in X_F}$, therefore ${xw=x'y}$ for some ${x' \in X_F}$, ${\mu \in F(x')}$ and ${y \in \Obs^+}$ s.t. ${\mu(y)\ > 0}$. ${x \sqsubset x'y}$, therefore, by Proposition A.13, ${x \sqsubseteq x'}$. Let ${z \in \Obs^*}$ be s.t. ${x' = xz}$. ${xw=x'y=xzy}$, therefore ${w=zy}$ and we can use the induction hypothesis to show that ${z \in X_G}$. ${\mu \in F(x') = F(xz)=G(z)}$, implying that ${w \in X_G}$.

\subsection{Proposition A.18}

In the setting of the Construction, consider some ${x \in X_F}$ and define ${G: \Obs^* \rightarrow \CC(\Obs^+)}$ by ${G(z):=F(xz)}$. Consider any ${w \in X_G}$. Then, ${xw \in X_F}$.

\subsection{Proof of Proposition A.18}

We prove by induction on ${\Abs{w}}$. For ${\Abs{w} = 0}$, the proposition is obvious. Assume that ${\Abs{w} > 0}$. Then, there is some ${w' \in X_G}$, ${\mu \in G(w')=F(xw')}$ and ${y \in \Obs^+}$ s.t. ${\mu(y) > 0}$ and ${w = w'y}$. By the induction hypothesis, ${xw' \in X_F}$. Therefore, ${xw=xw'y}$ is also in ${X_F}$.

\subsection{Proposition A.19}

In the setting of the Construction, ${\Phi_F}$ factorizes at ${x}$ for any ${x \in X_F}$.

\subsection{Proof of Proposition A.19}

Fix ${x \in X_F}$. Let ${f^x}$ be as in Definition 1. Define ${G: \Obs^* \rightarrow \CC(\Obs^+)}$ by ${G(z):=F(xz)}$. We claim that ${\Phi_F = f^x_*(\Phi_F \times \Phi_G)}$.

Consider any ${\mu \in \Phi_F}$, ${\nu \in \Phi_G}$ and denote ${\xi:=f^x_*(\mu \times \nu)}$. Consider some ${z \in X_F}$ and ${A_z \subseteq \Obs^+}$ corresponding.

Assume ${z = xw}$ for some ${w \in \Obs^*}$. By Proposition A.17, ${w \in X_G}$. Note that for any ${\nu_0 \in G(w)=F(z)}$, ${\nu_0(y) > 0}$ implies ${y \in A_z}$. Let ${\nu_w \in \Prob(\Obs^+)}$ be as in Proposition A.10. For any ${y \in A_z}$, we have

$$\nu(wy\ObsO) = \nu(w\ObsO) \nu_w(y)$$

Multiplying both sides by ${\mu(x\ObsO)}$:

$$\mu(x\ObsO) \nu(wy\ObsO) = \mu(x\ObsO) \nu(w\ObsO) \nu_w(y)$$

Using the definition of ${\xi}$ and Proposition A.15, we get

$${\xi(z\ObsO)=\mu(x\ObsO) \nu(w\ObsO)}$$

$$\xi(zy\ObsO) = \mu(x\ObsO) \nu(wy\ObsO)$$

Combining, we get

$$\xi(zy\ObsO) = \xi(z\ObsO) \nu_w(y)$$

Now, assume ${x \not\sqsubseteq z}$. Let ${\mu_z \in \Prob(\Obs^+)}$ be as in Proposition A.10. For any ${y \in \Obs^+}$ s.t. ${\mu_z(y) > 0}$, we have

$$\mu(zy\ObsO)=\mu(z\ObsO) \mu_z(y)$$

Using the definition of ${\xi}$ and Proposition A.16, we get

$$\xi(z\ObsO)=\mu(z\ObsO)$$

By Proposition A.13, ${x \not\sqsubset zy}$. Therefore, we can use Proposition A.16 again to conclude

$$\xi(zy\ObsO)=\mu(zy\ObsO)$$

Combining, we get

$$\xi(zy\ObsO) = \xi(z\ObsO) \mu_z(y)$$

We proved that ${\xi \in \Phi_F}$. It remains to show that, conversely, ${\mu \in f_*^x(\Phi_F \times \Phi_G)}$. 

Define ${\mu^x \in \Prob(\ObsO)}$ as follows. If ${\mu(x\ObsO)=0}$, choose arbitrary ${\mu^x \in \Phi_G}$. If ${\mu(x\ObsO) > 0}$, define ${\mu^x}$ by 

$${\mu^x(z\ObsO):=\frac{\mu(xz\ObsO)}{\mu(x\ObsO)}}$$

Consider any ${w \in X_G}$. By Proposition A.18, ${xw \in X_F}$. By Proposition A.10, there is ${\mu_{xw} \in F(xw)}$ s.t. for any ${y \in A_{xw}}$:

$$\mu(xwy\ObsO)=\mu(xw\ObsO)\mu_{xw}(y)$$

If ${\mu(x\ObsO) > 0}$, we can divide both sides by $\mu(x\ObsO)$ and get

$$\mu^x(wy\ObsO)=\mu^x(w\ObsO)\mu_{xw}(y)$$

It follows that, in either case, ${\mu^x \in \Phi_G}$ and 

$$\mu(x\ObsO)\mu^x(z\ObsO)=\mu(xz\ObsO)$$

Denote ${\xi := f^x_*(\mu \times \mu^x)}$. Consider any ${z \in \Obs^*}$.

Assume ${z = xw}$ for some ${w \in \Obs^*}$. By Proposition A.15

$$\xi(z\ObsO) = \mu(x\ObsO)\mu^x(w\ObsO)=\mu(z\ObsO)$$

Now, assume ${x \not\sqsubseteq z}$. By Proposition A.16

$$\xi(z\ObsO) = \mu(z\ObsO)$$

We conclude that ${\mu = \xi}$.

\subsection{Proposition A.20}

In the setting of the construction, consider any ${\mu \in \Phi_F}$ and ${e \in \ObsO}$ s.t. for any ${n \in \Nats}$, ${\mu(e_{<n}\ObsO) > 0}$. Then, there are infinitely many ${x \in X_F}$ s.t. ${x \sqsubset e}$.

\subsection{Proof of Proposition A.20}

We prove by induction on ${n}$ that there are at least ${n}$ prefixes of ${e}$ in ${X_F}$. For ${n=0}$ the claim is vacuously true. Consider any ${n > 0}$. By the induction hypothesis, there are ${x_0 \sqsubset x_1 \sqsubset \ldots \sqsubset x_{n-1} \sqsubset e}$ s.t. ${x_k \in X_F}$ for all ${k < n}$. Without loss of generality, we can assume ${A_{x_{n-1}}}$ is s.t.

$$\bigcup_{y \in A_{x_{n-1}}} y\ObsO = \ObsO$$

Indeed, if this condition is false we can make it true by adding more elements to ${A_{x_{n-1}}}$. By Proposition A.10, there is ${\nu \in F(x_{n-1})}$ s.t. for any ${y \in A_{x_{n-1}}}$

$$\mu(x_{n-1}y\ObsO)=\mu(x_{n-1}\ObsO) \nu(y)$$

By the assumption on ${A_{x_{n-1}}}$, there is ${y_0 \in A_{x_{n-1}}}$ s.t. ${x_{n-1}y_0 \sqsubset e}$. Denote ${x_n:=x_{n-1}y_0}$. We get

$$\mu(x_{n}\ObsO)=\mu(x_{n-1}\ObsO) \nu(y_0)$$

The left hand side is positive since ${x_n \sqsubset e}$, therefore ${\nu(y_0) > 0}$.\ It follows that ${x_n \in X_F}$.

\subsection{Proof of Proposition 6}

By Propositions A.11, A.12 and A.14, ${\Phi_F \in \CC(\ObsO)}$. It remains to show that ${\Phi_F}$ factorizes everywhere.

Consider any ${\mu \in \Phi_F}$.

TBD

\subsection{Proof of Corollary 1}

TBD

\section{Appendix B}

The following theorem is adapted from [Blackwell and Dubbins (1962)](https://projecteuclid.org/euclid.aoms/1177704456).

\subsection{Theorem B.1}

Consider any ${\mu, \nu \in \Prob(\ObsO)}$. Assume that ${\mu}$ is absolutely continuous with respect to ${\nu}$. Then:

$$\Prb_{e \sim \mu}[\lim_{n \rightarrow \infty} \Dtv(\mu \mid e_{<n}\ObsO, \nu \mid e_{<n}\ObsO) = 0] = 1$$

\subsection{Theorem B.2}

Consider any ${\mu, \nu \in \Prob(\ObsO)}$. Define ${\{D_n: \ObsO \rightarrow \Reals \sqcup \{+\infty\}\}_{n \in \Nats}}$ by

$$D_n(x):=\begin{cases}\frac{\mu(x_{<n}\ObsO)}{\nu(x_{<n}\ObsO)} \text{ if } \nu(x_{<n}\ObsO) > 0\\+\infty \text{ otherwise}\end{cases}$$

Define ${D:= \limsup_{n \rightarrow \infty} D_n}$. Then, for any Borel ${A \subseteq \ObsO}$:

$$\mu(A) = \int_A D(x) d\nu(x) + \mu(A \cap D^{-1}(+\infty))$$

\subsection{Proof of Theorem B.2}

This is almost a special case of Theorem 5.3.3 in [Durret (2010)](https://books.google.ie/books/about/Probability.html?id=evbGTPhuvSoC), except that we don't assume ${\mu}$ is locally absolutely continuous w.r.t. ${\nu}$ (i.e. we don't assume that ${\mu(x\ObsO)=0}$ whenever ${\nu(x\ObsO)=0}$). Careful reading of the proof shows that in our case it doesn't matter: ${D_n}$ (${X_n}$ is Durret's notation) is no longer a ${\nu}$-martingale but is still a ${\nu}$-supermartingale, and the identity ${X_n = \frac{Y_n}{Z_n}}$ still holds if the fraction is understood to stand for ${+\infty}$ whenever ${Z_n = 0}$.

\subsection{Corollary B}

Consider any ${\mu, \nu \in \Prob(\ObsO)}$. Define

$$A:=\{e \in \ObsO \mid \lim_{n \rightarrow \infty} \Dtv(\mu \mid e_{<n}\ObsO, \nu \mid e_{<n}\ObsO) = 0\}$$

$$B:=\{e \in \ObsO \mid \lim_{n \rightarrow \infty} \frac{\nu(e_{<n}\ObsO)}{\mu(e_{<n}\ObsO)} = 0\}$$

Then, ${\mu(A \cup B) = 1}$.

\subsection{Proof of Corollary B}

Assume that ${\E_\nu[D] > 0}$. Then, we can define ${\mu_r \in \Prob(\ObsO)}$ by

$$\mu_r(C):=\frac{\int_{C} D(x) d\nu(x)}{\int_{\ObsO} D(x) d\nu(x)}$$

Obviously, ${\mu_r}$ is absolutely continuous w.r.t. ${\nu}$, therefore we can use Theorem B.1 to conclude

$$\Prb_{e \sim \mu_r}[\lim_{n \rightarrow \infty} \Dtv(\mu_r \mid e_{<n}\ObsO, \nu \mid e_{<n}\ObsO) = 0] = 1$$

By Theorem B.2, ${\mu_r}$ is dominated by ${\mu}$ and in particular is absolutely continuous w.r.t. ${\mu}$. Therefore, again by Theorem B.1:

$$\Prb_{e \sim \mu_r}[\lim_{n \rightarrow \infty} \Dtv(\mu_r \mid e_{<n}\ObsO, \mu \mid e_{<n}\ObsO) = 0] = 1$$

Using the triangle inequality for ${\Dtv}$, we get

$$\Prb_{e \sim \mu_r}[\lim_{n \rightarrow \infty} \Dtv(\mu \mid e_{<n}\ObsO, \nu \mid e_{<n}\ObsO) = 0] = 1$$

By definition of ${\mu_r}$ and ${A}$, this means that

$$\frac{\int_{A} D(x) d\nu(x)}{\int_{\ObsO} D(x) d\nu(x)} = 1$$

$$\int_{A} D(x) d\nu(x) = \int_{\ObsO} D(x) d\nu(x)$$

This holds even without the assumption ${\E_\nu[D] > 0}$, since if ${\E_\nu[D] = 0}$ then both sides vanish.

${\frac{\nu(e_{<n}\ObsO)}{\mu(e_{<n}\ObsO)}}$ is ${\mu}$-supermatingale, therefore (by Doob's convergence theorem) its limit exists ${\mu}$-almost surely. If the limit exists for some ${e \in \ObsO}$, then it vanishes iff ${D(e)=+\infty}$. It follows that

$$\mu(B \cap D^{-1}(+\infty)) = \mu(D^{-1}(+\infty))$$

By Theorem B.2

$$\mu(A \cup B) = \int_{A \cup B} D(x) d\nu(x) + \mu((A \cup B) \cap D^{-1}(+\infty))$$

$$\mu(A \cup B) \geq \int_{A} D(x) d\nu(x) + \mu(B \cap D^{-1}(+\infty))$$

$$\mu(A \cup B) \geq \int_{\ObsO} D(x) d\nu(x) + \mu(D^{-1}(+\infty))$$

Applying Theorem B.2 to the right hand side:

$$\mu(A \cup B) \geq \mu(\ObsO) = 1$$

\end{document}



