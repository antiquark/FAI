%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}
\usepackage{bm}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}
\newcommand{\WordsLen}[1]{{\Bool^{#1}}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

\newcommand{\FOO}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Ev}{ev}

% special symbols that are not really operators
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\R}{r}
\DeclareMathOperator{\A}{a}
\DeclareMathOperator{\M}{M}
\DeclareMathOperator{\UM}{UM}
\DeclareMathOperator{\Un}{U}
\DeclareMathOperator{\En}{c}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\textnormal{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\NatPoly}{\Nats[K_0, K_1 \ldots K_{n-1}]}
\newcommand{\NatPolyJ}{\Nats[J_0, J_1 \ldots J_{n-2}]}
\newcommand{\NatFun}{\Nats^n \rightarrow}

\newcommand{\Estr}{\bm{\lambda}}
\newcommand{\LLU}{\mathbf{LLU}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Dist}{\mathcal{D}}
\newcommand{\GrowR}{\Gamma_{\mathfrak{R}}}
\newcommand{\GrowA}{\Gamma_{\mathfrak{A}}}
\newcommand{\Grow}{\Gamma:=(\GrowR,\GrowA)}
\newcommand{\MGrow}{\mathrm{M}\Gamma}
\newcommand{\Fall}{\mathcal{F}}
\newcommand{\EG}{\Fall(\Gamma)}
\newcommand{\ESG}{\Fall^\sharp(\Gamma)}
\newcommand{\EMG}{\Fall(\MGrow)}
\newcommand{\ESMG}{\Fall^\sharp(\MGrow)}
\newcommand{\BoolR}[1]{\Bool^{\R_{#1}(K)}}

\newcommand{\GammaPoly}{\Gamma_{\textnormal{poly}}}
\newcommand{\GammaLog}{\Gamma_{\textnormal{log}}}
\newcommand{\FallU}{{\Fall_{\textnormal{uni}}^{(n)}}}
\newcommand{\FallUt}[1]{{\Fall_{\textnormal{uni}}^{(#1)}}}
\newcommand{\FallM}{\Fall_{\textnormal{mon}}^{(n)}}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}
\newcommand{\Scheme}{\xrightarrow{\Gamma}}
\newcommand{\MScheme}{\xrightarrow{\MGrow}}
\newcommand{\ORC}{\xrightarrow{\textnormal{orc}}}

\newcommand{\Base}{\mathcal{B}}
\newcommand{\Prob}{\mathcal{P}}

\begin{document}

*The notation of this post is based on VK16 rather than the notation of previous posts. The concept of "quasi-optimal estimator" introduced here is completely unrelated to the previously introduced "quasi-optimal predictors". The latter term should be considered obsolete (it can be expressed different in the language of VK16).*

Inspired by (presently unpublished) work by Garrabrant, we introduce a generalization of the concept of optimal polynomial-time estimators which we named "quasi-optimal polynomial-time estimators." As agnostic PAC learning (with a quadratic loss function) gives rise to optimal estimators, online convex optimisation gives rise to quasi-optimal estimators. At the same time, most properties of optimal estimators carry over to quasi-optimal estimators.

%##Results
\section{Results}

%#Definition 1
\subsection{Definition 1}

An *ordered ring circuit* is a circuit with 3 types of binary gates: ${+}$, ${\times}$ and ${\max}$. Given ${A,B}$ finite sets, ${\Rats^A \ORC \Rats^B}$ is the set of ordered ring circuits with $\Abs{A}$ normal inputs labeled by ${A}$, ${\Abs{B}}$ outputs labeled by ${B}$ and a finite set of auxiliary inputs that receive constants in ${\Rats}$ (we can think of them as 0-nary gates). The notation ${\phi: \Rats^A \ORC \Rats^B}$ means ${\phi \in (\Rats^A \ORC \Rats^B)}$. Any ${\phi: \Rats^A \ORC \Rats^B}$ can be regarded as a function from ${\Rats^A}$ to ${\Rats^B}$ in the obvious way (circuit evaluation). 

Given ${\phi: \Rats^A \ORC \Rats^B}$ with ${N}$ gates and ${M}$ auxiliary inputs ${\{c_i \in \Rats\}_{i \in [M]}}$, the *norm* of ${\phi}$ is ${\Norm{\phi}:=N+\sum_{i \in [M]} \Abs{c_i}}$.

***

For the rest of the post, we fix ${n \in \Nats^{>0}}$ and ${\Gamma}$ a pair of growth space of rank ${n}$. We will use a slightly different (but equivalent) definition of a distributional estimation problem than in VK16.

\subsection{Definition 2}

An *encoded ensemble of rank ${n}$* is a pair ${(X,\Dist)}$ where ${X}$ is an encoded set and ${\Dist=\{\Dist^K \in \Prob(X)\}_{K \in \Nats^n}}$.

\subsection{Definition 3}

A *distributional estimation problem of rank ${n}$* is a triple ${(X,\Dist,f)}$ where ${(X,\Dist)}$ is an encoded ensemble of rank ${n}$ and ${f: \Supp \Dist \rightarrow \Reals}$ is a bounded function.

\subsection{Definition 4}

Consider a finite set ${\Base}$, ${\{(X,\Dist_i,f_i)\}_{i \in \Base}}$ a collection of distributional estimation problems and ${\{\Fall_i\}_{i \in \Base}}$ a collection of fall spaces. ${\{P_i: X \Scheme \Rats\}_{i \in \Base}}$ is called a *family of ${\EG}$-quasi-optimal polynomial-time estimators for ${(X,\Dist,f)}$* when for any ${Q: X \Scheme (\Rats^{\Base} \ORC \Rats^{\Base})}$ s.t. ${\Norm{Q^K(x,y)}}$ is bounded by a constant, there are ${p \in \NatPoly}$ and ${\{\varepsilon_i \in \Fall_i\}_{i \in \Base}}$ s.t. for all ${i \in \Base}$, ${J \in \Nats^{n-1}}$, ${k \in \Nats}$ and ${l \geq p(J,k)}$

$$\EE{x \sim \Dist^{Jk}}{y \sim \Un_P^{Jl}}[(P_i^{Jl}(x,y)-f_i(x))^2] \leq \EEE{x \sim \Dist^{Jk}}{y \sim \Un_P^{Jl}}{z \sim \Un_Q^{Jk}}[(Q^{Jk}(x,z;P^{Jl}(x,y))_i-f_i(x))^2]+\varepsilon_i(J,l)$$

Here, the notation ${Q^{Jk}(x,z;P^{Jl}(x,y))_i}$ means that the circuit ${Q^{Jk}(x,z)}$ is evaluated on ${\{P^{Jl}(x,y)_j\}_{j \in \Base}}$ and component ${i}$ of the result is used.

\subsection{Definition 5}

Consider a finite set ${\Base}$, ${\{(\Dist_i,f_i)\}_{i \in \Base}}$ a collection of distributional estimation problems and ${\{\Fall_i\}_{i \in \Base}}$ a collection of fall spaces. ${\{P_i: X \Scheme \Rats\}_{i \in \Base}}$ is called a *family of ${\ESG}$-quasi-optimal polynomial-time estimators for ${(\Dist,f)}$* when for any ${S: X \Scheme (\Rats^{\Base} \ORC \Rats^{\Base})}$ s.t. ${\Norm{S^K(x,y)}}$ is bounded by a constant, there are ${p \in \NatPoly}$ and ${\{\varepsilon_i \in \Fall_i\}_{i \in \Base}}$ s.t. for all ${i \in \Base}$, ${J \in \Nats^{n-1}}$, ${k \in \Nats}$ and ${l \geq p(J,k)}$

$$\Abs{\EEE{x \sim \Dist^{Jk}}{y \sim \Un_P^{Jl}}{z \sim \Un_P^{Jk}}[(P_i^{Jl}(x,y)-f_i(x))S^{Jk}(x,z;P^{Jl}(x,y))]} \leq \varepsilon_i(J,l)$$

***

Quasi-optimal estimators have properties similar to optimal estimators and in Appendix A we give several analogous theorems. On the other hand they seem to exist in more cases. Theorem 5.2 in VK16 shows that samplable distributional estimation problems have uniform optimal estimators based on the principle of agnostic PAC learning. The following theorem shows a class of distributional estimation problem families that allow uniform quasi-optimal estimators based on the principle of online convex optimization. Note that this new class of problems, as given here, does *not* include all samplable problems as a special case. We belief it is possible to construct a class which includes both these problems and samplable problems as special cases s.t. all families in the class have quasi-optimal estimators, but we do not attempt it at present.

%#Theorem
\subsection{Theorem}

Assume ${n=2}$. Define ${\zeta: \Nats^2 \rightarrow \Reals}$ by ${\zeta(j,k):=\min(\sqrt{\frac{k}{j+1}},1)}$.
% X = sequences of ...? For each j, we can have a different distribution on sequences of length ${j}$ which we then transform by truncating the sequence of a uniformly random length...

FOO

%##Appendix
\section{Appendix A}

Foo

%##Appendix
\section{Appendix B}

Foo

%##Appendix
\section{Appendix C}

Foo

\end{document}


