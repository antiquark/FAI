%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}
\usepackage{bm}

% Generic TCS commands

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}
\newcommand{\Sagas}{{\Bool^\omega}}
\newcommand{\Estr}{{\bm{\lambda}}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\newcommand{\PP}[2]{\operatorname{Pr}_{\substack{#1 \\ #2}}}
\newcommand{\PPP}[3]{\operatorname{Pr}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

\newcommand{\FOO}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Ev}{ev}

% special symbols that are not really operators
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\R}{r}
\DeclareMathOperator{\A}{a}
\DeclareMathOperator{\Un}{U}
\DeclareMathOperator{\En}{c}
\DeclareMathOperator{\Prj}{pr}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\textnormal{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\NatPoly}{\Nats[K_0, K_1 \ldots K_{n-1}]}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Dist}{\mathcal{D}}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}

\newcommand{\UTM}{\mathcal{U}}

\newcommand{\SP}[1]{\Delta #1}

% Paper specific commands

\newcommand{\Act}{\mathcal{A}}
\newcommand{\Per}{\mathcal{O}}
\newcommand{\His}{(\Per \times \Act)^*}
\newcommand{\Env}{\mathcal{E}}
\newcommand{\Beh}{\Delta_{\operatorname{b}}}
\newcommand{\EnvB}{\Beh\Env}
\newcommand{\Prog}{\operatorname{H}^\UTM}
\newcommand{\ProgS}{\bar{\operatorname{H}}^\UTM}
\newcommand{\Strat}{\mathcal{S}}

\begin{document}

Motivated by the grain of truth problem, we propose a generalization of Bayesian inference that allows for *incomplete* models. Such a model can be thought of as a set of constraints on the environment which doesn't specify it completely. This means that in addition to ordinary, probabilistic, uncertainty, another ("Knightian") type of uncertainty is introduced. This new uncertainty is managed using Savage's minimax regret decision rule. We construct analogues of AIXI and its bounded counterparts corresponding to this epistemology and speculate about behavior in multi-agent scenarios.

%##Results
\section{Motivation}

The *grain of truth* problem, as described by [Hutter](https://arxiv.org/abs/0907.0746) (see problem 5j) concerns the interaction between several AIXI agents. Ideally, we expect that rational agents in multi-agent scenarios should converge to behavior corresponding to some reasonable solution concept in game theory, e.g. Nash equilibrium (for the purposes of this post I completely ignore stronger desiderata such as superrationality; also purely Bayesian agents often fail to do so because of insufficient exploration but this is also a problem mostly orthogonal to grain of truth). However, it is difficult to prove anything about multi-agent scenarios of AIXI since AIXI's prior is supported on computable environments whereas AIXI itself is uncomputable. This problem survives in bounded analogues: if we limit the computational complexity of environments included in the prior, the computational complexity of the agent itself will invariable be higher. Thus, two agents of equal power seem unable to "comprehend" each other.

There are several reasons this problems seem important to AI alignment:

1. The ability of an agent to operate in environments of complexity similar to that of the agent or even surpassing that of the agent seems an important desideratum for the mathematical formalization of intelligence. Without a formal understanding how this desideratum can be satisfied it is difficult to imagine any reliable theory of intelligent agents.

2. Arguably, self-improving agents need the capacity to understand systems that equal themselves in complexity, more or less by definition.

3. Perhaps most importantly, IRL relies on the AI's ability to successfully reason about a complex agent (a human), presumably an agent that cannot be emulated precisely with the AI's computational power. CIRL relies on two-sided interaction between the AI and the human, where neither side can emulate the other. Thus, it seems impossible to prove reasonable guarantees for (C)IRL under quasi-realistic assumptions without solving the grain of truth problem.

To the best of our knowledge, the most serious attack on the problem to date is the work of [Leike, Taylor and Fallenstein](https://arxiv.org/abs/1609.05058). They prove that Thompson samplers relatively to a reflective oracle converge to an "asymptotic" Nash equilibrium. In our opinion, it should be possible to extent that work to ensure subgame perfect and even proper equilibria and, under some assumptions, allow for "tamer" forms of exploration than Thompson sampling (Thompson sampling necessarily sacrifices many horizons until convergence). There is no doubt reflective oracles constitute significant progress towards the solution, in the sense that they clearly demonstrate the grain of truth problem is not inherently unsolvable and provide a formal framework where agents satisfying this desideratum can be studied. Nevertheless, we retain skepticism regarding the prospect of reaching the ultimate solution through developing that approach.

In our opinion, there are 3 ways in which reflective oracles are an unsatisfactory solution:

1. There are many different reflective oracles. In multi-agent scenarios, all results rely on all agents using the same reflective oracle. Realistic agents cannot use uncomputable oracles so they will have to replace them by some sort of bounded analogues. However, there is every reason to suspect these bounded analogues will also be far from unique. Thus, it's not clear that interaction between agents using different brands of "bounded reflective oracles" will have any substantial guarantees. In fact, this lack of uniqueness seems directly related to the lack of uniqueness for Nash equilibria. Any solution for the grain of truth will have to solve equilibrium selection, but reflective oracles seem to "cheat" by implicitly selecting Nash equilibria for all possible games in advance. A more realistic model should imply some sort of negotiation process which will probably need some non-trivial assumptions to converge.

2. Multi-agent scenarios are a test case for the ability to reason about environments more complex than the agent itself, but the latter is more general than the former. We expect reasonable multi-agent behavior to arise as a special cases of some broad capability, useful in a great variety of other scenarios. However, it is not clear what physically realistic environments *except* multi-agent scenarios benefit from reasoning with reflective oracles. If there are few such environments, we would have to admit that either reflective oracles are an unsatisfactory model *or* that realistic agents need to be designed qualitatively differently in order to manage multi-agent scenarios as opposed to all other complex scenarios. In other words, in the latter case there would be uncontrived examples of AIs that solve almost any problem better than humans (including proving theorems, discovering physics, inventing nanotechnology, von Neumann probes and Dyson spheres) but that fail miserably in games. This possibility, although not impossible, appears to us less likely than the alternative.

3. Reflective oracles are uncomputable, although they were proven to be limit computable. Obviously realistic agents use algorithms that are not only computable but satisfy rather stringent complexity bounds.

Of the problems above, 3 seems to be the least significant since it is plausible that bounded analogues can be constructed. In any case, the agents we construct below are also uncomputable (constructing bounded counterparts seems to be straightforward, but they are still computationally infeasible). Problem 1 might be solved if a natural class of reflective oracles is found s.t. any agent that is computable w.r.t. any oracle ${O_1}$ in this class is also computable w.r.t. any other oracle ${O_2}$ in this class, at least approximately. This is not impossible but also not straightforward. Problem 2 might be the hardest.

\section{Incomplete Models}

Although an agent cannot simulate another agent of equal power without entering an infinite loop, it might be able to observe certain facts about the other agent's behavior. This calls for a notion of an incomplete model: a model of the environment that doesn't yield precise probabilistic predictions but provides some constraints on what might happen. 

To give a trivial example, given a sequence of two bits $b_1b_2$, the model might say that $b_1=\lnot b_2$ without assigning any probability to $b_1=0$. Knowing only this model one has unquantifiable (Knightian) uncertainty between the possibilities $01$ and $10$. The space of probability distributions on two bit sequences is a tetrahedron, and our model corresponds to one edge of this tetrahedron.

More generally, we have some measurable event space ${X}$ and ${\SP{X}}$ is the space of probability measures on ${X}$. A complete (Bayesian) model corresponds to some ${\mu \in \SP{X}}$. An incomplete model corresponds to some convex set ${\Phi \subseteq \SP{X}}$ (which can be assumed closed in the strong convergence topology). If we assign probability ${p}$ to model ${\mu}$ and ${1-p}$ to model ${\nu}$, this can be represented by the model ${p \mu + (1-p) \nu}$. Similarly, if we assign probability ${p}$ to model ${\Phi}$ and probability ${1-p}$ to model ${\Psi}$, this can be represented by the model 

$${p\Phi+(1-p)\Psi:=\{p \mu + (1-p) \nu \mid \mu \in \Phi, \nu \in \Psi\}}$$

In ordinary Bayesian statistics, a prior is often specified by a some countable space ${H}$ of hypotheses (i.e. for each ${x \in H}$ we have ${\mu_x \in \SP{X}}$; more generally ${H}$ can be some measurable space with a Markov kernel into ${X}$, but we won't need that) and a probability measure ${\zeta}$ on ${H}$, so that the prior is ${\E_{x \in \zeta}[\mu_x]}$. Similarly, we can have ${\Phi_x \subseteq \SP{X}}$ and the incomplete prior

$${\E_{x \in \zeta}[\Phi_x]:=\{\E_{h \in \zeta}[\mu_x] \mid \mu: H \rightarrow \SP{X}, \forall x \in H: \mu_x \in \Phi_x\}}$$

Now consider an agent interacting with an unknown environment. The agent has a finite set ${\Act}$ of actions and a finite set ${\Per}$ of percepts. The event space is the space of "pure" (deterministic) environments ${\Env:=\His \rightarrow \Per}$ (its ${\sigma}$-algebra comes from viewing it as the inverse limit of the sequence ${(\Per \times \Act)^{\leq n} \rightarrow \Per}$). In this context we don't usually work with the full space of "mixed" environments ${\SP{\Env}}$ but only with the quotient space of "behavioral" environments  ${\EnvB}$. Here, every ${\mu \in \EnvB}$ is a partial function from ${\His}$ to ${\SP{\Per}}$ s.t. ${\mu(h)}$ is defined iff ${\mu}$ assigns positive probability to all percepts appearing in ${h}$. For every ${\mu \in \SP{\Env}}$, we define the corresponding ${\mu_{\operatorname{b}} \in \EnvB}$ by

$$\Prb[\mu_{\operatorname{b}}(h)=o]:=\Prb_{e \sim \mu}[e(h) = o \mid \forall i \in [\Abs{h}]: h^\Per_{i} = e(h_{0:i-1})]$$

Here, ${\Abs{h}}$ is defined by ${h \in (\Per \times \Act)^{\Abs{h}}}$ and ${h^\Per_{i} \in \Per}$ stands for the first element of the pair ${h_i \in \Per \times \Act}$. Note that the operation of taking convex linear combinations in ${\SP{\Env}}$ descends to a well-defined operation in ${\EnvB}$ which we will also regard as convex linear combination (but which is *not* the same as taking pointwise convex linear combinations of the partial functions; instead it can be written as the result of Bayesian updating). 

Priors are usually constructed from environments that are in some sense computable. There are several variants of the definition, but we will consider environments that can be represented as

$${M: \His \times \Sagas \Alg \Per}$$

Here, ${\Sagas}$ is regarded as an infinite sequence of fair coin flips and the notation ${\Alg}$ means that ${M}$ is a Turing machine that halts with probability 1 for any fixed ${h \in \His}$ and ${r \in \Sagas}$ a sequence of fair coin flips. The corresponding environment is defined by ${\Prb[\mu_M(h)=o]:=\Prb_{r \sim \Un^\omega}[M(h,r)=o]}$, where ${\Un^\omega}$ is the fair coin probability measure on ${\Sagas}$ (it is easy to see that the set ${\{r \in \Sagas \mid M(h,r)=o\}}$ is measurable because ${M}$ is a Turing machine). 

Fix ${\UTM}$ a prefix-free universal Turing machine and denote ${\Prog \subseteq \Words}$ the set of programs that satisfy the halting condition above. We have ${\zeta \in \SP{\Prog}}$ defined by

$${\zeta(x):=\frac{2^{-\Abs{x}}}{\sum_{y \in \Prog} 2^{-\Abs{y}}}}$$

For any ${x \in \Prog}$, we can define the environment ${\mu_x^\UTM}$: 

$$\Pr[\mu_x^\UTM(h)=o]:=\Prb_{r \sim \Un^\omega}[\UTM(x;h,r)=o]$$ 

We can now define the universal prior ${\mu^\UTM:=\E_{x \sim \zeta}[\mu_x^\UTM]}$.

Alternatively, we can consider environments that have an unobservable state. For any ${\nu \in \EnvB}$ we denote the domain of ${\nu}$ by ${\His_\nu}$. An computable environment with unobservable state corresponds to a machine

$${M: \His \times \Nats \times \Sagas \Alg \Per \times \Nats}$$ 

The corresponding ${\mu_M \in \EnvB}$ is defined recursively, simultaneously with ${\rho_M: \His_{\mu_M} \rightarrow \SP{\Nats}}$, the latter describing the probability distribution on unobservable states. Denoting ${\Estr_{\Per \times \Act}}$ the empty element of ${\His}$:

\begin{align*}
\rho_{M}(\Estr_{\Per \times \Act}) &:= \delta_0 \\
\Prb[\mu_M(h)=o] &:= \PP{u \sim \rho_{M}(h)}{r \sim \Un^\omega}[M(h,u,r)^\Per=o] \\
\Prb[\rho_{M}(hoa)=u] &:= \PP{v \sim \rho_{M}(h)}{r \sim \Un^\omega}[M(h,v,r)^\Words=u \mid M(h,v,r)^\Per=o]
\end{align*}

For arbitrary computable environments the addition of an unobservable state gains us nothing. However, it makes a difference when imposing complexity bounds or passing to incomplete models.

We now generalize these concepts to incomplete models. A computable incomplete model (with unobservable state) corresponds to a machine 

$${M: \His \times \Nats \times \Sagas \times \Nats \Alg \Per \times \Nats}$$

The last ${\Nats}$ factor represents "Knightian coin flips." For any ${K: \His \times \Nats \rightarrow \Nats}$, we can define ${\mu_{MK} \in \EnvB}$ by the recursive equations

\begin{align*}
\rho_{MK}(\Estr_{\Per \times \Act}) &:= \delta_0 \\
\Prb[\mu_{MK}(h)=o] &:= \PP{u \sim \rho_{MK}(h)}{r \sim \Un^\omega}[M(h,u,r,K(h,u))^\Per=o] \\
\Prb[\rho_{MK}(hoa)=u] &:= \PP{v \sim \rho_{MK}(h)}{r \sim \Un^\omega}[M(h,v,r,K(h,v))^\Words=u \mid M(h,v,r,K(h,v))^\Per=o]
\end{align*}

We define the incomplete model ${\Phi_M :=\{\E_{K \sim \xi}[\mu_{MK}] \mid \xi \in \SP{(\His \times \Nats \rightarrow \Nats)}\}}$.

Denoting ${\ProgS}$ the set of programs for ${\UTM}$ that satisfy the halting condition for machines of above type, we can define ${\Phi_x \subseteq \EnvB}$ for any ${x \in \ProgS}$ in the obvious way. We define ${\bar{\zeta} \in \Delta \ProgS}$ by

$${\bar{\zeta}(x):=\frac{2^{-\Abs{x}}}{\sum_{y \in \ProgS} 2^{-\Abs{y}}}}$$

This allows us defining the "universal incomplete prior" ${\Phi^\UTM:=\E_{x \in \bar{\zeta}}[\Phi_x]}$.

It is also straightforward to impose complexity bounds on these objects (e.g. limit the time or space ${M}$ is allowed to use as a function of ${\Abs{h}}$).

[Hutter](https://arxiv.org/abs/0907.0746) observes that it's unknown whether Solomonoff induction can correctly predict relationships betweens bits in a sequence that contains uncomputable information (see problem 4g). To deal with similar issues in online learning one can use "sleeping experts" (see also [subsequence induction](https://agentfoundations.org/item?id=482)). Here, we deal with much more general scenarios than sleeping experts by including incomplete models in our prior.

%##Appendix
\section{Bayes-Savage Agents}

Consider an agent that has to choose from a finite strategy set ${\Strat}$. The utility function of the agent is ${u: \Strat \times X \rightarrow \Reals}$. If the agent's beliefs are described by a complete model ${\mu \in \SP{X}}$, the standard decision rule is maximizing expected utility: 

$${s(\mu):=\Argmax{s \in \Strat} \E_{w \sim \mu}[u(s,w)]}$$

For an incomplete model ${\Phi \subseteq \SP{X}}$, we propose using the minimax regret decision rule:

$$s(\Phi):=\Argmax{s \in \SP{\Strat}} \min_{\mu \in \Phi}(\E_{w \sim \mu}[u(s,w)] - \max_{t \in \Strat} \E_{w \sim \mu}[u(t,w)])$$

As opposed to expected utility maximization, minimax regret might demand randomization. This is perhaps not surprising since if we expect our agents to produce Nash equilibria, there must be randomization in their decision rule (e.g. in reflective oracles this randomization comes from the oracle itself).

As a simple example for a setting where the minimax regret rule yields reasonable behavior, consider stochastic multi-armed bandits...

This stands in sharp contrast to the *minimax* decision rule...

Subgame / proper...

Dynamics consistency...

\section{Discussion}

\end{document}


