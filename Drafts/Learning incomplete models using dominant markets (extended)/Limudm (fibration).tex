%&latex
\documentclass[a4paper]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}
\DeclareMathOperator{\Var}{Var}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}

\DeclareMathOperator{\Prj}{pr}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\text{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Estr}{\boldsymbol{\lambda}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}

\newcommand{\Prob}{\mathcal{P}}

% Paper specific

\newcommand{\T}{\mathcal{T}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\UM}{\mathcal{U}}
\newcommand{\W}{\operatorname{W}}
\newcommand{\SW}{\operatorname{\Sigma W}}
\newcommand{\I}{\operatorname{id}}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\NormL}[1]{\Norm{#1}_{\operatorname{Lip}}}
\newcommand{\Dkr}{\operatorname{d}_{\text{KR}}}
\newcommand{\Ball}{\operatorname{B}}

\begin{document}

*This post is formal treatment of the idea outlined [here](https://agentfoundations.org/item?id=1197).*

Given a countable set of [incomplete models], we define a forecasting function that converges in the Kantorovich-Rubinstein metric with probability 1 to every one of the models which is satisfied by the true environment. This is analogous to Blackwell-Dubins [merging of opinions](https://projecteuclid.org/euclid.aoms/1177704456), for complete models, except that Kantorovich-Rubinstein convergence is weaker than convergence in total variation. The forecasting function is a [dominant market](https://agentfoundations.org/item?id=1214) for a suitably constructed set of traders. We prove this result in a fairly general setting (for every finite time $t \in \Nats$, the observation space is an arbitrary compact subset of $\Reals^{D(t)}$).%, but only assuming projective determinacy. In simpler situations (e.g. when observations are strings over a finite alphabet), ZFC is sufficient.

Appendix A contains the proofs of key lemmas and theorems. Appendix B contains the proofs of technical propositions used in Appendix A, which are mostly straightforward.

***

\section{Notation}

Given ${X}$ a metric space and ${x \in X}$, ${\Ball_r(x):=\{y \in X \mid d(x,y) <\ r\}}$.

Given ${X}$ a topological space:

* ${\I_X: X \rightarrow X}$ is the identity mapping.

* ${\Prob(X)}$ is the space of Borel probability measures on ${X}$ equipped with the weak* topology.

* ${C(X)}$ is the Banach space of continuous functions ${X \rightarrow \Reals}$ with uniform norm.

* ${\mathcal{B}}(X)$ is the Borel ${\sigma}$-algebra on ${X}$.

* ${\UM(X)}$ is the ${\sigma}$-algebra of universally measurable sets on ${X}$.

* Given ${\mu \in \Prob(X)}$, ${\Supp \mu}$ denotes the support of ${\mu}$. 

Given ${X}$ and ${Y}$ measurable spaces, ${K: X \Markov\ Y}$ is a Markov kernel from ${X}$ to ${Y}$. For any ${x \in X}$, we have ${K(x) \in \Prob(Y)}$. Given ${\mu \in \Prob(X)}$, ${\mu \ltimes K \in \Prob(X \times Y)}$ is the semidirect product of ${\mu}$ and ${K}$ and ${K_*\mu \in \Prob(Y)}$ is the pushforward of ${\mu}$ by ${K}$.

Given ${X}$, ${Y}$ Polish spaces, ${\pi: X \rightarrow Y}$ Borel measurable and ${\mu \in \Prob(X)}$, we denote ${\mu \mid \pi}$ the set of Markov kernels ${K: Y \Markov X}$ s.t. ${\pi_* \mu \ltimes K}$ is supported on the graph of ${\pi}$ and ${K_*\pi_* \mu = \mu}$. By the disintegration theorem, ${\mu \mid \pi}$ is always non-empty and any two kernels in ${\mu \mid \pi}$ coincide ${\pi_*\mu}$-almost everywhere.

\section{Dominant Stochastic Markets}

The way we previously laid out the dominant market formalism, the sequence of observations (represented by the sets $\{X_i\}$) was fixed. To study forecasting, we instead need to assume this sequence is sampled from some probability measure (the true environment).

For each ${n \in \Nats}$, let ${Y_n}$ be a compact Polish space. ${Y_n}$ represents the space of possible observations made before time ${n}$. Let ${\pi_n: Y_{n+1} \rightarrow Y_n}$ be continuous mappings. Given ${n \leq m}$, ${\pi_{nm}: Y_m \rightarrow Y_n}$ denotes the composition $\pi_n \circ \pi_{n+1} \circ \ldots \circ \pi_{m-1}$. The typical example is ${Y_n=Y^n}$ where ${Y}$ is some fixed compact Polish space (at some point below we will have to assume it is a compact subset of ${\Reals^D}$) and ${\pi_n}$ are the projection mappings. Define ${X = \varprojlim Y_n}$, i.e. ${X}$ is the subspace of ${\prod_n Y_n}$ given by the equations ${\pi_n(y_{n+1})=y_n}$. ${X}$ is a compact Polish space (in the example above, ${X = Y^\omega}$). For each ${n \in \Nats}$ we denote ${\pi_{n\omega}: X \rightarrow Y_n}$ the projection mapping. Given ${y \in Y_n}$, we denote ${X_y:=\pi_{n\omega}^{-1}(y)}$, a closed subspace of ${X}$.

\#Definition N1

A *market* is a sequence of mappings ${\{M_n: Y_n \rightarrow \Prob(X)\}}_{n \in \Nats}$ s.t.

* Each ${M_n}$ is measurable w.r.t. ${\UM(Y_n)}$ and ${\B(\Prob(X))}$.

* For any ${y \in Y_n}$, ${\Supp M_n(y) \subseteq X_y}$.

***

As before, we define the space of trading strategies ${\T(X):=C(\Prob(X)\times X)}$, but this time we regard it as a Banach space. 

\#Definition N2

A *trader* is a sequence of mappings ${\{T_n: Y_n \times \Prob(X)^n \rightarrow \T(X)\}}_{n \in \Nats}$ which are measurable w.r.t. ${\UM(Y_n) \otimes \B(\Prob(X)^n)}$ and ${\B(\T(X))}$.

***

Given a trader ${T}$ and a market ${M}$, we define the mappings ${\{T^M_n: Y_n \rightarrow \T(X)\}_{n \in \Nats}}$ (measurable w.r.t. ${\UM(Y_n)}$ and ${\B(\T(X))}$) and ${\{\bar{T}^M_n: Y_n \rightarrow C(X)\}_{n \in \Nats}}$ (measurable w.r.t. ${\UM(Y_n)}$ and ${\B(C(X))}$) as follows:

$$T^M_n(y):= T_n(y, M_0(\pi_{0n}(y)),M_1(\pi_{1n}(y)) \ldots M_{n-1}(\pi_{n-1,n}(y)))$$

$$\bar{T}^M_n(y):= T^M_n(y,M_n(y))$$

The "market maker" lemma now requires some additional work due to the measurability requirement:

\#Lemma N1

Consider any trader ${T}$. Then, there is a market ${M}$ s.t. for all ${n \in \Nats}$ and ${y \in Y_n}$

$$\Supp M_n(y) \subseteq \Argmax{X_y} \bar{T}^M_n(y)$$

***

As before, we have the operator ${\W: \T(X) \rightarrow \T(X)}$ defined by 

$$\W \tau(\mu,x):= \tau(\mu,x) - \E_{z \sim \mu}[\tau(\mu,z)]$$

We also introduce the notation ${\{\W \bar{T}^M_n: Y_n \rightarrow C(X)\}_{n \in \Nats}}$ and ${\{\SW T^M_n: Y_n \rightarrow C(X)\}_{n \in \Nats}}$ which are measurable mappings defined by

$$\W \bar{T}^M_n(y) = \W T^M_n(y, M_n(y))$$

$$\SW T^M_n(y) = \sum_{m \leq n} \W \bar{T}^M_m(\pi_{mn}(y))$$

\#Definition N3

A market ${M}$ is said to dominate a trader ${T}$ when for any ${x \in X}$, if

$$\inf_{n \in \Nats} \min_{z \in \pi_{n\omega}^{-1}(\pi_{n\omega}(x))} \SW T^M_n(\pi_{n\omega}(x),z) > -\infty$$

then

$$\sup_{n \in \Nats} \max_{z \in \pi_{n\omega}^{-1}(\pi_{n\omega}(x))} \SW T^M_n(\pi_{n\omega}(x),z) < +\infty$$

\#Theorem N2

Given any countable set of traders $R$, there is a market ${M}$ s.t. ${M}$ dominates all ${T \in R}$.

***

Theorem N2 is proved exactly as [before](https://agentfoundations.org/item?id=1214) (modulo Lemma N1), and we omit the details.

\section{Profitable Metastrategies}

We now describe a class of traders associated with a fixed environment ${\mu^* \in \Prob(X)}$ s.t. if a market dominates a trader from this class, a certain function of the pricing converges to 0 with ${\mu^*}$-probability 1. Afterwards, we will apply this result to a trader associated with an incomplete models ${\Phi \subseteq \Prob(X)}$ by observing that the trader is in the class for any ${\mu^* \in \Phi}$.

\#Definition N4

A *trading metastrategy* is a uniformly bounded family of measurable mappings ${\{\upsilon_n: Y_n \rightarrow \T(X)\}_{n \in \Nats}}$. Given ${\mu^* \in \Prob(X)}$, ${\upsilon}$ is said said to be *profitable for ${\mu^*}$*, when there are ${c > 0}$ and ${\{K_n \in \mu^* \mid \pi_{n\omega}\}_{n \in \Nats}}$ s.t. for any ${n \in \Nats}$, ${\pi_{n\omega*}\mu^*}$-almost any ${y \in Y_n}$ and any ${\mu \in \Prob(X_y)}$:

$$\E_{K_n(y)}[\upsilon(y,\mu)] - \E_{\mu}[\upsilon(y,\mu)] \geq c \Norm{\upsilon(y,\mu)}$$

***

Even if a metastrategy is profitable, it doesn't mean that a smart trader should use this metastrategy all the time: in order to avoid running out of budget, a trader shouldn't place too many bets simultaneously. The following construction defines a trader that employs a metastrategy only when all previous bets are closed to being resolved.

\#Definition N5

Fix a metastrategy ${\upsilon}$. We define the trader ${T_\upsilon}$ and the measurable mappings ${\{U_{\upsilon  n}: Y_n \times P(X)^n \rightarrow C(X)\}_{n \in \Nats}}$ recursively as follows:
 
 $$U_0 := 0$$

$$T_{\upsilon0} := \upsilon$$

$$U_{\upsilon,n+1}(y, \{\mu_m\}_{m \leq n}) := U_{\upsilon n}(\pi_n(y), \{\mu_m\}_{m < n}) + T_{\upsilon n}(y, \{\mu_m\}_{m \leq n})$$

$$T_{\upsilon,n+1}(y, \{\mu_m\}_{m \leq n}) := \begin{cases}\upsilon(y) \text{ if } \max_{X_y} U_{\upsilon,n+1}(y,\{\mu_m\}_{m \leq n}) - \min_{X_y} U_{\upsilon,n+1}(y,\{\mu_m\}_{m \leq n})\leq 1\\0 \text{ otherwise}\end{cases}$$

\#Lemma N3

Consider ${\mu^* \in P(X)}$, ${\{K_n \in \mu^* \mid \pi_{n\omega}\}_{n \in \Nats}}$, ${\upsilon}$ a metastrategy profitable for ${\mu^*}$ and ${M}$ a market. Assume ${M}$ dominates ${T_\upsilon}$. Then, for ${\mu^*}$-almost any ${x \in X}$, denoting ${y_n:=\pi_{n\omega}(x)}$:

$$\lim_{n \rightarrow \infty} (\E_{K_n(y_n)}[\upsilon(y_n,M_n(y_n))]-\E_{M_n(y_n)}[\upsilon(y_n,M_n(y_n))])= 0$$

\section{Profiting from Incomplete Models}

Consider ${X}$ any compact Polish metric space and ${d: X \times X \rightarrow \Reals}$ its metric. We denote ${\Lip(X)}$ the Banach space of Lipschitz continuous functions ${X \rightarrow \Reals}$ with the norm

$$\NormL{f}:=\max_{x} \Abs{f(x)} + \sup_{x \ne y}\frac{\Abs{f(x)-f(y)}}{d(x,y)}$$

${\Prob(X)}$ (as before, with the weak topology) can be regarded as a compact subset of ${\Lip(X)'}$ (with the strong topology), yielding a metrization of ${\Prob(X)}$ which we will call the Kantorovich-Rubinstein metric ${\Dkr}$:

$$\Dkr(\mu,\nu):=\sup_{\NormL{f} \leq 1} \Abs{\E_\mu[f] - \E_\nu[f]}$$

In fact, the above differs from the standard definition of the Kantorovich-Rubinstein metric (a.k.a. 1st Wasserstein metric, a.k.a. earth's mover metric), but this abuse of terminology is mild since the two are strongly equivalent.

Now consider ${\Phi \subseteq \Prob(X)}$ convex. We will describe a class of trading strategies that are designed to exploit any ${\mu \in \Phi}$.

\#Definition N6

${\tau \in \T(X)}$ is said to be *profitable for ${\Phi}$* when for all ${\mu \in \Prob(X)}$, ${\nu \in \Phi}$ we have

$$\E_\nu[\tau(\mu)] \geq \E_\mu[\tau(\mu)] + \Dkr(\mu,\Phi)$$

\#Lemma N4

Consider ${Y}$ another compact Polish space and ${\Phi \subseteq Y \times \Prob(X)}$ closed. For any ${y \in Y}$, define ${\Phi_y \subseteq \Prob(X)}$ by

$${\Phi_y := \{\mu \in \Prob(X) \mid (y,\mu) \in \Phi\}}$$ 

Assume that for any ${y \in Y}$, ${\Phi_y}$ is convex. Then, there exists ${\upsilon: Y \rightarrow \T(X)}$ measurable w.r.t. ${\UM(Y)}$ and ${\B(\T(X))}$ s.t. for all ${y \in Y}$, ${\Norm{\upsilon(y)} \leq 2}$ and ${\upsilon(y)}$ is profitable for ${\Phi_y}$ (instead of 2, the norm can be bounded by ${1+\epsilon}$ for any ${\epsilon > 0}$, but for our purposes any uniform bound is sufficient).

***

Now consider ${\{Y_n\}_{n \in \Nats}}$ and ${X = \varprojlim Y_n}$ as before, but assume that for each ${n \in \Nats}$, ${Y_n}$ is a compact subset of ${\Reals^{D(n)}}$ where ${D: \Nats \rightarrow \Nats}$ is an arbitrary function. The following definition provides a notion of updating an incomplete model by observations:

\#Definition N7

Consider ${\Phi \subseteq \Prob(X)}$. For any ${n \in \Nats}$ and ${y \in Y_n}$, we define ${\Phi_y'' \subseteq \Prob(X)}$ by

$$\Phi_y'':=\{\lim_{r \rightarrow 0} (\mu \mid \pi_{n\omega}^{-1}(\Ball_r(y))) \mid \mu \in \Phi\}$$

Note that the limit in the definition above need not exist for every ${\mu \in \Phi}$.

Denote ${\Phi_y'}$ the convex hull of ${\Phi_y''}$ and define ${\Phi_n' \subseteq Y_n \times \Prob(X)}$ by

$$\Phi_n':=\{(y,\mu) \in Y_n \times \Prob(X) \mid \mu \in \Phi_y'\}$$

Finally, we define ${\Phi_n \subseteq Y_n \times \Prob(X)}$ to be the closure of ${\Phi_n'}$. Given ${y \in Y_n}$, we define ${\Phi_y \subseteq \Prob(X)}$ by

$$\Phi_y:=\{\mu \in \Prob(X) \mid (y,\mu) \in \Phi_n\}$$

***

Note that the above definition uses the Euclidean metric on ${Y_n}$. This is the only place through which the assumption that ${Y_n}$ is a compact subset of ${R^{D(n)}}$ enters. This is used the in the proof of Lemma N5 below, via the Lebesgue differentiation theorem.

Fix a family of metrizations of ${X}$: ${\{d_n: X \times X \rightarrow \Reals\}_{n \in \Nats}}$. The reason we need a family rather than a single metric is that convergence in ${\Dkr}$ is trivial unless we renormalize the metric for each ${n}$. On the other hand, completely arbitrary renormalization is allowed. For example, if ${X=Y^\omega}$ and ${d}$ is a metrization of ${Y}$ (e.g. the Euclidean metric) we can take 

$$d_n(x^1,x^2)= \max_{m \in \Nats} c_{nm} d(x^1_m,x^2_m)$$

Here, ${\{c_{nm} > 0\}_{n,m \in \Nats}}$ are required to satisfy ${\lim_{m \rightarrow \infty} c_{nm} = 0}$. To get a non-trivial result, we need the ${c_{nm}}$ to fall slower with ${m}$ as ${n}$ increases. The stronger we make this trend, the stronger conclusion we get (although it always remains weaker than convergence in total variation).

\#Lemma N5

Consider ${\Phi \subseteq \Prob(X)}$ and ${\epsilon > 0}$. For every ${n \in \Nats}$, denote ${\Dkr^n}$ the Kantorovich-Rubinstein metric associated with ${d_n}$. Define ${\Phi_n^\epsilon \subseteq Y_n \times \Prob(X)}$ by 

$$\Phi_n^\epsilon:=\{(y,\mu) \in Y_n \times \Prob(X) \mid \Dkr^n(\mu,\Phi_y) \leq \epsilon\}$$

For every ${y \in Y_n}$, we define ${\Phi^\epsilon_y \subseteq \Prob(X)}$ by

$$\Phi^\epsilon_y:=\{\mu \in \Prob(X) \mid (y,\mu) \in \Phi^\epsilon_n\}$$

Let ${\upsilon}$ be a metastrategy s.t. for each ${n \in \Nats}$ and ${y \in Y_n}$, ${\upsilon_n(y)}$ is profitable for ${\Phi^\epsilon_y}$. Then, ${\upsilon}$ is profitable for any ${\mu^* \in \Phi}$.

***

Combining Lemma N3 and Lemma N5, it is easy to get the following:

\#Corollary N6

Consider ${\Phi \subseteq \Prob(X)}$, ${\epsilon > 0}$ and ${\upsilon}$ a metastrategy s.t. for each ${n \in \Nats}$ and ${y \in Y_n}$, ${\upsilon_n(y)}$ is profitable for ${\Phi^\epsilon_y}$. Let ${M}$ be a market which dominates ${T_\upsilon}$. Then, for any ${\mu^* \in \Phi}$ and ${\mu^*}$-almost any ${x \in X}$, denoting ${y_n:=\pi_{n\omega}(x)}$:

$$\limsup_{n \rightarrow \infty} \Dkr^n(M_n(y_n),\Phi_{y_n}) \leq \epsilon$$

***

Thus, if we construct a set of traders ${T_{\upsilon_k}}$ as in Corollary N6 where ${\upsilon_k}$ corresponds to ${\epsilon=\frac{1}{k}}$, then a market dominating all of these traders would have to converge to ${\Phi_{y_n}}$ with ${\mu^*}$-probability 1. Putting this together with Lemma N4 (so that ${\upsilon_k}$ as above actually exists) and Theorem N2 we finally get

\#Theorem N7

Consider a ${H}$ countable set of subsets of ${\Prob(X)}$. Then, there exists a market ${M^H}$ s.t. for any ${\Phi \in H}$, ${\mu^* \in \Phi}$ and ${\mu^*}$-almost any ${x \in X}$, denoting ${y_n:=\pi_{n\omega}(x)}$:

$$\lim_{n \rightarrow \infty} \Dkr^n(M^H_n(y_n),\Phi_{y_n}) = 0$$

***

This leaves an interesting open question, namely whether the counterpart of Theorem N7 with ${\Dkr}$ replaced by total variation distance is true.

\section{Appendix A}

\#Proof of Lemma N1

We construct ${M_n}$ recursively in ${n}$. Define ${Z \subseteq Y_n \times \T(X) \times \Prob(X)}$ by

$${Z:=\{(y,\tau,\mu) \in Y_n \times \T(X) \times \Prob(X) \mid \Supp \mu \in \Argmax{X_y} \tau(\mu)\}}$$



TBD

\section{Appendix B}

\#Proposition N11

If ${X,Y}$ are compact Polish spaces and ${f: X \times Y \rightarrow \Reals}$ is continuous, then ${F: X \rightarrow C(Y)}$ defined by ${F(x)(y):=f(x,y)}$ is continuous.

***

We omit the proof of Proposition N11, since it appeared as "Proposition A.2" [before](https://agentfoundations.org/item?id=1214).

\#Proposition N8

Fix ${X,Y}$ compact Polish spaces. Define ${e: C(Y \times X) \times Y \rightarrow C(X)}$ by ${e(f,y)(x):=f(y,x)}$. Then, ${e}$ is continuous. In particular, we can apply this to ${Y = \Prob(X)}$ in which case ${e: \T(X) \times \Prob(X) \rightarrow C(X)}$.

\#Proof of Proposition N8

Consider ${f_k \rightarrow f}$ and ${y_k \rightarrow y}$. We have

$$\max_{x \in X} \Abs{f_k(y_k,x)-f(y_k,x)} \leq \Norm{f_k - f} \rightarrow 0$$

By Proposition N11

$$\max_{x \in X} \Abs{f(y_k,x)-f(y,x)} \rightarrow 0$$

Combining, we get

$$\max_{x \in X} \Abs{f_k(y_k,x)-f(y,x)} \rightarrow 0$$

\#Proposition N14

Fix ${X}$ a compact Polish space. Consider ${f \in C(X)}$ and ${\mu \in \Prob(X)}$ and denote ${M := \max f}$. Then, ${\Supp \mu \subseteq f^{-1}(M)}$ iff ${\E_\mu[f] = M}$.

\#Proof of Proposition N14

If ${\Supp \mu \subseteq f^{-1}(M)}$ then $\Prb_{x\sim \mu}[f(x) \ne M] = 0$ and therefore ${\E_\mu[f] = M}$.

Now, assume ${\E_\mu[f] = M}$. For any ${k \in \Nats}$, Markov's inequality yields 

$$\Prb_{x\sim \mu}[M - f(x) \geq \frac{1}{k}] \leq k\E_{x \sim \mu}[M - f(x)] = 0$$

Taking $k \rightarrow \infty$, we get ${\Prb_{x\sim \mu}[M > f(x)] = 0}$ and hence ${\Supp \mu \subseteq f^{-1}(M)}$.

\#Proposition N17

Fix ${X,Y}$ compact Polish spaces, ${\pi: X \rightarrow Y}$ continuous. Define ${F: Y \times C(X) \rightarrow \Reals}$ by 

$${F(y,f):=\max_{x \in \pi^{-1}(y)} f(x)}$$

Then, ${F}$ is upper semicontinuous.

\#Proof of Proposition N17

Consider ${y_k \rightarrow y}$, ${f_k \rightarrow f}$. It is enough to show that if $F(y_k,f_k) \rightarrow t$, then ${F(y,f) \geq t}$. Choose ${\{x_k \in \Argmax{\pi^{-1}(y_k)}f_k\}_{k \in \Nats}}$. Without loss of generality, we may assume that ${x_k \rightarrow x_\omega}$ (otherwise take a subsequence). Clearly, ${\pi(x_\omega)=y}$ and therefore

$$F(y,f) \geq f(x_\omega)=\lim_{k \rightarrow \infty} f(x_k) = \lim_{k \rightarrow \infty} f_k(x_k)= \lim_{k \rightarrow \infty} F(y_k,f_k)$$

\#Proposition N9

Fix ${X,Y}$ compact Polish spaces, ${\pi: X \rightarrow Y}$ continuous. Define ${Z \subseteq Y \times \T(X) \times \Prob(X)}$ by

$$Z:=\{(y,\tau,\mu) \in Y \times \T(X) \times \Prob(X) \mid \E_\mu[\tau(\mu)] = \max_{\pi^{-1}(y)} \tau(\mu)\}$$

Then, ${Z}$ is closed???

\#Proof of Proposition N9

Consider ${y_k \rightarrow y, \tau_k \rightarrow \tau}$, ${\mu_k \rightarrow \mu}$, ${(y_k,\tau_k,\mu_k) \in Z}$. By Proposition N8, ${\tau_k(\mu_k) \rightarrow \tau(\mu)}$ and hence??? 

$${\max \tau_k(\mu_k) \rightarrow \max \tau(\mu)}$$

and 

$$\Abs{\E_{\mu_k}[\tau_k(\mu_k)] - \E_{\mu_k}[\tau(\mu)]} \leq \Norm{\tau_k(\mu_k) - \tau(\mu)}  \rightarrow 0$$

Also, ${\mu_k \rightarrow \mu}$ implies that

$$\E_{\mu_k}[\tau(\mu)] \rightarrow \E_{\mu}[\tau(\mu)]$$

Combining, we conclude that $\E_{\mu_k}[\tau_k(\mu_k)] \rightarrow \E_{\mu}[\tau(\mu)]$ and ${(\tau,\mu) \in Z}$.

\#Proposition N10

Fix ${X}$ a compact Polish metric space and ${Z \subseteq X}$. Define ${d_Z: X \rightarrow \Reals}$ by

$${d_Z(x):=d(x,Z):=\inf_{z \in Z} d(x,z)}$$

Then, ${d_Z}$ is continuous.

\#Proof of Proposition N10

Define ${d': X \rightarrow C(X)}$ by ${d'(x)(y)=d(x,y)}$. ${d'}$ is continuous by Proposition N11, hence ${d_Z}$ is continuous as well.

\#Proposition N12

Fix ${X}$ a compact Polish metric space and ${Z \subseteq X}$ closed. Then, for any ${x \in X \setminus Z}$, ${d(x,Z) > 0}$.

\#Proof of Proposition N12

Consider ${x \in X}$ s.t. ${d(x,Z)=0}$. Then, fore any ${k \in \Nats}$ there is ${z_k \in Z}$ s.t. ${d(x,z_k) < \frac{1}{k}}$. Obviously, ${z_k \rightarrow x}$. Since ${Z}$ it closed, it follows that ${x \in Z}$.

\#Proposition N13

Fix ${X}$ a compact Polish metric space and ${Z \subseteq X}$ closed. Consider any ${\mu \in \Prob(X)}$. Then, ${\Supp \mu \subseteq Z}$ iff ${\E_\mu[d_Z] = 0}$.

\#Proof of Proposition N13

Follows directly from Proposition N12 and Proposition N14.

\#Proposition N15

Fix ${X,Y}$ compact Polish metric spaces and ${\pi: X \rightarrow Y}$ continuous. Consider ${\{y_k \in Y\}_{k \in \Nats}}$, ${y_k \rightarrow y}$. Then

$$\lim_{k \rightarrow \infty} \max_{x \in \pi^{-1}(y_k)} d(x,\pi^{-1}(y)) = 0$$

\#Proof of Proposition N15

Follows directly from Proposition N17 and Proposition N10.

\#Proposition N16

Consider ${X,Y}$ compact Polish spaces and ${\pi: X \rightarrow Y}$ continuous. Define ${\Delta \subseteq Y \times \Prob(X)}$ by

$$\Delta:=\{(y,\mu) \in Y \times \Prob(X) \mid \Supp \mu \subseteq \pi^{-1}(y)\}$$

Then, ${\Delta}$ is closed

\#Proof of Proposition N16

Consider ${y_k \rightarrow y}$, ${\mu_k \rightarrow \mu}$, ${(y_k, \mu_k) \in \Delta}$. Denote ${Z:=\pi^{-1}(y)}$, ${Z_k:=\pi^{-1}(y_k) \cup Z}$. By the triangle inequality, For any ${x \in X}$:

$$d(x, Z) \leq d(x, Z_k)  + \max_{z \in \pi^{-1}(y_k)} d(z,Z)$$

Obviously, also $d(x, Z_k) \leq d(x, Z)$. It follows that

$$\max_{x \in X} \Abs{d(x, Z_k) - d(x, Z)} \leq \max_{z \in \pi^{-1}(y_k)} d(z,Z)$$

By Proposition N15, we get ${d_{Z_k} \rightarrow d_Z}$. Also, since ${(y_k,\mu_k) \in \Delta}$, ${\E_{\mu_k}[d_{Z_k}] = 0}$. It follows that ${\E_\mu[d_Z] = 0}$ and, by Proposition N13, ${(y,\mu) \in \Delta}$.

TBD

\end{document}




