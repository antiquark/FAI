%
\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}
\usepackage{bm}

\numberwithin{equation}{section}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
%\newtheorem{conjecture}{Conjecture}[section]

%\theoremstyle{remark}
%\newtheorem{note}{Note}[section]

\newcommand{\Bool}{\{0,1\}}
\newcommand{\Words}{{\Bool^*}}
\newcommand{\WordsLen}[1]{{\Bool^{#1}}}

% operators that are separated from the operand by a space
\DeclareMathOperator{\Sgn}{sgn}
\DeclareMathOperator{\Supp}{supp}
\DeclareMathOperator{\Stab}{stab}
\DeclareMathOperator{\Img}{Im}

% operators that require brackets
\DeclareMathOperator{\Prb}{Pr}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}

% operators that require parentheses
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Sym}{Sym}

% special symbols that are not really operators
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\R}{r}
\DeclareMathOperator{\A}{a}
\DeclareMathOperator{\M}{M}
\DeclareMathOperator{\UM}{UM}
\DeclareMathOperator{\Un}{U}
\DeclareMathOperator{\En}{c}

\newcommand{\KL}[2]{\operatorname{D}_{\mathrm{KL}}(#1 \| #2)}
\newcommand{\Dtv}{\operatorname{d}_{\textnormal{tv}}}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\Lim}[1]{\lim_{#1 \rightarrow \infty}}
\newcommand{\LimInf}[1]{\liminf_{#1 \rightarrow \infty}}
\newcommand{\LimSup}[1]{\limsup_{#1 \rightarrow \infty}}

\newcommand{\Abs}[1]{\lvert #1 \rvert}
\newcommand{\Norm}[1]{\lVert #1 \rVert}
\newcommand{\Floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Ceil}[1]{\lceil #1 \rceil}
\newcommand{\Chev}[1]{\langle #1 \rangle}
\newcommand{\Quote}[1]{\ulcorner #1 \urcorner}

\newcommand{\GrowR}{\Gamma_{\mathfrak{R}}}
\newcommand{\GrowA}{\Gamma_{\mathfrak{A}}}
\newcommand{\Grow}{\Gamma:=(\GrowR,\GrowA)}
\newcommand{\MGrow}{\mathrm{M}\Gamma}
\newcommand{\Fall}{\mathcal{E}}
\newcommand{\EG}{\Fall(\Gamma)}
\newcommand{\ESG}{\Fall^\sharp(\Gamma)}
\newcommand{\EMG}{\Fall(\MGrow)}
\newcommand{\ESMG}{\Fall^\sharp(\MGrow)}
\newcommand{\BoolR}[1]{\Bool^{\R_{#1}(K)}}

\newcommand{\Alg}{\xrightarrow{\textnormal{alg}}}
\newcommand{\Markov}{\xrightarrow{\textnormal{mk}}}
\newcommand{\Scheme}{\xrightarrow{\Gamma}}
\newcommand{\MScheme}{\xrightarrow{\MGrow}}
\usepackage[normalem]{ulem}

\begin{document}

%\title[Optimal Predictors]{Optimal Predictors: A Bayesian Notion of Approximation Algorithm (Draft)}
\title{Optimal Predictors: A Bayesian Notion of Approximation Algorithm (Draft)}

\author{Vadim Kosoy}
\affil{Affiliation TBD}

\date{\today}

\maketitle

\begin{abstract}
The concept of an \enquote{approximation algorithm} is usually only applied to optimization problems since in optimization problems the performance of the algorithm on any given input is a continuous parameter. We introduce a new concept of approximation applicable to decision problems and functions, inspired by Bayesian probability. From the perspective of a Bayesian reasoner with limited computational resources, the answer to a problem that cannot be solved exactly is uncertain and therefore should be described by a random variable. It thus should make sense to talk about the expected value of this random variable, an idea we formalize in the language of average-case complexity theory by introducing the concept of \enquote{optimal predictor.} We show that optimal predictors exhibit many parallels with \enquote{classical} probability theory, prove some existence theorems and demonstrate some applications to artificial general intelligence.%
\end{abstract}%

\section*{Introduction}
%
Imagine you are strolling in the city with a friend when a car passes by with the license plate number \enquote{7614829}. Your friend proposes a wager, claiming that the number is composite and offering 10 : 1 odds in your favor. Knowing that your friend has no exceptional ability in mental arithmetic and that it's highly unlikely they saw this car before, you realize they are just guessing. Your mental arithmetic is also insufficient to test the number for primality, but is sufficient to check that ${7614829 \equiv 1 \pmod{3}}$ and $\frac{1}{\ln 7614829} \approx 0.06$. Arguing from the prime number theorem and observing that 7614829 is odd and is divisible neither by 3 nor by 5, you conclude that the probability 7614829 is prime is ${\frac{1}{\ln 7614829} \times 2 \times \frac{3}{2} \times \frac{5}{4} \approx 22\%}$. Convinced that the odds are in your favor you accept the bet\footnote{Alas, $7614829 = 271 \times 28099$.}.

From the perspective of frequentist probability the question \enquote{what is the probability 7614829 is prime?} seems meaningless, since it is either prime or not so there is no frequency to observe (unless the frequency is 0 or 1). From a Bayesian perspective, probability represents a degree of confidence, however in classical Bayesian probability theory it is assumed the only source of uncertainty is lack of information. The number 7614829 already contains all information needed to determine whether it's prime so the probability again has to be 0 or 1. However, real life uncertainty is not only information-theoretic but also complexity-theoretic. Even when we have all information to obtain the answer, out computational resources are limited so we remain uncertain. The rigorous formalization of this idea is the main goal of the present work.

The idea of assigning probabilities to purely mathematical questions was studied by several authors\cite{Gaifman_2004,Hutter_2013,Demski_2012,Christiano_2014,Garrabrant_2015}, mainly in the setting of formal logic. That is, their approach was looking for functions from the set of sentences in some formal logical language to $[0,1]$. However, although there is a strong intuitive case for assigning probabilities to sentences like

\[\varphi_1:=\text{\enquote{7614829 is prime}}\]

it is much less clear there is a meaningful assignment of probabilities to sentences like 

\[\varphi_2 := \text{\enquote{there are no odd perfect numbers}}\] 

or (even worse) 

\[\varphi_3 := \text{``there is no cardinality } \kappa \text{ s.t. } \aleph_0 < \kappa < 2^{\aleph_0} \text{"}\]

A wager on $\varphi_1$ can be resolved in a predetermined finite amount of time (the amount of time it takes to test it directly). On the other hand, it is unknown how long the resolution of $\varphi_2$ will take. It is possible that there is an odd perfect number but finding it (or otherwise becoming certain of its existence) will take a very long time. It is also possible there is no odd perfect number, a fact that cannot be directly verified because of its infinite nature. It is possible that there a proof of $\varphi_1$ within some formal theory, but accepting such a proof as resolution requires us to be completely certain of the consistency of the theory (whereas it is arguable that the consistency of formal mathematical theories, especially more abstract theories like ZFC, is itself only known empirically and in particular with less than absolute certainty). Moreover, there is no knowing a priori whether a proof exists or how long it will take to find it. For $\varphi_3$ there is no way to \enquote{directly} verify neither the sentence nor its negation, and it is actually known to be independent of ZFC.

In the present work we avoid choosing a specific category of mathematical questions. Instead, we consider the abstract setting of arbitrary distributional decision problems. This leads to the perspective that an assignment of probabilities is a form of \emph{approximate} solution to a problem. This is not the same sense of approximation as used in optimization problems, where the approximation error is the difference between the ideal solution and the actual solution. Instead, the approximation error is the prediction accuracy of our probability assignment. This is also different from average-complexity theory where the solution is required to be exact on most input instances. However the language of average-complexity theory (in particular the concept of a distributional decision problem) turns out to be well-suited to our purpose.
The concept of \enquote{optimal predictor} that arises from the approach turns out to behave much like probabilities, or more generally expected values, in \enquote{classical} probability theory. They display an appropriate form of calibration. The \enquote{expected values} are linear in general and multiplicative for functions that are independent in an appropriate sense. There is a natural parallel of conditional probabilities. For simple examples constructed from one-way functions we get the probability values we expect. Also they are well behaved in the complexity-theoretic sense that a natural class of reductions transforms optimal predictors into optimal predictors, and complete problems for these reductions exist for important complexity classes.

Optimal predictors turn out to be unique up to a certain equivalence relation. The existence of optimal predictors depends on the specific variety you consider. We show that in the non-uniform case (allowing advice) there is a variety of optimal predictors that exist for completely arbitrary problems. Uniform optimal predictors of this kind exist for a certain class of problems we call \enquote{samplable} which can be very roughly regarded as an average-case analogue of $\textsc{NP} \cap \textsc{coNP}$. More generally mapping the class of problems which admit optimal predictors allows for much further research.

Assignment of probabilities to mathematical questions is interesting in the context of artificial general intelligence\cite{Hutter_2013,Christiano_2014}. In this context is useful to consider situations in which an agent reasons about itself or several agents reason about each other \cite{Fallenstein_2015}. We give a formal model of such situations in our setting and use the Kakutani-Glicksberg-Fan theorem to prove they still admit existence of optimal predictors. We then use this to model agents playing a game in normal form and show that such agents converge to the analogue of Nash equilibrium (or even a proper equilibrium, depending on the precise implementation) in the time bounded case. That is, each agent plays a near-optimal response among those it can find within the allotted time. We suggest more potential applications for optimal predictors to decision theory and the theory of self-modifying agents.

The structure of the paper is as follows. Section~\ref{sec:fundamentals} introduces the main definitions and gives a simple example using one-way functions. Section~\ref{sec:probability} shows the parallel between properties of optimal predictors and classical probability theory. Section~\ref{sec:reductions} discusses behavior of optimal predictors and reductions and shows $\textsc{SampNP}$ has a complete problem under appropriate reductions. Section~\ref{sec:e_and_u} discusses existence and uniqueness of optimal predictors. Section~\ref{sec:reflective} discusses applications to AGI. Section~\ref{sec:discussion} discusses possible avenues for further research. The Appendix reviews relevant material about hard-core predicates and one-way functions.

\setcounter{section}{-1}

\section{Notation}
%
\subsection{Sets, Numbers and Functions}

$\Nats$ is the set of natural numbers. We will use the convention in which natural numbers start from 0, so $\Nats = \{0, 1, 2 \ldots \}$. 

$\Ints$ is the ring of integers, $\Rats$ is the field of rational numbers, $\Reals$ is the field of real numbers.

For $F \in \{\Rats,\Reals\}$, $F^{>0} := \{x \in F \mid x > 0\}$, $F^{\geq 0} := \{x \in F \mid x \geq 0\}$.

For any $t \in \Reals$, $\Floor{t} := \max \{n \in \Ints \mid n \leq t\}$, $\Ceil{t} := \min \{n \in \Ints \mid n \geq t\}$.

$\log: \Reals^{\geq 0} \rightarrow \Reals \sqcup \{-\infty\}$ will denote the logarithm in base 2.

Given $n \in \Nats$, $[n]:=\{i \in \Nats \mid i < n\}$. Given sets $X_0, X_1 \ldots X_{n-1}$, ${x \in \prod_{i \in [n]} X_i}$ and $m \in [n]$, $x_m \in X_m$ is the $m$-th component of the $n$-tuple $x$ i.e. ${x=(x_0, x_1 \ldots x_{n-1})}$.

Given a set $X$ and $x,y \in X$, $\delta_{xy}$ (or $\delta_{x,y})$ will denote the the Kronecker delta

$$\delta_{xy} := \begin{cases}1 & \text{if } x=y \\ 0 & \text{if } x \ne y \end{cases}$$

Given a set $X$ and a subset $Y$, $\chi_Y: X \rightarrow \Bool$ will denote the indicator function of $Y$ (when $X$ is assumed to be known from the context)

$$\chi_Y(x):=\begin{cases}1 & \text{if } x \in Y \\ 0 & \text{if } x \not\in Y \end{cases}$$

$\theta: \Reals \rightarrow \Bool$ will denote the Heaviside step function $\theta:=\chi_{[0,\infty)}$. ${\Sgn: \Reals \rightarrow \Bool}$ will denote the function $2 \theta - 1$.
\subsection{Measures and Probabilities}

For $X$ a measurable space, $\mu$ a probability measure on $X$, $V$ a finite dimensional vector space over $\Reals$ and $f: X \rightarrow V$, $\E_{x \sim \mu}[f(x)]$ will denote the expected value of $f$ with respect to $\mu$, i.e. 

\[\E_{x \sim \mu}[f(x)] := \int_X f(x) d\mu(x)\]

We will the abbreviated notations $\E_\mu[f(x)]$, $\E[f(x)]$, $\E_\mu[f]$, $\E[f]$ when no confusion is likely to occur.

Given a topological space $X$ and a Borel probability measure $\mu$ on $X$, $\Supp \mu$ will denote the support of $\mu$. In particular when $X$ is discrete

\[\Supp \mu = \{x \in X \mid \mu(x) > 0\}\]

Given $X,Y$ measurable spaces, $\mu$ a measure on $X$ and $f: X \rightarrow Y$ a measurable mapping, $f_*\mu$ will denote the corresponding pushforward measure on $Y$.

Given $X,Y$ measurable spaces, the notation $f: X \xrightarrow{\textnormal{mk}} Y$ signifies $f$ is a Markov kernel with source $X$ and target $Y$. Given $x \in X$, $f_x$ is the corresponding probability measure on $Y$ and $f(x)$ is a random variable sampled from $f_x$. Given $\mu$ a probability measure on $X$, $\mu \ltimes f$ (resp. $f \rtimes \mu$) is the semidirect product measure on $X \times Y$ (resp. $Y \times X$). $f_*\mu$ is the pushforward measure on $Y$, i.e. $f_*\mu:=\pi_*(\mu \ltimes f)$ where $\pi: X \times Y \rightarrow Y$ is the projection.

For $X$ a measurable space, $\mu$ a probability measure on $X$ and $A$ a measurable subset of $X$ s.t. ${\mu(A) > 0}$, $\mu \mid A$ will denote the corresponding conditional probability measure, i.e. $(\mu \mid A)(B):=\frac{\mu(B \cap A)}{\mu(A)}$. Given $Y$ another measurable space, $f: X \Markov Y$ and $A$ a measurable subset of $Y$ s.t. $(\mu \ltimes f)(X \times A) > 0$, $\mu \mid f^{-1}(A)$ will denote the probability measure on $X$ defined by 

\[(\mu \mid f^{-1}(A))(B):=(\mu \ltimes f \mid X \times A)(B \times Y)\]

Note that when $f$ is deterministic (i.e. $f_x$ is a Dirac measure for every $x$), this corresponds to conditioning by the inverse image of $A$ with respect to $f$. When $A=\{a\}$ we will use the shorthand notation $\mu \mid f^{-1}(a)$.

Given $X$ a measurable space and $\mu$, $\nu$ probability measures on $X$, $\Dtv(\mu,\nu)$ will denote the total variation distance between $\mu$ and $\nu$.

For $X$ a measurable space and $x \in X$, $\delta_x$ will denote the Dirac measure associated with $x$, i.e. $\delta_x(A)=1$ iff $x \in A$.

For $X$ a discrete measurable space, a probability measure on $X$ can be represented by a function $\mu: X \rightarrow [0,1]$ s.t. $\sum_{x \in X} \mu(x) = 1$. Abusing notation, we will use the same symbol to denote the function and the probability measure.

\subsection{Algorithms}

$\Words$ is the set of all finite binary strings (words), i.e. $\Words:=\bigsqcup_{n \in \Nats} \Bool^n$. For any $x \in \Words$, $\Abs{x}$ is the length of $x$ i.e. $x \in \WordsLen{\Abs{x}}$. For any $n \in \Nats$, $\Bool^{\leq n}:=\{x \in \Words \mid \Abs{x} \leq n\}$, ${\Bool^{>n}:=\{x \in \Words \mid \Abs{x} > n\}}$. For any $x \in \Words$ and $n \in \Nats$, $x_{\leq n}$ stands for the prefix of $x$ of length $n$ if $\Abs{x} \geq n$ and $x$ otherwise. Given $x,y \in \Words$, $xy$ stands for the concatenation of $x$ and $y$ (in particular $\Abs{xy}=\Abs{x}+\Abs{y}$). Given ${n \in \Nats}$ and ${x_0, x_1 \ldots x_{n-1} \in \Words}$, ${\prod_{i \in [n]} x_i}$ is also concatenation. Given $n \in \Nats$ and $x,y \in \WordsLen{n}$, $x \cdot y$ stands for $\bigoplus_{i \in [n]} x_i y_i$. For any $n \in \Nats$, $\Un^n$ is the uniform probability distribution on $\WordsLen{n}$.

Given $n \in \Nats$ and ${x_0, x_1 \ldots x_{n-1} \in \Words}$, $\Chev{x_0,x_1 \ldots x_{n-1}} \in \Words$ denotes the encoding of $(x_0,x_1 \ldots x_{n-1})$ obtained by repeating each bit of $x_0, x_1 \ldots x_{n-1}$ twice and inserting the separators 01.
\begin{definition}

An \emph{encoded set} is a set $X$ together with an injection ${\En_X: X \rightarrow \Words}$ (the encoding) s.t. $\Img \En_X$ is decidable in polynomial time.

\end{definition}

There are standard encodings we implicitly use throughout. $\bm{1}$ denotes an encoded set with 1 element $\bullet$ whose encoding is the empty string. $\Words$ is an encoded set with the trivial encoding ${\En_\Words(x):=x}$. $\Nats$ is an encoded set where $\En_\Nats(n)$ is the binary representation of $n$. $\Rats$ is an encoded set where ${\En_\Rats(\frac{n}{m}):=\Chev{n,m}}$ for an irreducible fraction $\frac{n}{m}$. For any encoded set $X$ and $D \in \textsc{P}$, $\{x \in X \mid \En_X(x) \in D\}$ is an encoded set whose encoding is the restriction of $\En_X$. For $X_0,X_1 \ldots X_{n-1}$ encoded sets, $\prod_{i \in [n]} X_i$ is an encoded set with encoding $\En_{\prod_{i \in [n]} X_i}(x_0,x_1 \ldots x_{n-1}):=\Chev{\En_{X_0}(x_0),\En_{X_1}(x_1) \ldots \En_{X_{n-1}}(x_{n-1})}$. For any $n \in \Nats$ we use the shorthand notation $\En^n:=\En_{(\Words)^n}$.

Given $n \in \Nats$, encoded sets $X_0, X_1 \ldots X_{n-1}$ and encoded set $Y$ we use the notation ${A: \prod_{i \in [n]} X_i \Alg Y}$ to mean a Turing machine with $n$ input tapes that halts on every input for which the $i$-th tape is initialized to a value in $\Img \En_X$ and produces an output in $\Img \En_Y$. Given $\{x_i \in X_i\}_{i \in [n]}$ the notation $A(x_0, x_1 \ldots x_{n-1})$ stands for the unique $y \in Y$ s.t. applying $A$ to the input composed of $\En_{X_i}(x_i)$ results in output $\En_Y(y)$. We use different input tapes for different components of the input instead of encoding the $n$-tuple as a single word in order to allow $A$ to process some components of the input in time smaller than the length of other components. This involves abuse of notation since a Cartesian product of encoded sets is naturally an encoded set, but hopefully this won't cause much confusion.

Given $A: X \Alg Y$ and $x \in X$, $\T_A(x)$ stands for the number of time steps in the computation of $A(x)$.

For any $n \in \Nats$, we fix $\mathcal{U}_n$, a prefix free universal Turing machine with $n+1$ input tapes: 1 program tape and $n$ tapes that serve as input to the program. Given $A: \prod_{i \in [n]} X_i \Alg\ Y$, $\Quote{A} \in \Words$ denotes the corresponding program for $\mathcal{U}_n$.

\section{Fundamentals}
\label{sec:fundamentals}

\subsection{Basic Concepts}

\subsubsection{Distributional Estimation Problems}

We start with a simple model to help build intuition and motivate the following definitions.

Consider finite sets $X$ and $Y$, a probability distribution $\mu: X \rightarrow [0,1]$, a mapping $m: X \rightarrow Y$ and a function $f: X \rightarrow \Reals$. Suppose $x$ was sampled from $\mu$ and we were told $y := m(x)$ (but not told $x$ itself). Our expected value of $f(x)$ in these conditions is ${\E[f(x)] = \E_{x' \sim \mu}[f(x') \mid m(x') = y]}$.

Let $P: X \rightarrow \Reals$ be the function $P(x) := \E_{x' \sim \mu}[f(x') \mid m(x) = m(x)]$. How can we characterize $P$ without referring to the concept of a conditional expected value? For any $Q: X \rightarrow \Reals$ we can consider the \enquote{error} $\E_\mu[(Q - f)^2]$. $Q$ is called \enquote{efficient} when it factors as $Q = q \circ m$ for some $q: Y \rightarrow \Reals$. It is easy to see that $P$ has the least error among all efficient functions.

Note that the characterization of $P$ depends not only on $f$ but also on $\mu$. That is, the accuracy of an estimator depends on the prior probabilities to encounter different questions. In general, we assume that the possible questions are represented by elements of $\Words$. Thus we need to consider a probability distribution on $\Words$. However, in the spirit of average-complexity theory we will only require our estimators to be \emph{asymptotically} optimal. Therefore instead of considering a single probability distribution we consider a family of probability distribution indexed by integer parameters\footnote{It is convenient to allow more than 1 parameter for reasons that will become clear in section~\ref{sec:e_and_u}. Roughly, some parameters represent the complexity of the input whereas other parameters represent the amount of computing resources available for probability estimation.}, where the role of the parameters is defining the relevant limit. We thereby arrive at the following:

\begin{definition}

Fix ${n \in \Nats}$. A \emph{word ensemble of rank ${n}$} is a family $\mu$ of probability distributions ${\{\mu^{K}: \Words \rightarrow [0,1]\}_{K \in \Nats^n}}$.

We will use the notation $\Supp \mu := \bigcup_{K \in \Nats^n} \Supp \mu^K$.

\end{definition}

We now introduce our abstraction for a \enquote{class of mathematical questions} (with quantitative real-valued answers). This abstraction is a trivial generalization of the concept of a distributional decision problem from average-case complexity theory (see e.g. \cite{Bogdanov_2006}).

\begin{definition}

Fix ${n \in \Nats}$. A \emph{distributional estimation problem of rank ${n}$} is a pair $(\mu,f)$ where $\mu$ is a word ensemble of rank ${n}$ and $f: \Supp \mu \rightarrow \Reals$ is bounded.

\end{definition}

\subsubsection{Growth Spaces and \texorpdfstring{$\Gamma$}{Γ}-Schemes}

In the motivational model, the estimator was restricted to lie in a class of functions that factor through a fixed mapping. Of course we are interested in more realistic notions of efficiency. In the present work we consider restrictions on time complexity, access to random bits and size of advice strings. Spatial complexity is also of interest but treating it is out of our current scope. It is possible to consider weaker or stronger restrictions which we represent using the following abstraction:

\begin{samepage}
\begin{definition}

Fix $n$. A \emph{growth space} $\Gamma$ of rank $n$ is a set of functions ${\gamma: \Nats^n \rightarrow \Nats}$ s.t.

\begin{enumerate}[(i)]

\item $0 \in \Gamma$

\item If $\gamma_1, \gamma_2 \in \Gamma$ then $\gamma_1 + \gamma_2 \in \Gamma$.

\item If $\gamma_1 \in \Gamma$, $\gamma_2: \Nats^n \rightarrow \Nats$ and $\forall K \in \Nats^n: \gamma_2(K) \leq \gamma_1(K)$ then $\gamma_2 \in \Gamma$.

\item For any $\gamma \in \Gamma$ there is a polynomial $p: \Nats^n \rightarrow \Nats$ s.t. $\gamma \leq p$.

\end{enumerate}

\end{definition}
\end{samepage}

\begin{example}

For any $n \in \Nats$, $\Gamma_0^n$ is a growth space of rank $n$. $\gamma \in \Gamma_0^n$ iff $\gamma \equiv 0$.

\end{example}

\begin{example}

For any $n \in \Nats$, $\Gamma_{\text{poly}}^n$ is a growth space of rank $n$. $\gamma \in \Gamma_{\text{poly}}^n$ iff there is a polynomial $p: \Nats^n \rightarrow \Nats$ s.t. $\gamma(K) \leq p(K)$.

\end{example}

\begin{example}

For any $n \in \Nats$, $\Gamma_{\text{log}}^n$ is a growth space of rank $n$. $\gamma \in \Gamma_{\text{log}}^n$ iff there is $c \in \Nats$ s.t. $\gamma(K_0, K_1 \ldots K_{n-1}) \leq c \sum_{i \in [n]} \log(K_i+1)$.

\end{example}

We now introduce our notion of an \enquote{efficient} algorithm.

\begin{samepage}
\begin{definition}

Fix $n \in \Nats$ and $\Gamma=(\GrowR$, $\GrowA)$ a pair of growth spaces of rank $n$. Given encoded sets $X$ and $Y$, a \emph{$\Gamma$-scheme of signature $X \rightarrow Y$} is a triple $(S,\R_S,\A_S)$ where ${S: \Nats^n \times X \times \Words^2 \Alg Y}$, $\R_S: \Nats^n \times \Words \Alg \Nats$ and $\A_S: \Nats^n \rightarrow \Words$ are s.t.

\begin{enumerate}[(i)]

\item $\max_{x \in X} \max_{y \in \BoolR{S}} \T_S(K,x,y,\A_S(K)) \in \Gamma_{\text{poly}}^n$

\item $\T_{\R_S}(K,\A_S(K)) \in \Gamma_{\text{poly}}^n$

\item The function $r: \Nats^n \rightarrow n$ defined by $r(K):=\R_S(K,\A_S(K))$ lies in $\GrowR$.

\item $\Abs{\A_S} \in \GrowA$

\end{enumerate}

Abusing notation, we denote the $\Gamma$-scheme $(S,\R_S,\A_S)$ by $S$. $S^K(x,y,z)$ will denote $S(K,x,y,z)$, $S^K(x,y)$ will denote $S(K,x,y,\A_S(K))$ and $S^K(x)$ will denote the $Y$-valued random variable which equals $S(K,x,y,a(K))$ for $y$ sampled from $\Un^{\R_S(K)}$. $\Un_S^K$ will denote $\Un^{\R_S(K)}$. We think of $S$ as a randomized algorithm with advice where $y$ are the internal coin tosses and $\A_S$ is the advice\footnote{Note that the number of random bits $\R_S(K)$ has to be efficiently computable modulo the advice $\A_S(K)$ rather than being an arbitrary function. This requirement is needed to prevent using the function $\R_S$ as advice in itself. In particular, when $\GrowA=\Gamma_0^2$, $S$ represents a uniform randomized algorithm.}. Similarly, $\R_S(K)$ will denote $\R_S(K,\A_S(K))$.

We will use the notation $S: X \Scheme Y$ to signify $S$ is a $\Gamma$-scheme of signature $X \rightarrow Y$.

\end{definition}
\end{samepage}

There is a natural notion of composition for $\Gamma$-schemes.

\begin{samepage}
\begin{definition}

Fix $n \in \Nats$ and $\Gamma=(\GrowR$, $\GrowA)$ a pair of growth spaces of rank $n$. Consider encoded sets $X$, $Y$, $Z$ and $S: X \Scheme Y$, $T: Y \Scheme Z$. Choose a polynomial $p: \Nats^n \rightarrow \Nats$ s.t. $\Abs{\A_S(K)} \leq p(K)$, $\Abs{\A_T(K)} \leq p(K)$, $\R_S(K) \leq p(K)$ and $\R_T(K) \leq p(K)$. We can then construct $U: X \Scheme Z$ s.t. for any $K \in \Nats^n$, $a,b,v,w \in \Bool^{\leq p(K)}$ and $x \in X$

\begin{align}
\A_U(K) &= \Chev{\A_T(K),\A_S(K)} \\
\R_U(K, \Chev{a,b}) &= \R_T(K,a)+\R_S(K,b) \\
U^K(x,vw,\Chev{a,b}) &= T^K(S^K(x,w,b),v,a)
\end{align}

Such a $U$ is called the \emph{composition} of $T$ and $S$ and denoted $U = T \circ S$. There is a slight abuse of notation due to the freedoms in the construction of $U$ but these freedoms have no real significance since all versions of $T \circ S$ induce the same Markov kernel from $X$ to $Z$.

\end{definition}
\end{samepage}

It will also be useful to consider families of $\Gamma$-schemes satisfying uniform resource bounds.

\begin{definition}
\label{def:family}

Fix $n \in \Nats$, $\Gamma=(\GrowR$, $\GrowA)$ a pair of growth spaces of rank $n$ and encoded sets $X$, $Y$. A set $F$ of $\Gamma$-schemes of signature $X \rightarrow Y$ is called a \emph{uniform family} when

\begin{enumerate}[(i)]

\item\label{con:def__family__time} $\max_{S \in F} \max_{x \in X} \max_{y \in \BoolR{S}} \T_S(K,x,y,\A_S(K)) \in \Gamma_{\text{poly}}^n$

\item\label{con:def__family__rtime} $\max_{S \in F}\T_{\R_S}(K,\A_S(K))) \in \Gamma_{\text{poly}}^n$

\item\label{con:def__family__rand} $\max_{S \in F} \R_S \in \GrowR$

\item\label{con:def__family__adv} $\max_{S \in F} \Abs{\A_S(K)} \in \GrowA$

\item There are only finitely many different machines ${S}$ and ${\R_S}$ for ${S \in F}$.

\end{enumerate}

\end{definition}

The details of this definition are motivated by the following proposition.

\begin{proposition}
\label{prp:fam_diag}

Fix $n \in \Nats$ and $\Gamma=(\GrowR$, $\GrowA)$ a pair of growth spaces of rank $n$ s.t. $1 \in \GrowA$. Consider $X$, $Y$ encoded sets, $F$ a uniform family of $\Gamma$-schemes of signature $X \rightarrow Y$ and a collection ${\{\mathcal{S}_K \in F\}_{K \in \Nats^n}}$. Then, there is $\Delta_\mathcal{S}: X \xrightarrow{
\Gamma} Y$ s.t. for any $K \in \Nats^n$, $x \in X$ and $y \in Y$, ${\Pr[\Delta_\mathcal{S}^K(x)=y] = \Pr[\mathcal{S}_K^K(x)=y]}$.

\end{proposition}

\begin{proof}

We take $\A_{\Delta_{\mathcal{S}}}(K):=\Chev{\Quote{\mathcal{S}_K},\Quote{\R_{\mathcal{S}_K}},\A_{\mathcal{S}_K}(K)}$. $\R_{\Delta_{\mathcal{S}}}$ is constructed so that ${\R_{\Delta_{\mathcal{S}}}(K)=\R_{\mathcal{S}_K}(K)}$. $\Delta_{\mathcal{S}}$ is constructed so that $\Delta_{\mathcal{S}}^K(x,z)=\mathcal{S}_K^K(x,z)$.
\end{proof}

\subsubsection{Fall Spaces}

Fix $n \in \Nats$ and $\Gamma$ a pair of growth spaces of rank $n$. Given a distributional estimation problem $(\mu,f)$ and $Q: \Words \Scheme \Rats$, we can consider the estimation error $\E_{(x,y) \sim \mu^{K} \times \Un_Q^K}[(Q^K(x,y) - f(x))^2]$. It makes little sense to require this error to be minimal for every $K \in \Nats^n$, since we can always hard-code a finite number of answers into $Q$ without violating the resource restrictions. Instead we require minimization up to an asymptotically small error. Since it makes sense to consider different kind of asymptotic requirements, we introduce an abstraction that corresponds to this choice.

\begin{definition}
\label{def:fall}

Given $n \in \Nats$, a \emph{fall space of rank $n$} is a set $\Fall$ of bounded functions $\varepsilon: \Nats^n \rightarrow \Reals^{\geq 0}$ s.t.

\begin{enumerate}[(i)]

\item\label{con:def__fall__add} If $\varepsilon_1, \varepsilon_2 \in \Fall$ then $\varepsilon_1 + \varepsilon_2 \in \Fall$.

\item\label{con:def__fall__ineq} If $\varepsilon_1 \in \Fall$, $\varepsilon_2: \Nats^n \rightarrow \Reals^{\geq 0}$ and $\forall K \in \Nats^n: \varepsilon_2(K) \leq \varepsilon_1(K)$ then $\varepsilon_2 \in \Fall$.

\item\label{con:def__fall__pol} There is a polynomial $h: \Nats^n \rightarrow \Nats$ s.t. $2^{-h} \in \Fall$.

\end{enumerate}

Given $f,g: \Nats^n \rightarrow \Reals$, the notation $f(K) \equiv g(K) \pmod \Fall$ means $\Abs{f-g} \in \Fall$. $f(K) \leq g(K) \pmod \Fall$ means $\exists \varepsilon \in \Fall \forall K \in \Nats^n: f(K) \leq g(K) + \varepsilon(K)$. Similarly, $f(K) \geq g(K) \pmod \Fall$ means $\exists \varepsilon \in \Fall \forall K \in \Nats^n: f(K) \geq g(K) - \varepsilon(K)$.

\end{definition}

\begin{example}

We define $\Fall_{\text{neg}}$, a fall space of rank $1$. For any $\varepsilon: \Nats \rightarrow \Reals^{\geq 0}$ bounded, $\varepsilon \in \Fall_{\text{neg}}$ iff for any $d \in \Nats$, $\Lim{k} k^d \varepsilon(k) = 0$.

\end{example}

We note a few simple properties of fall spaces which will be useful in the following.

\begin{proposition}
\label{prp:err_spc_zero}

For any fall space $\Fall$, $0 \in \Fall$.

\end{proposition}

\begin{proof}

Follows from conditions \ref{con:def__fall__ineq} and \ref{con:def__fall__pol}, since $0 \leq 2^{-h}$.
\end{proof}

\begin{proposition}

For any fall space $\Fall$, $\varepsilon \in \Fall$ and $c \in \Reals^{\geq 0}$, $c \varepsilon \in \Fall$.

\end{proposition}

\begin{proof}

By induction, condition~\ref{con:def__fall__add} implies that for any $m \in \Nats$, $m\varepsilon \in \Fall$. It follows that $c\varepsilon \in \Fall$ since $c\varepsilon \leq \Ceil{c}\varepsilon$.
\end{proof}

\begin{proposition}

For any fall space $\Fall$ and $\varepsilon_1, \varepsilon_2 \in \Fall$, $\max(\varepsilon_1,\varepsilon_2) \in \Fall$

\end{proposition}

\proof{$$\max(\varepsilon_1,\varepsilon_2) \leq \varepsilon_1+\varepsilon_2$$}

\begin{proposition}
\label{prp:fall_space_closed_wrt_power}

For any fall space $\Fall$, $\varepsilon \in \Fall$ and $\alpha \in \Reals$, if $\alpha \geq 1$ then $\varepsilon^\alpha \in \Fall$.

\end{proposition}

\begin{proof}

$$\varepsilon^\alpha = (\sup \varepsilon)^\alpha (\frac{\varepsilon}{\sup \varepsilon})^\alpha \leq  (\sup \varepsilon)^\alpha \frac{\varepsilon}{\sup \varepsilon} \in \Fall$$
\end{proof}

\begin{samepage}
\begin{definition}

For any fall space $\Fall$ and $\alpha \in \Reals^{>0}$, we define ${\Fall^\alpha := \{\varepsilon^\alpha \mid \varepsilon \in \Fall\}}$.

\end{definition}
\end{samepage}

\begin{proposition}

Consider $\Fall$ a fall space and $\alpha \in \Reals^{>0}$. Then, $\Fall^\alpha$ is a fall space.

\end{proposition}

\begin{proof}

To check condition~\ref{con:def__fall__add}, consider $\varepsilon_1, \varepsilon_2 \in \Fall$. 

If $\alpha > 1$, $(\varepsilon_1^\alpha + \varepsilon_2^\alpha)^\frac{1}{\alpha} \leq \varepsilon_1 + \varepsilon_2 \in \Fall$ hence $(\varepsilon_1^\alpha + \varepsilon_2^\alpha)^\frac{1}{\alpha} \in \Fall$ and $\varepsilon_1^\alpha + \varepsilon_2^\alpha \in \Fall^\alpha$.

If $\alpha \leq 1$, $(\varepsilon_1^\alpha + \varepsilon_2^\alpha)^\frac{1}{\alpha} = 2^\frac{1}{\alpha}(\frac{\varepsilon_1^\alpha + \varepsilon_2^\alpha}{2})^\frac{1}{\alpha} \leq 2^\frac{1}{\alpha} \frac{\varepsilon_1+\varepsilon_2}{2} \in \Fall$ hence $(\varepsilon_1^\alpha + \varepsilon_2^\alpha)^\frac{1}{\alpha} \in \Fall$ and $\varepsilon_1^\alpha + \varepsilon_2^\alpha \in \Fall^\alpha$.

Conditions \ref{con:def__fall__ineq} and \ref{con:def__fall__pol} are obvious.
\end{proof}

\begin{proposition}

Consider $\Fall$ a fall space and $\alpha_1,\alpha_2 \in \Reals^{>0}$ with $\alpha_1 \leq \alpha_2$. Then, ${\Fall^{\alpha_2} \subseteq \Fall^{\alpha_1}}$.

\end{proposition}

\begin{proof}

Follows from Proposition~\ref{prp:fall_space_closed_wrt_power}.
\end{proof}

\begin{samepage}
\begin{definition}

For any $n \in \Nats$, fall space $\Fall$ of rank $n$ and $\gamma: \Nats^n \rightarrow \Reals$ s.t. $\inf \gamma > 0$, we define $\gamma \Fall := \{\gamma \varepsilon \text{ bounded} \mid \varepsilon \in \Fall\}$.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{proposition}
\label{prp:tbd}

For any $n \in \Nats$, fall space $\Fall$ of rank $n$ and $\gamma: \Nats^n \rightarrow \Reals$ s.t. $\inf \gamma > 0$, $\gamma \Fall$ is a fall space.

\end{proposition}
\end{samepage}

\begin{proof}

Conditions \ref{con:def__fall__add} and \ref{con:def__fall__ineq} are obvious. To verify condition~\ref{con:def__fall__pol} note that for any $\varepsilon \in \Fall$ we have $\frac{\varepsilon}{\gamma} \leq \frac{\varepsilon}{\inf \gamma} \in \Fall$ and therefore $\varepsilon = \gamma \frac{\varepsilon}{\gamma} \in \gamma \Fall$. In particular if $h$ is a polynomial s.t. $2^{-h} \in \Fall$ then $2^{-h} \in \gamma \Fall$.
%
\end{proof}

\subsubsection{Optimal Predictors}

We are now ready to give our central definition, which corresponds to a notion of \enquote{expected value} for distributional estimation problems.

\begin{definition}
\label{def:op}

Fix $n \in \Nats$, $\Gamma$ a pair of growth spaces of rank $n$ and $\Fall$ a fall space of rank $n$. Consider $(\mu,f)$ a distributional estimation problem and $P: \Words \Scheme \Rats$ with bounded range. $P$ is called an \emph{$\EG$-optimal predictor for $(\mu,f)$} when for any $Q: \Words \Scheme \Rats$

\begin{equation}
\label{eqn:op}
\E_{\mu^{K} \times \Un_P^K}[(P^K - f)^2] \leq \E_{\mu^{K} \times \Un_Q^K}[(Q^K - f)^2] \pmod \Fall
\end{equation}

\end{definition}

Distributional \emph{decision} problems are the special case when the range of $f$ is $\Bool$. In this special case, the outputs of an optimal predictors can be thought of as probabilities\footnote{With some caveats. First, $P$ can take values outside $[0,1]$ but it's easy to see that clipping all values to $[0,1]$ preserves optimality. Second, $P^{K}(x,y)=1$ doesn't imply $f(x) = 1$ and $P^{K}(x,y)=0$ doesn't imply $f(x)=0$. We can try to fix this using a logarithmic error function instead of the squared norm, however this creates other difficulties and is outside the scope of the present work.}.

\subsection{Basic Properties}

From now on we fix $n \in \Nats$, $\Grow$ a pair of growth spaces of rank $n$ and $\Fall$ a fall space of rank $n$. All word ensembles and distributional estimation problems will be of rank ${n}$ unless specified otherwise.

In this subsection we discuss some basic properties of optimal predictors which will be used in the following.

\subsubsection{Optimality Relatively to Uniform Families}

Note that $\varepsilon$ in (\ref{eqn:op}) depends on $Q$. However in some sense the optimality condition is automatically uniform w.r.t. the resources required by $Q$.

\begin{proposition}
\label{prp:unif}

Consider $(\mu,f)$ a distributional estimation problem, $P$ an $\EG$-optimal predictor for $(\mu,f)$ and $F$ a uniform family of $\Gamma$-schemes of signature $\Words \rightarrow \Rats$. Then there is $\varepsilon \in \Fall$ s.t. for any $Q \in F$

\begin{equation}
\E_{\mu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2] + \varepsilon(K)
\end{equation}

\end{proposition}

\begin{proof}

For any $K \in \Nats^n$, $\{\E_{\mu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2] \mid Q \in F\}$ is a finite set because $F$ is a uniform family so the runtime of $Q^{K}$ is bounded by a polynomial in $K$ that doesn't depend on $Q$. Therefore we can choose 

\[Q_{K} \in \Argmin{Q \in F} \E_{\mu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2]\]

By Proposition~\ref{prp:fam_diag}, there is $\bar{Q}: \Words \Scheme \Rats$ s.t. $\bar{Q}^{K}(x)$ is distributed the same as $Q_{K}^{K}(x)$.

Since $P$ is an $\EG$-optimal predictor, there is $\varepsilon \in \Fall$ s.t.

\begin{equation}
\label{eqn:prp__unif__prf1}
\E_{\mu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_{\bar{Q}}^{K}}[(\bar{Q}^{K} - f)^2] + \varepsilon(K)
\end{equation}

For any $Q \in F$, we have 

$$\E_{\mu^{K} \times \Un_{\bar{Q}}^{K}}[(\bar{Q}^{K} - f)^2]=\E_{\mu^{K} \times \Un_{Q_{K}}^{K}}[(Q_{K}^{K} - f)^2]$$

\begin{equation}
\label{eqn:prp__unif__prf2}
\E_{\mu^{K} \times \Un_{\bar{Q}}^{K}}[(\bar{Q}^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2]
\end{equation}

Combining \ref{eqn:prp__unif__prf1} and \ref{eqn:prp__unif__prf2} we get the desired result.
\end{proof}

\subsubsection{Random vs. Advice}

As usual, random is no more powerful than advice. This is demonstrated by the following two propositions.

\begin{proposition}

Observe that $\bar{\Gamma}_{\mathfrak{R}}:=\GrowR+\GrowA$ is a growth space and denote $\bar{\Gamma}:=(\bar{\Gamma}_{\mathfrak{R}},\GrowA)$. Consider $(\mu,f)$ a distributional estimation problem and $P$ an $\EG$-optimal predictor for $(\mu,f)$. Then, $P$ is also an $\Fall(\bar{\Gamma})$-optimal predictor for $(\mu,f)$.

\end{proposition}

\begin{proof}

Consider any $Q: \Words \xrightarrow{\bar{\Gamma}} \Rats$. Suppose $\R_Q=r_{\mathfrak{R}}+r_{\mathfrak{A}}$ where $r_{\mathfrak{R}} \in \GrowR$ and $r_{\mathfrak{A}} \in \GrowA$. For any $K \in \Nats^n$, choose 
\[\bar{\A}_Q(K) \in \Argmin{y \in \WordsLen{r_{\mathfrak{A}}(K)}} \E_{(x,z) \sim \mu^{K} \times U^{r_{\mathfrak{R}}(K)}}[(Q^{K}(x,yz) - f(x))^2]\]

As easy to see, there is $\bar{Q}: \Words \Scheme \Rats$ s.t. for all $K \in \Nats^n$, $x \in \Supp \mu^{K}$ and $z \in \WordsLen{r_{\mathfrak{R}}(K)}$, $\R_{\bar{Q}}(K) = r_{\mathfrak{R}}(K)$, ${\A_{\bar{Q}}(K):=\Chev{\A_Q(K),\bar{\A}_Q(K)}}$ and ${\bar{Q}^{K}(x,z)=Q^{K}(x,\bar{\A}_Q(K)z)}$.

It follows that there is $\varepsilon \in \Fall$ s.t.

$$\E_{\mu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \E_{\mu^{K} \times U^{r_{\mathfrak{R}}(K)}}[(\bar{Q}^{K} - f)^2] + \varepsilon(K)$$

Obviously $\E_{\mu^{K} \times U^{r_{\mathfrak{R}}(K)}}[(\bar{Q}^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2]$ therefore

$$\E_{\mu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2] + \varepsilon(K)$$
\end{proof}

\begin{proposition}

Denote $\bar{\Gamma}_{\mathfrak{R}}:=\GrowR+\GrowA$ and $\bar{\Gamma}:=(\bar{\Gamma}_{\mathfrak{R}},\GrowA)$. Consider $(\mu,f)$ a distributional estimation problem and $\bar{P}$ an $\Fall(\bar{\Gamma})$-optimal predictor for $(\mu,f)$. Then, there exists an $\EG$-optimal predictor for $(\mu,f)$.

\end{proposition}

\begin{proof}

Suppose $\R_{\bar{P}}=r_{\mathfrak{R}}+r_{\mathfrak{A}}$ where $r_{\mathfrak{R}} \in \GrowR$ and $r_{\mathfrak{A}} \in \GrowA$. For any ${K \in \Nats^n}$, choose 

\[\bar{\A}_P(K) \in \Argmin{y \in \WordsLen{r_{\mathfrak{A}}(K)}} \E_{(x,z) \sim \mu^{K} \times \Un^{r_{\mathfrak{R}}(K)}}[(\bar{P}^{K}(x,yz) - f(x))^2]\]

We can construct $P: \Words \Scheme \Rats$ so that for all $K \in \Nats^n$, $x \in \Supp \mu^{K}$ and ${z \in \WordsLen{r_{\mathfrak{R}}(K)}}$

\begin{align*}
\R_P(K) &= r_{\mathfrak{R}}(K) \\
\A_P(K) &:=\Chev{\A_{\bar{P}}(K),\bar{\A}_P(K)} \\
P^{K}(x,z) &=\bar{P}^{K}(x,\bar{\A}_P(K)z)
\end{align*}

Clearly ${\E_{\mu^{K} \times \Un^{r_{\mathfrak{R}}(K)}}[(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_{\bar{P}}^{K}}[(\bar{P}^{K} - f)^2]}$ and therefore $P$ is an $\EG$-optimal predictor for $(\mu,f)$.
\end{proof}

\subsubsection{Optimality of Weighted Error}

Although the word ensemble plays a central role in the definition of an optimal predictor, the dependence on the word ensemble is lax in some sense. To see this, consider the following proposition.

\begin{definition}

Given a growth space $\Gamma_*$ of rank $n$, $\Fall$ is called \emph{$\Gamma_*$-ample} when there is $\zeta: \Nats^n \rightarrow (0,\frac{1}{2}]$ s.t.  $\zeta \in \Fall$ and $\Floor{\log \frac{1}{\zeta}} \in \Gamma_*$.

\end{definition}

\begin{proposition}
\label{prp:weight}

Assume $\Fall$ is $\GrowA$-ample. Consider $(\mu,f)$ a distributional estimation problem, $P$ an $\EG$-optimal predictor for $(\mu,f)$, $Q: \Words \Scheme \Rats$ and ${W: \Words \Scheme \Rats^{\geq 0}}$ bounded s.t. ${\R_W \geq \max(\R_P, \R_Q)}$. Denote 

\begin{align*}
\mu^{K}_{P,W} &:=\mu^{K} \times \Un_P^{K} \times \Un^{\R_W(K)-\R_P(K)} \\
\mu^{K}_{Q,W} &:=\mu^{K} \times \Un_Q^{K} \times \Un^{\R_W(K)-\R_Q(K)}
\end{align*}

Then

\begin{equation}
\E_{\mu^{K}_{P,W}}[W^{K}(P^{K} - f)^2] \leq \\ \E_{\mu^{K}_{Q,W}}[W^{K}(Q^{K} - f)^2] \pmod \Fall
\end{equation}

\end{proposition}

To relationship to the role of the word ensemble is as follows.

\begin{samepage}
\begin{corollary}
\label{crl:weight}

Assume $\Fall$ is $\GrowA$-ample. Consider $(\mu,f)$ a distributional estimation problem and $P$ an $\EG$-optimal predictor for $(\mu,f)$. Suppose ${\Gamma^1:=(\GrowR^1,\GrowA^2)}$ is a pair of growth spaces of rank $1$ s.t. for any $\gamma \in \GrowR^1$ the function ${\gamma^2(K):=\gamma(k)}$ is in $\GrowR$ and for any $\gamma \in \GrowA^1$ the function $\gamma^2(K):=\gamma(k)$ is in $\GrowA$. Consider ${W: \Words \xrightarrow{\Gamma^1} \Rats^{\geq 0}}$ bounded s.t. for any $k \in \Nats$ there is $x \in \Supp \mu^k$ and $y \in \WordsLen{\R_W(k)}$ s.t. $W^k(x,y) > 0$. Define 

$${\Fall_W:=\{\varepsilon_W: \Nats^2 \rightarrow \Reals^{\geq 0} \textnormal{ bounded} \mid  \E_{\mu^k \times \Un_W^k}[W^k]\varepsilon_W(K) \in \Fall\}}$$

It is easy to see $\Fall_W$ is a fall space. Define the word ensemble $\nu$ by 

\[\nu^k(x):=\frac{\E_{y \sim \Un_W^k}[W^k(x,y)] \mu^k(x)}{\E_{(x',y) \sim \mu^k \times \Un_W^k}[W^k(x',y)]}\]

Then, $P$ is an $\Fall_W(\Gamma)$-optimal predictor for $(\nu,f)$.

\end{corollary}
\end{samepage}

\begin{proof}

Consider any $Q: \Words \Alg \Rats$. Proposition~\ref{prp:weight} implies there is $\varepsilon \in \Fall$ s.t.

$$\E_{\mu^{K} \times \Un_P^{K} \times \Un_W^{K}}[W^{K}(P^{K} - f)^2] \leq \\ \E_{\mu^{K} \times \Un_Q^{K} \times \Un_W^{K}}[W^{K}(Q^{K} - f)^2] + \varepsilon(K)$$

$$\E_{\mu^{K} \times \Un_P^{K}}[\E_{\Un_W^{K}}[W^{K}](P^{K} - f)^2] \leq \\ \E_{\mu^{K} \times \Un_Q^{K}}[\E_{\Un_W^{K}}[W^{K}](Q^{K} - f)^2] + \varepsilon(K)$$

Dividing both sides of the inequality by $\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(x)]$ we get

$$\E_{\nu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \\ \E_{\nu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2] + \frac{\varepsilon(K)}{\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(x)]}$$

Let $M$ be the supremum of the left hand side.

$$\E_{\nu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \\ \E_{\nu^{K} \times \Un_Q^{K}}[(Q^{K} - f)^2] + \min(\frac{\varepsilon(K)}{\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(x)]},M)$$

The second term on the right hand side is clearly in $\Fall_W$.
\end{proof}

We now give the proof of Proposition~\ref{prp:weight}.

\begin{proof}[Proof of \ref{prp:weight}]

Consider $\zeta: \Nats^n \rightarrow (0,\frac{1}{2}]$ s.t.  $\zeta \in \Fall$ and $\Floor{\log \frac{1}{\zeta}} \in \GrowA$. For any $K \in \Nats^n$ and $t \in \Reals$, let $\rho_\zeta^{K}(t) \in \Argmin{s \in \Rats \cap [t-\zeta(K),t+\zeta(K)]} \Abs{\En_\Rats(s)}$. Denote $M:= \sup W$. It is easy to see that there is $\gamma \in \GrowA$ s.t. for any $t \in [0, M]$, ${\Abs{\En_\Rats(\rho_\zeta^{K}(t))} \leq \gamma(K)}$.

For any $t \in \Reals$ there is $Q_t: \Words \Scheme \Rats$ s.t. $\R_Q=\R_W$ and for any ${x \in \Supp \mu^{K}}$ and ${y \in \WordsLen{\R_W(K)}}$

$$Q_t^{K}(x,y)=\begin{cases}Q^{K}(x,y_{\leq \R_Q(K)}) \text{ if } W^{K}(x,y) \geq \rho^{K}_\zeta(t) \\ P^{K}(x,y_{\leq \R_P(K)}) \text{ if } W^{K}(x,y) < \rho^{K}_\zeta(t)\end{cases}$$

Moreover we can construct the $Q_t$ for all $t \in [0, M]$ s.t. they form a uniform family. By Proposition~\ref{prp:unif} there is $\varepsilon \in \Fall$ s.t. for all $t \in [0, M]$

$$\E_{\mu^{K} \times \Un_P^{K}}[(P^{K}-f)^2] \leq \E_{\mu^{K} \times \Un_W^{K}}[(Q_t^{K}-f)^2] + \varepsilon(K)$$

$$\E_{(x,y) \sim \mu^{K} \times \Un_W^{K}}[(P^{K}(x,y_{\leq \R_P(K)})-f(x))^2-(Q_t^{K}(x,y)-f(x))^2] \leq \varepsilon(K)$$

The expression inside the expected values vanishes when $W^{K}(x,y) < \rho^{K}_\zeta(t)$. In other cases, 
\[Q_t^{K}(x,y) = Q^{K}(x,y_{\leq \R_Q(K)})\]

We get

$$\E_{(x,y) \sim \mu^{K} \times \Un_W^{K}}[\theta(W^{K}(x,y)-\rho_\zeta^{K}(t)) \cdot ((P^{K}(x,y_{\leq \R_P(K)})-f(x))^2-(Q^{K}(x,y_{\leq \R_Q(K)})-f(x))^2)] \leq \varepsilon(K)$$

We integrate both sides of the inequality over $t$ from 0 to $M$.

\begin{equation}
\label{eqn:prp__weight__prf1}
\E[\int_0^M\theta(W^{K}-\rho_\zeta^{K}(t)) \dif t \cdot ((P^{K}-f)^2-(Q^{K}-f)^2)] \leq M \varepsilon(K)
\end{equation}

For any $s \in \Reals$

$$\int_0^M \theta(s-\rho_\zeta^{K}(t)) \dif t = \int_0^{s-\zeta(K)} \theta(s-\rho_\zeta^{K}(t)) \dif t + \int_{s-\zeta(K)}^{s+\zeta(K)} \theta(s-\rho_\zeta^{K}(t)) \dif t + \int_{s+\zeta(K)}^M \theta(s-\rho_\zeta^{K}(t)) \dif t$$

$\Abs{\rho_\zeta^{K}(t)-t} \leq \zeta(K)$ therefore the integrand in the first term is 1 and in the last term 0:

$$\int_0^M \theta(s-\rho_\zeta^{K}(t)) \dif t = \int_0^{s-\zeta(K)} \dif t + \int_{s-\zeta(K)}^{s+\zeta(K)} \theta(s-\rho_\zeta^{K}(t)) \dif t$$

$$\int_0^M \theta(s-\rho_\zeta^{K}(t)) \dif t = s-\zeta(K) + \int_{s-\zeta(K)}^{s+\zeta(K)} \theta(s-\rho_\zeta^{K}(t)) \dif t$$

$$\int_0^M \theta(s-\rho_\zeta^{K}(t)) \dif t - s = \zeta(K) + \int_{s-\zeta(K)}^{s+\zeta(K)} \theta(s-\rho_\zeta^{K}(t)) \dif t$$

\begin{equation}
\label{eqn:prp__weight__prf2}
\int_0^M \theta(s-\rho_\zeta^{K}(t)) \dif t - s \in [0,3\zeta(K)]
\end{equation}

Combining \ref{eqn:prp__weight__prf1} and \ref{eqn:prp__weight__prf2} we conclude that for some $M' \in \Reals^{\geq 0}$

$$\E[W^{K} \cdot ((P^{K}-f)^2-(Q^{K}-f)^2)] \leq M \varepsilon(K) + M'\zeta(K)$$
\end{proof}

\subsection{Orthogonality Theorems}

There is a variant of Definition~\ref{def:op} which is nearly equivalent in many cases and often useful.

We can think of functions $f: \Supp \mu \rightarrow \Reals$ as vectors in a real inner product space with inner product $\Chev{f,g}:=\E_\mu[fg]$. Informally, we can think of $\Gamma$-schemes as a subspace (although a $\Gamma$-scheme is not even a function) and an $\EG$-optimal predictor for $(\mu,f)$ as the nearest point to $f$ in this subspace. Now, given an inner product space $V$, a vector $f \in V$, an actual subspace $W \subseteq V$ and $p = \Argmin{q \in W} \Norm{q - f}^2$, we have $\forall v \in W: \Chev{p-f,v}=0$. This motivates the following:

\begin {definition}

Consider $(\mu,f)$ a distributional estimation problem and ${P: \Words \Scheme \Rats}$ with bounded range. Denote $\mu_P^{K}:=\mu^{K} \times \Un_P^{K}$. $P$ is called an \emph{$\ESG$-optimal predictor for $(\mu,f)$} when for any bounded $S: \Words \times \Rats \Scheme \Rats\footnote{The $\Rats$-valued argument of $S$ is only important for non-trivial $\GrowR$, otherwise we can absorb it into the definition of $S$ using $P$ as a subroutine.}$

\begin{equation}
\label{eqn:op_sharp}
\E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K}(x,y) - f(x))S^{K}(x,P^{K}(x,y),z)] \equiv 0 \pmod \Fall
\end{equation}

\end {definition}

The following theorem is the analogue in our language of the previous fact about inner product spaces.

\begin{theorem}
\label{thm:ort}

Assume there is $\zeta: \Nats^n \rightarrow (0,\frac{1}{4}]$ s.t. $\zeta \in \Fall^{\frac{1}{2}}$ and ${\Floor{\log \log \frac{1}{\zeta}} \in \GrowA}$\footnote{If $\Gamma_{\text{log}}^n \subseteq \GrowA$ then this condition holds for any $\Fall$ since we can take $\zeta = 2^{-h}$ for $h$ polynomial.}. Consider $(\mu,f)$ a distributional estimation problem and $P$ an $\EG$-optimal predictor for $(\mu,f)$. Then, $P$ is also an $\Fall^{\frac{1}{2}\sharp}(\Gamma)$-optimal predictor for $(\mu,f)$.

\end{theorem}

\begin{proof}

Assume without loss of generality that there is a polynomial ${h: \Nats^n \rightarrow \Nats}$ s.t. $\zeta \geq 2^{-h}$ (otherwise we can take any polynomial $h$ s.t. $2^{-h} \in \Fall$ and consider $\zeta':=\zeta+2^{-h}$). Fix $S: \Words \times \Rats \Scheme \Rats$ bounded. Consider any ${\sigma: \Nats^n \rightarrow \{ \pm 1 \}}$ and $m: \Nats^n \rightarrow \Nats$ s.t. $m \leq \log \frac{1}{\zeta}$ (in particular ${m \leq h}$). Define ${t(K) := \sigma(K) 2^{-m(K)}}$. It is easy to see there is ${Q_t: \Words \Scheme \Rats}$ s.t. ${\R_{Q_t}=\R_P+\R_S}$ and given $K \in \Nats^n$, $x \in \Supp \mu^{K}$, ${y \in \WordsLen{\R_P(K)}}$ and ${z \in \WordsLen{ \R_S(K)}}$

$$Q_t^{K}(x,yz) = P^{K}(x,y) + t(K) S^{K}(x,P^{K}(x,y),z)$$

Moreover, we can construct $Q_t$ for all admissible choices of $t$ (but fixed $S$) to get a uniform family.

Applying Proposition~\ref{prp:unif}, we conclude that there is $\varepsilon \in \Fall$ which doesn't depend on $t$ s.t.

$$\E_{\mu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(Q_t^{K} - f)^2] + \varepsilon(K)$$

$$\E_{\mu^{K} \times \Un_P^{K}}[(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} + t(K)S^{K}  - f)^2] + \varepsilon(K)$$

$$\E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} - f)^2 - (P^{K} + t(K)S^{K} - f)^2] \leq \varepsilon(K)$$

$$-\E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(t(K)S^{K} + 2 (P^{K} - f)) S^{K}] t(K) \leq \varepsilon(K)$$

$$-\E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(S^{K})^2] t(K)^2 + 2 \E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} - f) S^{K}] t(K) \leq \varepsilon(K)$$

$$2 \E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} - f) S^{K}] t(K) \leq \E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(S^{K})^2] t(K)^2 + \varepsilon(K)$$

$$2 \E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} - f) S^{K}] t(K) \leq (\sup \Abs{S^{K}})^2 t(K)^2 + \varepsilon(K)$$

$$2 \E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} - f) S^{K}] \sigma(K) 2^{-m(K)} \leq (\sup \Abs{S^{K}})^2 4^{-m(K)} + \varepsilon(K)$$

Multiplying both sides by $2^{m(K)-1}$ we get

$$\E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} - f) S^{K}] \sigma(K) \leq \frac{1}{2}((\sup \Abs{S^{K}})^2 2^{-m(K)} + \varepsilon(K) 2^{m(K)})$$

Let $\sigma(K):=\Sgn \E_{\mu^{K} \times \Un_S^{K}}[(P^{K} - f) S^{K}]$.

$$\Abs{\E_{\mu^{K} \times \Un_P^{K} \times \Un_S^{K}}[(P^{K} - f) S^{K}]} \leq \frac{1}{2}((\sup \Abs{S^{K}})^2 2^{-m(K)} + \varepsilon(K) 2^{m(K)})$$

Let $m(K):=\min(\Floor{\frac{1}{2}\log \max(\frac{1}{\varepsilon(K)},1)},\Floor{\log \frac{1}{\zeta(K)}})$.

$$\Abs{\E[(P^{K} - f) S^{K}]} \leq (\sup \Abs{S^{K}})^2 \max(\min(\varepsilon(K)^{\frac{1}{2}},1),\zeta(K)) + \frac{1}{2}\varepsilon(K) \min(\max(\varepsilon(K)^{-\frac{1}{2}},1),\zeta(K)^{-1})$$

$$\Abs{\E[(P^{K} - f) S^{K}]} \leq (\sup \Abs{S^{K}})^2 \max(\varepsilon(K)^{\frac{1}{2}},\zeta(K)) + \frac{1}{2} \max(\varepsilon(K)^{\frac{1}{2}},\varepsilon(K))$$

The right hand side is obviously in $\Fall^{\frac{1}{2}}$.
\end{proof}

Conversely, we have the following:

\begin{theorem}

Consider $(\mu,f)$ a distributional estimation problem and $P$ an $\ESG$-optimal predictor for $(\mu,f)$. Then, $P$ is also an $\EG$-optimal predictor for $(\mu,f)$.

\end{theorem}

\begin{proof}

Consider any $Q: \Words \Scheme \Rats$. We have

$$\E_{\mu^{K} \times \Un_Q^{K}}[(Q^{K}-f)^2]=\E_{\mu^{K} \times \Un_Q^{K} \times \Un_P^{K}}[(Q^{K}-P^{K}+P^{K}-f)^2]$$

$$\E[(Q^{K}-f)^2]=\E[(Q^{K}-P^{K})^2]+2\E[(Q^{K}-P^{K})(P^{K}-f)]+\E[(P^{K}-f)^2]$$

$$\E[(P^{K}-f)^2]+\E[(Q^{K}-P^{K})^2]=\E[(Q^{K}-f)^2]+2\E[(P^{K}-Q^{K})(P^{K}-f)]$$

$$\E[(P^{K}-f)^2] \leq \E[(Q^{K}-f)^2] + 2\E[(P^{K}-Q^{K})(P^{K}-f)]$$

We can assume $Q$ is bounded without loss of generality since given any $Q$ it easy to construct bounded $\tilde{Q}$ s.t. $\E[(\tilde{Q}^{K}-f)^2] \leq \E[(Q^{K}-f)^2]$. Applying \ref{eqn:op_sharp}, we conclude that there is $\varepsilon \in \Fall$ s.t. \ref{eqn:op} holds.
\end{proof}

\subsection{Simple Example}
\label{sec:fundamentals__one_way}

The concept of an optimal predictor is in some sense complementary to the concept of pseudorandom: a pseudorandom process deterministically produces output that appears random to bounded algorithms whereas optimal predictors compute the moments of the perceived random distributions of the outputs of deterministic processes. To demonstrate this complementarity and give an elementary example of an optimal predictor, we use the concept of a hard-core predicate (which may be regarded as en elementary example of pseudorandom).

\begin{theorem}
\label{thm:hard_core}

Consider $\mu$ a word ensemble of rank ${1}$ s.t. for any different $k,l \in \Nats$, ${\Supp \mu^k \cap \Supp \mu^l = \varnothing}$, ${f: \Supp \mu \rightarrow \Words}$ one-to-one and $B$ a hard-core predicate of $(\mu,f)$. Define ${m: \Supp \mu \rightarrow \Nats}$ by 

\[\forall x \in \Supp \mu^k: m(x):=k\]

For every $k \in \Nats$, define ${\mu_f^k:=f_*^k\mu^k}$.  Finally, define ${\chi_B: \Supp \mu_f \rightarrow \Bool}$ by ${\chi_B(f(x)):=B^{m(x)}(x)}$.

Let $\Gamma:=(\Gamma_{\textnormal{poly}}^1,\Gamma_0^1)$. Let $P: \Words \Scheme \Rats$ satisfy $P \equiv \frac{1}{2}$. Then, $P$ is an $\Fall_{\textnormal{neg}}(\Gamma)$-optimal predictor for $(\mu_f, \chi_B)$.

\end{theorem}

\begin{proof}

Assume to the contrary that $P$ is not optimal. Then there is ${Q: \Words \Scheme \Rats}$, $d \in \Nats$, an infinite set ${I \subseteq \Nats}$ and $\epsilon \in \Reals^{>0}$ s.t.

$$ \forall k \in I: \E_{\mu_f^k}[(\frac{1}{2}-\chi_B)^2] \geq \E_{\mu_f^k \times \Un_Q^k}[(Q^k-\chi_B)^2] +\frac{\epsilon}{k^d}$$

$$ \forall k \in I: \E_{\mu_f^k \times \Un_Q^{k}}[(Q^{k}-\chi_B)^2] \leq \frac{1}{4} - \frac{\epsilon}{k^d} $$

$$ \forall k \in I: \E_{\mu_f^k}[(\E_{\Un_Q^{k}}[Q^{k}]-\chi_B)^2] \leq \frac{1}{4} - \frac{\epsilon}{k^d} $$

There is $G: \Words \xrightarrow{\Gamma} \Bool$ s.t. for all ${x \in \Words}$, 

\[\Abs{\E[Q^{k}(x)]-\Pr[G^k(x)=1]}\leq 2^{-k}\] 

$G^k$ works by evaluating ${\alpha \leftarrow Q^{k}}$ and then returning 1 with probability ${\alpha \pm 2^{-k}}$ and 0 with probability $1-\alpha \pm 2^{-k}$, where the $2^{-k}$ error comes from rounding a rational number to a binary fraction. Denoting 

\[\delta(x):=\E[Q^{k}(x)]-\Pr[G^k(x)=1]\]

we get

$$ \forall k \in I: \E_{\mu_f^k}[(\Prb_{\Un_G^k}[G^k=1]+\delta-\chi_B)^2] \leq \frac{1}{4} - \frac{\epsilon}{k^d} $$

$$ \forall k \in I: \E_{\mu_f^k}[(\Prb_{\Un_G^k}[G^k=1]-\chi_f)^2]+2 \E_{\mu_f^k}[(\Prb_{\Un_G^k}[G^k=1]-\chi_B)\delta]+\E_{\mu_f^k}[\delta^2] \leq \frac{1}{4} - \frac{\epsilon}{k^d}$$

$$ \forall k \in I: \E_{\mu_f^k}[(\Prb_{\Un_G^k}[G^k=1]-\chi_B)^2]-2 \cdot 2^{-k}- 4^{-k} \leq \frac{1}{4} - \frac{\epsilon}{k^d}$$

Since $2^{-k}$ falls faster than $k^{-d}$, there is $I_1 \subseteq \Nats$ infinite and $\epsilon_1 \in \Reals^{>0}$ s.t.

$$ \forall k \in I_1: \E_{\mu_f^k}[(\Prb_{\Un_G^k}[G^k=1]-\chi_B)^2] \leq \frac{1}{4} - \frac{\epsilon_1}{k^d}$$

$$ \forall k \in I_1: \E_{\mu_f^k}[\Abs{\Prb_{\Un_G^k}[G^k=1]-\chi_B}] \leq \sqrt{\frac{1}{4} - \frac{\epsilon_1}{k^d}} $$

$$ \forall k \in I_1: \E_{\mu_f^k}[\Prb_{\Un_G^k}[G^k \ne \chi_B]] \leq \sqrt{\frac{1}{4} - \frac{\epsilon_1}{k^d}} $$

$$ \forall k \in I_1: \E_{x \sim \mu^k}[\Prb_{\Un_G^k}[G^k(f(x)) \ne B^k(x)]] \leq \sqrt{\frac{1}{4} - \frac{\epsilon_1}{k^d}} $$

$$ \forall k \in I_1: \Prb_{\mu^{k} \times \Un_G^k}[G^k(f(x)) \ne B^k(x)] \leq \sqrt{\frac{1}{4} - \frac{\epsilon_1}{k^d}} $$

Since $\sqrt{t}$ is a concave function and the derivative of $\sqrt{t}$ is $\frac{1}{2\sqrt{t}}$, we have $\sqrt{t} \leq \sqrt{t_0} + \frac{t-t_0}{2\sqrt{t_0}}$. We get

$$ \forall k \in I_1: \Prb_{\mu^{k} \times \Un_G^k}[G^k(f(x)) \ne B^k(x)] \leq \frac{1}{2}-\frac{\epsilon_1}{k^d}$$

$$ \forall k \in I_1: \Prb_{\mu^{k} \times \Un_G^k}[G^k(f(x)) = B^k(x)] \geq \frac{1}{2}+\frac{\epsilon_1}{k^d}$$

This contradicts the definition of hard-core predicate.
\end{proof}

\begin{corollary}
\label{crl:one_way}

Consider $f: \Words \Alg \Words$ a one-to-one one-way function. For every ${k \in \Nats}$, define $f^{(k)}: \WordsLen{k} \times \WordsLen{k} \rightarrow \Words$ by ${f^{(k)}(x,y):=\Chev{f(x),y}}$. Define the distributional estimation problem $(\mu_{(f)}, \chi_f)$ by 

\begin{align*}
\mu_{(f)}^k:=f_*^{(k)}(\Un^k \times \Un^k) \\
\chi_f(\Chev{f(x),y}):=x \cdot y
\end{align*}

Let $\Gamma:=(\Gamma_{\textnormal{poly}}^1,\Gamma_0^1)$. Let $P: \Words \Scheme \Rats$ satisfy $P \equiv \frac{1}{2}$. Then, $P$ is an $\Fall_{\textnormal{neg}}(\Gamma)$-optimal predictor for $(\mu_{(f)}, \chi_f)$.

\end{corollary}

\begin{proof}

Follows immediately from Theorem~\ref{thm:hard_core} and Theorem~\ref{thm:goldreich_levin}.
\end{proof}

The following is the non-uniform version of Theorem~\ref{thm:hard_core} which we state without proof since the proof is a straightforward adaptation of the above.

\begin{theorem}
\label{thm:hard_core_circ}

Consider $\mu$ a word ensemble s.t. for any different $k,l \in \Nats$, $\Supp \mu^k \cap \Supp \mu^l = \varnothing$, $f: \Supp \mu \rightarrow \Words$ one-to-one and $B$ a non-uniformly hard-core predicate of $(\mu,f)$. 

Let $\Gamma:=(\Gamma_{\textnormal{poly}}^1,\Gamma_{\textnormal{poly}}^1)$. Let $P: \Words \Scheme \Rats$ satisfy $P \equiv \frac{1}{2}$. Then, $P$ is an $\Fall_{\textnormal{neg}}(\Gamma)$-optimal predictor for $(\mu_f, \chi_B)$.

\end{theorem}

\begin{corollary}

Consider $f: \Words \Alg \Words$ a one-to-one non-uniformly hard to invert one-way function.

Let $\Gamma:=(\Gamma_{\textnormal{poly}}^1,\Gamma_{\textnormal{poly}}^1)$. Let $P: \Words \Scheme \Rats$ satisfy $P \equiv \frac{1}{2}$. Then, $P$ is an $\Fall_{\textnormal{neg}}(\Gamma)$-optimal predictor for $(\mu_f, \chi_f)$.

\end{corollary}

\begin{proof}

Follows immediately from Theorem~\ref{thm:hard_core_circ} and Theorem~\ref{thm:goldreich_levin_circ}.
\end{proof}

\section{Optimal Predictors and Probability Theory}
\label{sec:probability}

\subsection{Calibration}

From a Bayesian perspective, a good probability assignment should be calibrated. For example, suppose there 100 people in a room and you assign each person a probability they are married. If there are 60 people you assigned probabilities in the range 70\%-80\%, the number of married people among these 60 should be close to the interval $60 \times [0.7, 0.8] = [42,48]$. The same requirement can be made for expected value assignments. For example, if you now need to assign an expected value to the age of each person and you assigned an expected age in the range 30-40 to some sufficiently large group of people, the mean age in the group should be close to the interval $[30,40]$. 

We will now show that optimal predictors satisfy an analogous property.

\begin{theorem}
\label{thm:calib}

Assume $\Fall$ is $\GrowA$-ample. Consider $(\mu,f)$ a distributional estimation problem, $P$ an $\EG$-optimal predictor for $(\mu,f)$ and ${W: \Words \Scheme \Rats^{\geq 0}}$ bounded s.t. $\R_W \geq \R_P$ and for every $K \in \Nats^n$ there is $x \in \Supp \mu^{K}$ and $y \in \Un_W^{K}$ with $W^{K}(x,y) > 0$. Denote

\begin{align*}
\alpha(K)&:=\E_{\mu^{K} \times \Un_W^{K}}[W^{K}] \\ 
\delta(K)&:=\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(P^{K}-f)]
\end{align*} 

Then, $\frac{\delta^2}{\alpha} \in \Fall$.

\end{theorem}

To see the relationship between Theorem~\ref{thm:calib} and calibration, consider the following corollary.

\begin{corollary}
\label{crl:calib}

Assume $\Fall$ is $\GrowA$-ample. Consider $(\mu,f)$ a distributional estimation problem, $P$ an $\EG$-optimal predictor for $(\mu,f)$ and $A,B: \bm{1} \Scheme \Rats$ s.t. $\R_A \equiv 0$ and $\R_B \equiv 0$. Denote

\[\alpha(K):=\Prb_{\mu^{K} \times \Un_P^{K}}[A^{K} \leq P^{K} \leq B^{K}]\] 

Then, there is $\varepsilon \in \Fall$ s.t. 

\begin{equation}
\label{eqn:crl__calib}
A^{K} - \sqrt{\frac{\varepsilon(K)}{\alpha(K)}} \leq \E[f \mid A^{K} \leq P^{K} \leq B^{K}] \leq B^{K} + \sqrt{\frac{\varepsilon(K)}{\alpha(K)}}
\end{equation}

\end{corollary}

The appearance of $\alpha$ in the denominator in \ref{eqn:crl__calib} is not surprising since we only expect calibration to hold for large sample size.

We now proceed with the proofs.

\begin{proof}[Proof of Corollary \ref{crl:calib}]

Construct $W: \Words \Scheme \Bool$ s.t. 

\[W^{K}(x,y)=\theta(P^{K}(x,y)-A^{K})\theta(B^{K}-P^{K}(x,y))\] 

Denote $\delta(K):=\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(P^{K}-f)]$ and $\varepsilon:=\frac{\delta^2}{\alpha}$. According to Theorem~\ref{thm:calib}, $\varepsilon \in \Fall$.
We get

$$\frac{\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(P^{K}-f)]^2}{\alpha(K)} = \varepsilon(K)$$

$$\frac{\E_{\mu^{K} \times \Un_W^{K}}[\theta(P^{K}(x,y)-A^{K})\theta(B^{K}-P^{K}(x,y))(P^{K}-f)]^2}{\alpha(K)} = \varepsilon(K)$$

$$\frac{(\E_{\mu^{K} \times \Un_W^{K}}[\theta(P^{K}(x,y)-A^{K})\theta(B^{K}-P^{K}(x,y))]\E[P^{K}-f \mid A^{K} \leq P^{K} \leq B^{K}])^2}{\alpha(K)} = \varepsilon(K)$$

$$\frac{(\alpha(K)\E[P^{K}-f \mid A^{K} \leq P^{K} \leq B^{K}])^2}{\alpha(K)} = \varepsilon(K)$$

$$\alpha(K)\E[P^{K}-f \mid A^{K} \leq P^{K} \leq B^{K}]^2 = \varepsilon(K)$$

\begin{equation}
\label{eqn:crl__calib__prf}
\Abs{\E[P^{K}-f \mid A^{K} \leq P^{K} \leq B^{K}]} = \sqrt{\frac{\varepsilon(K)}{\alpha(K)}}
\end{equation}

On the other hand

$$\E[f \mid A^{K} \leq P^{K} \leq B^{K}] = \E[P^{K}-P^{K}+f \mid A^{K} \leq P^{K} \leq B^{K}]$$

$$\E[f \mid A^{K} \leq P^{K} \leq B^{K}] = \E[P^{K} \mid A^{K} \leq P^{K} \leq B^{K}]-\E[P^{K}-f \mid A^{K} \leq P^{K} \leq B^{K}]$$

Applying \ref{eqn:crl__calib__prf}

$$\E[f \mid A^{K} \leq P^{K} \leq B^{K}] \leq \E[P^{K} \mid A^{K} \leq P^{K} \leq B^{K}]+\sqrt{\frac{\varepsilon(K)}{\alpha(K)}}$$


$$\E[f \mid A^{K} \leq P^{K} \leq B^{K}] \leq B^{K} + \sqrt{\frac{\varepsilon(K)}{\alpha(K)}}$$

In the same manner, we can show that

$$\E[f \mid A^{K} \leq P^{K} \leq B^{K}] \geq A^{K} - \sqrt{\frac{\varepsilon(K)}{\alpha(K)}}$$
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:calib}]

Consider $\zeta: \Nats^n \rightarrow (0,\frac{1}{2}]$ s.t.  $\zeta \in \Fall$ and $\Floor{\log \frac{1}{\zeta}} \in \GrowA$. Define 

\begin{align*}
I&:=\{K \in \Nats^n \mid \frac{\Abs{\delta(K)}}{\alpha(K)} \geq \zeta(K)\} \\
D^{K}&:=\Rats \cap [\frac{\Abs{\delta(K)}}{2\alpha(K)},\frac{\Abs{\delta(K)}}{\alpha(K)}] \\
\epsilon(K) &\in (\Sgn \delta(K)) \cdot \Argmin{t \in D^{K}} \Abs{\En_\Rats(t)}
\end{align*}

It is easy to see that ${\Abs{\En_\Rats(\epsilon)} = O(\log \frac{\alpha}{\Abs{\delta}})}$, hence we can construct $Q: \Words \Scheme \Rats$ s.t. $\R_Q=\R_P$ and for any $(K) \in I$, and $x,y \in \Words$, $Q^{K}(x,y)=P^{K}(x,y)-\epsilon(K)$ (we supply $\epsilon$ as advice for $Q$). Applying Proposition~\ref{prp:weight} to $P$, $Q$ and $W$, we conclude there is $\varepsilon \in \Fall$ s.t.

$$\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_W^{K}}[W^{K}(Q^{K}-f)^2] + \varepsilon(K)$$

$$\E_{\mu^{K} \times \Un_W^{K}}[W^{K}(P^{K} - f)^2] \leq \E_{\mu^{K} \times \Un_W^{K}}[W^{K}(P^{K}-f-\epsilon(K))^2] + \varepsilon(K)$$

$$\E_{\mu^{K} \times \Un_W^{K}}[W^{K}((P^{K} - f)^2 - (P^{K}-f-\epsilon(K))^2] \leq \varepsilon(K)$$

$$ \epsilon(K) \E_{\mu^{K} \times \Un_W^{K}}[W^{K}(2(P^{K} - f) - \epsilon(K))] \leq \varepsilon(K)$$

$$ \epsilon(K) (2 \E_{\mu^{K} \times \Un_W^{K}}[W^{K}(P^{K} - f)]-\E_{\mu^{K} \times \Un_W^{K}}[W^{K}]\epsilon(K)) \leq \varepsilon(K)$$

$$ \epsilon(K) (2 \delta(K) - \alpha(K)\epsilon(K)) \leq \varepsilon(K)$$

Dividing both sides by $ \alpha(K)$ we get

$$\epsilon(K) (\frac{2\delta(K)}{\alpha(K)} - \epsilon(K)) \leq \frac{\varepsilon(K)}{\alpha(K)}$$

$$\frac{\delta(K)^2}{\alpha(K)^2}-(\epsilon(K) - \frac{\delta(K)}{\alpha(K)})^2 \leq \frac{\varepsilon(K)}{\alpha(K)}$$

$\epsilon$ is between $\frac{\delta}{2\alpha}$ and $\frac{\delta}{\alpha}$ therefore $(\epsilon-\frac{\delta}{\alpha})^2 \leq (\frac{\delta}{2\alpha} - \frac{\delta}{\alpha})^2$  which yields

$$\frac{\delta(K)^2}{\alpha(K)^2}-(\frac{\delta(K)}{2\alpha(K)} - \frac{\delta(K)}{\alpha(K)})^2 \leq \frac{\varepsilon(K)}{\alpha(K)}$$

$$\frac{3}{4} \cdot \frac{\delta(K)^2}{\alpha(K)^2} \leq \frac{\varepsilon(K)}{\alpha(K)}$$

$$\frac{\delta(K)^2}{\alpha(K)} \leq \frac{4}{3}\varepsilon(K)$$
\end{proof}

\subsection{Algebraic Properties}

In this subsection and subsection~\ref{subsec:indep_var}, we show that several algebraic identities satisfied by expected values have analogues for optimal predictors.

\subsubsection{Linearity}

Given $F_1,F_2$ random variables and $t_1,t_2 \in \Reals$, we have 

\begin{equation}
\E[t_1 F_1 + t_2 F_2] = t_1 \E[F_1] + t_2 \E[F_2]
\end{equation}

Optimal predictors have an analogous property:

\begin{proposition}
\label{prp:linearity}

Consider $\mu$ a word ensemble, $f_1,f_2: \Supp \mu \rightarrow \Reals$ bounded and $t_1,t_2 \in \Rats$. Denote $f: = t_1 f_1 + t_2 f_2$. Suppose $P_1$ is an $\ESG$-optimal predictor for $(\mu,f_1)$ and $P_2$ is an $\ESG$-optimal predictor for $(\mu,f_2)$. Construct $P: \Words \Scheme \Rats$ s.t. $\R_P(K) = \R_{P_1}(K) + \R_{P_2}(K)$ and for any $x \in \Supp \mu^{K}$, $y_1 \in \WordsLen{\R_{P_1}(K)}$, $y_2 \in \WordsLen{\R_{P_1}(K)}$, $P^{K}(x,y_1 y_2)=t_1 P_1^{K}(x,y_1) + t_2 P_2^{K}(x, y_2)$. Then, $P$ is an $\ESG$-optimal predictor for $(\mu, f)$.

\end{proposition}

\begin{proof}

Consider any bounded $S: \Words \times \Rats \Scheme \Rats$. We have

$$\E[(P^{K} - f)S^{K}] = \E[(t_1 P_1^{K} + t_2 P_2^{K} - (t_1 f_1 + t_2 f_2))S^{K}]$$

$$\E[(P^{K} - f)S^{K}] = t_1 \E[(P_1^{K} - f_1)S^{K}] + t_2 \E[(P_2^{K} - f_2)S^{K}]$$

$$\Abs{\E[(P^{K} - f)S^{K}]} \leq \Abs{t_1} \cdot \Abs{\E[(P_1^{K} - f_1)S^{K}]} + \Abs{t_2} \cdot \Abs{\E[(P_2^{K} - f_2)S^{K}]}$$

Using \ref{eqn:op_sharp} for $P_1$ and $P_2$ we see that the right hand side is in $\Fall$.
\end{proof}

\subsubsection{Conditional Expectation}

Consider a random variable $F$ and an event $A$. Denote $\chi_A$ the $\Bool$-valued random variable corresponding to the indicator function of $A$. We have

\begin{equation}
\label{eqn:cond_ev}
\E[F \mid A] = \frac{\E[\chi_A F]}{\Prb[A]}
\end{equation}

This identity is tautologous if interpreted as a definition of $\E[F \mid A]$. However, from the perspective of Bayesian probability it is more natural to think of $\E[F \mid A]$ as an atomic entity (the subjective expectation of $F$ after observing $A$). 

The language of optimal predictors provides a natural way to define an analogue of conditional expectation. Namely, consider a distributional estimation problem $(\mu, f)$ and a decision problem ${D \subseteq \Words}$. Then, $P: \Words \Scheme \Rats$ represents the conditional expectation of $f$ given $D$ when it is an optimal predictor for $(\mu \mid D, f)$. That is, the conditional expectation is the best estimate of $f(x)$ when the problem instance $x$ is sampled with the \emph{promise} $x \in D$.

The above perspective allows us stating and proving non-tautological theorems analogous to \ref{eqn:cond_ev}. We give two such theorems, corresponding to two different ways to group the variables in \ref{eqn:cond_ev}.

\begin{samepage}
\begin{theorem}
%\label{thm:tbd}

Consider $(\mu, f)$ a distributional estimation problem and ${D \subseteq \Words}$ s.t. for all $k \in \Nats$, $\mu^k(D) > 0$. Define $\gamma_D: \Nats^n \rightarrow \Reals$ by $\gamma(K):=\mu^{K}(D)^{-1}$ and $\Fall_D:=\gamma_D \Fall$. Let $P_D$ be an $\ESG$-optimal predictor for $(\mu, \chi_D)$ and $P_{f \mid D}$ be an $\Fall_D^\sharp(\Gamma)$-optimal predictor for ${(\mu \mid D, f)}$. Construct ${P_{\chi f}: \Words \Scheme \Rats}$ s.t. $\R_{P_{\chi f}}=\R_{P_D} + \R_{P_{f \mid D}}$ and for any $y \in \BoolR{P_D}$ and ${z \in \BoolR{P_{f \mid D}}}$

\begin{equation}
P_{\chi f}^K(x,yz)=P_D^K(x,y) P_{f \mid D}^K(x,z)
\end{equation}

Then, $P_{\chi f}$ is an $\ESG$-optimal predictor for $(\mu, \chi_Df)$.

\end{theorem}
\end{samepage}

\begin{proof}

Consider any $K \in \Nats^n$, $x \in \Supp \mu^{K}$, $y \in \BoolR{P_D}$ and $z \in \BoolR{P_{f \mid D}}$.

\[P_{\chi f}^K(x,yz) - \chi_D(x) f(x) = P_D^K(x,y) P_{f \mid D}^K(x,z) - \chi_D(x) f(x)\]

\[P_{\chi f}^K(x,yz) - \chi_D(x) f(x) = P_D^K(x,y) P_{f \mid D}^K(x,z) - \chi_D(x) P_{f \mid D}^K(x,z) + \chi_D(x) P_{f \mid D}^K(x,z) - \chi_D(x) f(x)\]

\[P_{\chi f}^K(x,yz) - \chi_D(x) f(x) = (P_D^K(x,y) - \chi_D(x)) P_{f \mid D}^K(x,z) + \chi_D(x) (P_{f \mid D}^K(x,z) - f(x))\]

Consider any $S: \Words \times \Rats \Scheme \Rats$ bounded. We get

\[\E_{\mu^{K} \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_{\chi f}^K - \chi_D f)S^K] = \E_{\mu^{K} \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_D^K - \chi_D) P_{f \mid D}^KS^K)] + \E_{\mu^{K} \times \Un_{P_{\chi f}}^K \times \Un_S^K}[\chi_D (P_{f \mid D}^K - f)S^K]\]

Using the fact $P_D^K$ is $\ESG$-optimal for $(\mu,\chi_D)$,

\[\E_{\mu^{K} \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_{\chi f}^K - \chi_D f)S^K] \equiv  \E_{\mu^{K} \times \Un_{P_{\chi f}}^K \times \Un_S^K}[\chi_D (P_{f \mid D}^K - f)S^K] \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_{\chi f}^K - \chi_D f)S^K] \equiv \mu^{K}(D)  \E_{(\mu^{K} \mid D) \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_{f \mid D}^K - f)S^K] \pmod \Fall\]
 
Using the fact $P_{f \mid D}^K$ is $\Fall_D^\sharp(\Gamma)$-optimal for $(\mu \mid D, f)$, we conclude
 
 \[\Abs{\E_{\mu^{K} \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_{\chi f}^K - \chi_D f)S^K]} \equiv 0 \pmod \Fall\]
%
\end{proof}
 
\begin{samepage}
\begin{theorem}
\label{thm:cond}

Consider $(\mu, f)$ a distributional estimation problem and ${D \subseteq \Words}$ s.t. for all $k \in \Nats$, $\mu^k(D) > 0$. Define $\gamma_D: \Nats^n \rightarrow \Reals$ by $\gamma(K):=\mu^{K}(D)^{-1}$ and $\Fall_D:=\gamma_D \Fall$. Let $P_D$ be an $\ESG$-optimal predictor for $(\mu, \chi_D)$ and $P_{\chi f}$ be an $\ESG$-optimal predictor for $(\mu, \chi_D f)$. Choose any $M \in \Rats$ s.t. ${M \geq \sup \Abs{f}}$ and construct $P_{f \mid D}: \Words \Scheme \Rats$ s.t. $\R_{P_{f \mid D}} = \R_{P_D} + \R_{P_{\chi f}}$ and for any ${y \in \BoolR{P_D}}$ and $z \in \BoolR{P_{\chi f}}$ 

\begin{equation}
P_{f \mid D}^K(x,yz)=\begin{cases}P_D^K(x,y)^{-1} P_{\chi f}^K(x,z) \textnormal{ if this number is in } [-M,M] \\ M \textnormal{ if } P_D^K(x,y)=0 \textnormal{ or } P_D^K(x,y)^{-1} P_{\chi f}^K(x,z) > M\\ -M \textnormal{ if } P_D^K(x,y)^{-1} P_{\chi f}^K(x,z) < -M\end{cases}
\end{equation}

Then, $P_{f \mid D}$ is an $\Fall_D^\sharp(\Gamma)$-optimal predictor for $(\mu \mid D, f)$.

\end{theorem}
\end{samepage}

In order to prove Theorem~\ref{thm:cond}, we will need the following.

Consider $s,t \in \Rats$, an $[s,t]$-valued random variable $F$ and an event $A$. Denote $\chi_A$ the $\Bool$-valued random variable corresponding to the indicator function of $A$. We have 

\begin{equation}
\Prb[A]s \leq \E[\chi_A F] \leq \Prb[A]t
\end{equation}

For optimal predictors the analogous inequalities don't have to hold strictly (they only hold within an asymptotically small error), but the following proposition shows they can always be enforced.

\begin{samepage}
\begin{proposition}
\label{prp:thm__cond__lemma}

Consider $(\mu, f)$ a distributional estimation problem, ${D \subseteq \Words}$ and $s, t \in \Rats$ s.t. ${s \leq \inf f}$, $t \geq \sup f$. Let $P_D$ be an $\ESG$-optimal predictor for $(\mu, \chi_D)$ and $P_{\chi f}$ be an $\ESG$-optimal predictor for $(\mu, \chi_D f)$. Construct $\tilde{P}_{\chi f}: \Words \Scheme \Rats$ s.t. $\R_{\tilde{P}_{\chi f}} = \R_{P_D} + \R_{P_{\chi f}}$ and for any ${y \in \BoolR{P_D}}$ and $z \in \BoolR{P_{\chi f}}$, ${\tilde{P}_{\chi f}^K(x,yz)=\min(\max(P_{\chi f}^K(x,z),P_D^K(x,y) s),P_D^K(x,y) t)}$. Denote 

\[\mu_P^K:=\mu^{K} \times \Un_{P_D}^K \times \Un_{P_{\chi f}}^K\] 

Then, for any $S: \Words \times \Rats^2 \Scheme \Rats$ bounded

\begin{equation}
\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K(x)-\chi_D(x)f(x))S^K(x,P_D^K(x),P_{\chi f}^K(x))]  \equiv 0 \pmod \Fall
\end{equation}

In particular, $\tilde{P}$ is also an $\ESG$-optimal predictor for $(\mu, \chi_D f)$.

\end{proposition}
\end{samepage}

\begin{proof}

$P_D$ is an $\ESG$-optimal predictor for $(\mu, \chi_D)$, therefore

\begin{equation}
\label{eqn:thm__cond__lemma__prf1}
\E_{\mu_P^K}[(P_D^K- \chi_D) \theta(P_{\chi f}^K- P_D^K t)] \equiv 0 \pmod \Fall
\end{equation}

$P_{\chi f}$ is an $\ESG$-optimal predictor for $(\mu, \chi_D f)$, therefore

\begin{equation}
\label{eqn:thm__cond__lemma__prf2}
\E_{\mu_P^K}[(P_{\chi f}^K - \chi_D f) \theta(P_{\chi f}^K - P_D^K t)] \equiv 0 \pmod \Fall
\end{equation}

Multiplying \ref{eqn:thm__cond__lemma__prf1} by $t$ and subtracting \ref{eqn:thm__cond__lemma__prf2} we get

\[\E_{\mu_P^K}[(P_D^K t - P_{\chi f}^K - \chi_D \cdot (t - f)) \theta(P_{\chi f}^K- P_D^K t)] \equiv 0 \pmod \Fall\]

\[\E_{\mu_P^K}[(P_D^K t - P_{\chi f}^K ) \theta(P_{\chi f}^K- P_D^K t)] \equiv \E_{\mu_P^K}[\chi_D \cdot (t- f) \theta(P_{\chi f}^K- P_D^K t)] \pmod \Fall\]

The left-hand side is non-positive and the right-hand side is non-negative, therefore

\[\E_{\mu_P^K}[(P_D^K t - P_{\chi f}^K ) \theta(P_{\chi f}^K- P_D^K t)] \equiv 0 \pmod \Fall\]

\begin{equation}
\label{eqn:thm__cond__lemma__prf3}
\E_{\mu_P^K}[(\tilde{P}_{\chi f}^K - P_{\chi f}^K) \theta(P_{\chi f}^K - \tilde{P}_{\chi f}^K)] \equiv 0 \pmod \Fall
\end{equation}

In the same way we can show that

\[\E_{\mu_P^K}[(P_D^K s - P_{\chi f}^K) \theta(P_D^K s-P_{\chi f}^K)] \equiv 0 \pmod \Fall\]

\begin{equation}
\label{eqn:thm__cond__lemma__prf4}
\E_{\mu_P^K}[(\tilde{P}_{\chi f}^K - P_{\chi f}^K) \theta(\tilde{P}_{\chi f}^K-P_{\chi f}^K )] \equiv 0 \pmod \Fall
\end{equation}

Subtracting \ref{eqn:thm__cond__lemma__prf3} from \ref{eqn:thm__cond__lemma__prf4}, we get

\[\E_{\mu_P^K}[(\tilde{P}_{\chi f}^K - P_{\chi f}^K) (\theta(\tilde{P}_{\chi f}^K-P_{\chi f}^K)- \theta(P_{\chi f}^K - \tilde{P}_{\chi f}^K))] \equiv 0 \pmod \Fall\]

\begin{equation}
\label{eqn:thm__cond__lemma__prf5}
\E_{\mu_P^K}[\Abs{\tilde{P}_{\chi f}^K - P_{\chi f}^K}] \equiv 0 \pmod \Fall
\end{equation}

Consider any $S: \Words \times \Rats^2 \Scheme \Rats$ bounded.

\[\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - \chi_Df) S^K(x,P_D^K,P_{\chi f}^K)]=\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - P_{\chi f}^K + P_{\chi f}^K - \chi_Df) S^K(x,P_D^K,P_{\chi f}^K)]\]

\[\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - \chi_D f) S^K]=\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - P_{\chi f}^K) S^K]+\E_{\mu_P^K \times \Un_S^K}[( P_{\chi f}^K - \chi_Df) S^K]\]

Using the fact $P_{\chi f}$ is an $\ESG$-optimal predictor for $(\mu, \chi_D f)$, we get

\[\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - \chi_D f) S^K] \equiv \E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - P_{\chi f}^K) S^K] \pmod \Fall\]

\[\Abs{\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - \chi_D f) S^K]} \leq \E_{\mu_P^K \times \Un_S^K}[\Abs{\tilde{P}_{\chi f}^K - P_{\chi f}^K}] \sup S \pmod \Fall\]

Applying \ref{eqn:thm__cond__lemma__prf5} we conclude that

\[\E_{\mu_P^K \times \Un_S^K}[(\tilde{P}_{\chi f}^K - \chi_D f) S^K] \equiv 0 \pmod \Fall\]
%
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:cond}]

Construct $\tilde{P}_{\chi f}: \Words \Scheme \Rats$ s.t. $\R_{\tilde{P}_{\chi f}} = \R_{P_D} + \R_{P_{\chi f}}$ and for any ${y \in \BoolR{P_D}}$ and $z \in \BoolR{P_{\chi f}}$

\[\tilde{P}_{\chi f}^K(x,yz)=\min(\max(P_{\chi f}^K(x,z),-P_D^K(x,y) M),P_D^K(x,y) M)\] 

For any ${y \in \BoolR{P_D}}$ and $z \in \BoolR{P_{\chi f}}$, we have 

\[
\tilde{P}_{\chi f}^K(x,yz) = P_D^K(x,y) P_{f \mid D}^K(x,yz)\]

\[\tilde{P}_{\chi f}^K(x,yz) - \chi_D(x) f(x) = P_D^K(x,y) P_{f \mid D}^K(x,yz) - \chi_D(x) f(x)\]

\[\tilde{P}_{\chi f}^K(x,yz) - \chi_D(x) f(x) = P_D^K(x,y) P_{f \mid D}^K(x,z) - \chi_D(x) P_{f \mid D}^K(x,yz) + \chi_D(x) P_{f \mid D}^K(x,yz) - \chi_D(x) f(x)\]

\[\tilde{P}_{\chi f}^K(x,yz) - \chi_D(x) f(x) = (P_D^K(x,y) - \chi_D(x)) P_{f \mid D}^K(x,yz) + \chi_D(x) (P_{f \mid D}^K(x,yz) - f(x))\]

\[\chi_D(x) (P_{f \mid D}^K(x,yz) - f(x)) = \tilde{P}_{\chi f}^K(x,yz) - \chi_D(x) f(x) - (P_D^K(x,y) - \chi_D(x)) P_{f \mid D}^K(x,yz)\]

Consider any $S: \Words \times \Rats \Scheme \Rats$ bounded. Denote 

\[\mu_{PS}^K:=\mu^{K} \times \Un_{P_D}^K \times \Un_{P_{\chi f}}^K \times \Un_S^K\]

We have

\[\E_{\mu_{PS}^K}[\chi_D (P_{f \mid D}^K - f)S^K(x,P_{f \mid D}^K)] = E_{\mu_{PS}^K}[(\tilde{P}_{\chi f}^K - \chi_D f)S^K(x,P_{f \mid D}^K)] - E_{\mu_{PS}^K}[(P_D^K - \chi_D) P_{f \mid D}^K S^K(x,P_{f \mid D}^K)]\]

Applying Proposition~\ref{prp:thm__cond__lemma} to the first term on the right-hand side and the fact $P_D^K$ is an $\ESG$-optimal predictor for $(\mu,\chi_D)$ to the second term on the right-hand side,

\[\E_{\mu_{PS}^K}[\chi_D (P_{f \mid D}^K - f)S^K(x,P_{f \mid D}^K)] \equiv 0 \pmod \Fall\]

\[\mu^{K}(D) \E_{(\mu^{K} \mid D)\times \Un_{P_D}^K \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_{f \mid D}^K - f)S^K(x,P_{f \mid D}^K)] \equiv 0 \pmod \Fall\]

\[\E_{(\mu^{K} \mid D)\times \Un_{P_D}^K \times \Un_{P_{\chi f}}^K \times \Un_S^K}[(P_{f \mid D}^K - f)S^K(x,P_{f \mid D}^K)] \equiv 0 \pmod {\Fall_D}\]
%
\end{proof}
\subsection{\texorpdfstring{$\MGrow$}{MΓ}-Schemes and Samplers}

The next subsection and subsequent sections will require several new concepts. Here, we introduce these concepts and discuss some of their properties.

\subsubsection{Congruent Measure Families}

The notation $f(K) \equiv g(K) \pmod \Fall$ can be conveniently generalized from real-valued functions to families of probability measures.

\begin{samepage}
\begin{definition}

Consider a measurable space $X$ and two families of probability measures on $X$: $\{\mu^K\}_{K \in \Nats^n}$ and $\{\nu^K\}_{K \in \Nats^n}$. We say that \emph{$\mu$ is congruent to $\nu$ modulo $\Fall$} when $\Dtv(\mu^K,\nu^K) \in \Fall$. In this case we write $\mu^K \equiv \nu^K \pmod \Fall$ or $\mu \equiv \nu \pmod \Fall$.

\end{definition}
\end{samepage}

Congruence of probability measures modulo $\Fall$ has several convenient properties which follow from elementary properties of total variation distance.

\begin{samepage}
\begin{proposition}
\label{prp:prob_cong_eq}

Congruence of probability measures modulo $\Fall$ is an equivalence relation.

\end{proposition}
\end{samepage}

\begin{proof}

Obvious since $\Dtv$ is a metric.
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:prob_cong_ev}

Consider $X$ a measurable space, $\{\mu^K\}_{K \in \Nats^n}$, $\{\nu^K\}_{K \in \Nats^n}$ families of probability measures on $X$ and $f: X \rightarrow \Reals$ bounded and measurable. Then, $\mu \equiv \nu \pmod \Fall$ implies $\E_{x \sim \mu^K}[f(x)] \equiv \E_{x \sim \nu^K}[f(x)] \pmod \Fall$.

\end{proposition}
\end{samepage}

\begin{proof}

$\Abs{\E_{x \sim \mu^K}[f(x)] - \E_{x \sim \nu^K}[f(x)]} \leq  (\sup f - \inf f)\Dtv(\mu^K, \nu^K)$
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:prob_cong_semidir}

Consider $X$, $Y$ measurable spaces, $\{\mu^K\}_{K \in \Nats^n}$, $\{\nu^K\}_{K \in \Nats^n}$ families of probability measures on $X$ and $f: X \Markov Y$. Then, $\mu \equiv \nu \pmod \Fall$ implies $\mu^K \ltimes f \equiv \nu^K \ltimes f \pmod \Fall$.

\end{proposition}
\end{samepage}

\begin{proof}

Total variation distance is contracted by semi-direct product with a fixed Markov kernel therefore $\Dtv(\mu^K \ltimes f, \nu^K \ltimes f) \leq \Dtv(\mu^K, \nu^K)$.
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:prob_cong_push}

Consider $X$, $Y$ measurable spaces, $\{\mu^K\}_{K \in \Nats^n}$, $\{\nu^K\}_{K \in \Nats^n}$ families of probability measures on $X$ and $f: X \Markov Y$. Then, $\mu \equiv \nu \pmod \Fall$ implies $f_*\mu^K \equiv f_*\nu^K \pmod \Fall$.

\end{proposition}
\end{samepage}

\begin{proof}

Total variation distance is contracted by pushforward therefore \[\Dtv(f_*\mu^K, f_*\nu^K) \leq \Dtv(\mu^K, \nu^K)\]
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:prob_cong_dir}

Consider $X_1$, $X_2$ measurable spaces, $\{\mu_1^K\}_{K \in \Nats^n}$, $\{\nu_1^K\}_{K \in \Nats^n}$ families of probability measures on $X_1$ and $\{\mu_2^K\}_{K \in \Nats^n}$, $\{\nu_2^K\}_{K \in \Nats^n}$ families of probability measures on $X_2$. Then, $\mu_1 \equiv \nu_1 \pmod \Fall$ and $\mu_2 \equiv \nu_2 \pmod \Fall$ imply $\mu_1^K \times \mu_2^K \equiv \nu_1^K \times \nu_2^K \pmod \Fall$. 

\end{proposition}
\end{samepage}

\begin{proof}

Total variation distance is subadditive w.r.t. direct products therefore 

\[\Dtv(\mu_1^K \times \mu_2^K, \nu_1^K \times \nu_2^K) \leq \Dtv(\mu_1^K, \nu_1^K) + \Dtv(\mu_2^K, \nu_2^K)\]
\end{proof}

\subsubsection{\texorpdfstring{$\MGrow$}{MΓ}-Schemes}

The concept of a $\Gamma$-scheme can be generalized in a way which allows the advice to become random in itself.

\begin{samepage}
\begin{definition}

Given encoded sets $X$ and $Y$, an \emph{$\MGrow$-scheme of signature ${X \rightarrow Y}$} is a triple $(S,\R_S,\M_S)$ where $S: \Nats^n \times X \times \Words^2 \Alg Y$, ${\R_S: \Nats^n \times \Words \Alg \Nats}$ and a family of probability distributions $\{\M_S^K: \Words \rightarrow [0,1]\}_{K \in \Nats^n}$ are s.t.

\begin{enumerate}[(i)]

\item $\max_{x \in X} \max_{y \in \BoolR{S}} \max_{z \in \Supp \M_S^K} \T_S(K,x,y,z) \in \Gamma_{\text{poly}}^n$

\item ${\max_{z \in \Supp \M_S^K} \T_{\R_S}(K,z) \in \Gamma_{\text{poly}}^n}$

\item There is $r \in \GrowR$ s.t. for any $K \in \Nats^n$ and $z \in \Supp \M_S^K$, $\R_S(K,z) \leq r(K)$.

\item There is $l \in \GrowA$ s.t. for any $K \in \Nats^n$, $\Supp \M_S^K \subseteq \WordsLen{l(K)}$.

\end{enumerate}

Abusing notation, we denote the $\MGrow$-scheme $(S,\R_S,\M_S)$ by $S$.

$\R_S^K(z)$ will denote $\R_S(K,z)$. $\UM_S^K$ will denote the probability distribution on ${\Words \times \Words}$ given by $\UM_S^K(y,z):= \M_S^K(z) \delta_{\Abs{y},\R_S^K(z)} 2^{-\R_S^K(z)}$.

$S^K(x,y,z)$ will denote $S(K,x,y,z)$. Given $w=(y,z)$, $S^K(x,w)$ will denote $S(K,x,y,z)$. $S^K(x)$ will denote the $Y$-valued random variable which equals $S(K,x,y,z)$ for $(y,z)$ sampled from $\UM_S^K$. $S_x^K$ will denote the probability distribution of this random variable i.e. $S_x^K$ is the push-forward of $\UM_S^K$ by the mapping $(y,z) \mapsto S(K,x,y,z)$.

We think of $S$ as a randomized algorithm with advice which is random in itself. In particular any $\Gamma$-scheme can $S$ can be regarded as an $\MGrow$-scheme with $\M_S^K(z):=\delta_{z\A_S^K}$.

We will use the notation $S: X \MScheme Y$ to signify $S$ is an $\MGrow$-scheme of signature $X \rightarrow Y$.

\end{definition}
\end{samepage}

We introduce composition of ${\MGrow}$-schemes as well.

\begin{samepage}
\begin{definition}

Consider encoded sets $X$, $Y$, $Z$ and $S: X \MScheme Y$, $T: Y \MScheme Z$. Choose a polynomial $p: \Nats^n \rightarrow \Nats$ s.t. $\Supp \UM_S^K \subseteq \Bool^{\leq p(K)} \times \Bool^{\leq p(K)}$ and $\Supp \UM_T^K \subseteq \Bool^{\leq p(K)} \times \Bool^{\leq p(K)}$. We can then construct $U: X \Scheme Z$ s.t. for any $K \in \Nats^n$, $a,b,v,w \in \Bool^{\leq p(K)}$ and $x \in X$

\begin{align}
\M_U^K &= \En_*^2(\M_S^K \times \M_T^K) \\
\R_U(K, \Chev{a,b}) &= \R_T(K,a)+\R_S(K,b) \\
U^K(x,vw,\Chev{a,b}) &= T^K(S^K(x,w,b),v,a)
\end{align}

Such a $U$ is called the \emph{composition} of $T$ and $S$ and denoted $U = T \circ S$.

\end{definition}
\end{samepage}

\subsubsection{Samplers and Samplability}

The concept of a \emph{samplable} word ensemble is commonly used in average-case complexity theory. Here we introduce a relaxation of this concept which allows approximate sampling with an error compatible with the given fall space. We then proceed to introduce samplable distributional estimation problems.

Samplable word ensembles can be thought of as those ensembles which can be produced by a computationally bounded process. Samplable distributional estimation problems can be thought of as those questions that can be efficiently produced together with their answers, like an exam where the examinee cannot easily find the answer but the examinator knows it (even though the examinator is also computationally bounded).

\begin{samepage}
\begin{definition}

A word ensemble $\mu$ is called \emph{$\EMG$-samplable} (resp. \emph{$\EG$-samplable}) when there is an $\MGrow$-scheme (resp. $\Gamma$-scheme) $\sigma$ of signature ${\bm{1} \rightarrow \Words}$  s.t. $\mu^{K} \equiv \sigma_\bullet^K \pmod \Fall$.

In this case, $\sigma$ is called an \emph{$\EMG$-sampler (resp. $\EG$-sampler) of $\mu$}.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}

A distributional estimation problem $(\mu,f)$ is called \emph{$\EMG$-samplable (resp. $\EG$-samplable)} when there is an $\MGrow$-scheme (resp. $\Gamma$-scheme) $G$ of signature $\bm{1} \rightarrow \Words \times \Rats$ s.t. 

\begin{enumerate}[(i)]

\item $G_0$ is an $\EMG$-sampler (resp. $\EG$-sampler) of $\mu$.

\item For any $K \in \Nats^n$, denote $X_{G}^K:=\Supp G_{0\bullet}^K$. For any $x \in \Words$, denote 

$$f_G^K(x):=\begin{cases}\E_{z \sim\UM_G^K}[G^K(z)_1 \mid G^K(z)_0 = x] \text{ if } x \in X_{G}^K \\ 0 \text{ if } x \not\in X_{G}^K \end{cases}$$

We require that the function $\varepsilon(K):=\E_{x \sim \mu^{K}}[\Abs{f_G^K(x)-f(x)}]$ is in $\Fall$.

\end{enumerate}

When $\sup{\Abs{G_1}} < \infty$ (since $f$ is bounded, this can always be assumed without loss of generality), $G$ is called an \emph{$\EMG$-sampler (resp. $\EG$-sampler) of $(\mu,f)$}.

\end{definition}
\end{samepage}

For sufficiently large $\GrowA$ the requirements of $\EMG$-samplability and $\EMG$-samplability become very weak, as seen in the following propositions.

\begin{samepage}
\begin{proposition}
\label{prp:adv_mgamma_smp}

Consider a word ensemble $\mu$ s.t. for some $l \in \GrowA$

\begin{equation}
\label{eqn:prp__adv_mgamma_smp}
\mu^{K}(\Bool^{\leq l(K)}) \equiv 1 \pmod \Fall
\end{equation}

Denote ${I:=\{K \in \Nats^n \mid \mu^{K}(\Bool^{\leq l(K)}) > 0\}}$. Consider ${\sigma: \bm{1} \MScheme \Words}$ s.t. for any ${K \in I}$

\begin{align*}
\M_\sigma^K&:=\mu^{K} \mid \Bool^{\leq l(K)} \\
\sigma^K(y,z)&=z
\end{align*}

Then, $\sigma$ is an $\EMG$-sampler of $\mu$. In particular, since such an $\sigma$ can always be constructed, $\mu$ is $\EMG$-samplable.

\end{proposition}
\end{samepage}

\begin{proof}

$\chi_I \geq \mu^{K}(\Bool^{\leq l(K)})$, $1 - \chi_{I} \leq 1 - \mu^{K}(\Bool^{\leq l(K)})$ and therefore $1 - \chi_I \in \Fall$.

Given $K \in I$, ${\sigma_\bullet^K = \mu^{K} \mid \Bool^{\leq l(K)}}$ and we get

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = \Dtv(\mu^{K}, \mu^{K} \mid \Bool^{\leq l(K)})$$

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = \frac{1}{2} \sum_{x \in \Words} \Abs{\mu^{K}(x)-(\mu^{K} \mid \WordsLen{\leq l(K)})(x)}$$

Denote $\chi^K:=\chi_{\WordsLen{\leq l(K)}}$.

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = \frac{1}{2} \sum_{x \in \Words} \Abs{\mu^{K}(x)-\frac{\chi^K(x)\mu^{K}(x)}{\mu^{K}(\WordsLen{\leq l(K)})}}$$

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = \frac{1}{2} \sum_{x \in \Words} \mu^{K}(x) \Abs{1-\frac{\chi^K(x)}{\mu^{K}(\Bool^{\leq l(K)})}}$$

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = \frac{1}{2} (\sum_{x \in \Bool^{\leq l(K)}} \mu^{K}(x) \Abs{1-\frac{\chi^K(x)}{\mu^{K}(\Bool^{\leq l(K)})}}+\sum_{x \in \Bool^{>l(K)}} \mu^{K}(x) \Abs{1-\frac{\chi^K(x)}{\mu^{K}(\Bool^{\leq l(K)})}})$$

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = \frac{1}{2} (\sum_{x \in \Bool^{\leq l(K)}} \mu^{K}(x)(\frac{1}{\mu^{K}(\Bool^{\leq l(K)})}-1)+\sum_{x \in \Bool^{>l(K)}} \mu^{K}(x))$$

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = \frac{1}{2} (\mu^{K}(\Bool^{\leq l(K)})(\frac{1}{\mu^{K}(\Bool^{\leq l(K)})}-1)+1 - \mu^{K}(\Bool^{\leq l(K)}))$$

$$\Dtv(\mu^{K}, \sigma_\bullet^K) = 1-\mu^{K}(\Bool^{\leq l(K)})$$

Given arbitrary $K \in \Nats^n$,

$$\Dtv(\mu^{K}, \sigma_\bullet^K) \leq \max(1-\mu^{K}(\Bool^{\leq l(K)}), 1-\chi_I)$$
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:adv_mgamma_gen}

Assume $\Fall$ is $\GrowA$-ample. Consider a distributional estimation problem $(\mu,f)$ s.t. for some $l \in \GrowA$, \ref{eqn:prp__adv_mgamma_smp} holds. Then, $(\mu,f)$ is $\EMG$-samplable. 

\end{proposition}
\end{samepage}

\begin{proof}

Consider $\zeta: \Nats^n \rightarrow (0,\frac{1}{2}]$ s.t.  $\zeta \in \Fall$ and $\Floor{\log \frac{1}{\zeta}} \in \GrowA$. For any $K \in \Nats^n$ and ${t \in \Reals}$, let ${\rho^K(t) \in \Argmin{s \in \Rats \cap [t-\zeta(K),t+\zeta(K)]} \Abs{\En_\Rats(s)}}$. For any $K \in \Nats^n$, define ${\alpha^K: \Words \rightarrow \Words}$ by 

\[\alpha^K(x):=\Chev{x,c_\Rats(\rho^K(f(x)))}\]

Denote 

\[I:=\{K \in \Nats^n \mid \mu^{K}(\Bool^{\leq l(K)}) > 0\}\]

Construct ${G: \bm{1} \MScheme \Words \times \Rats}$ s.t. for any $K \in I$

\begin{align*}
\M_G^K:=\alpha_*^K(\mu^{K} \mid \Bool^{\leq l(K)}) \\
G^K(y,\Chev{z,\En_\Rats(t)})=(z,t)
\end{align*}

Let $\sigma$ be an $\EMG$-sampler of $\mu$ from Proposition~\ref{prp:adv_mgamma_smp}. Clearly $\sigma_{\bullet}^K=G_{0\bullet}^K$ therefore $G_0$ is also an $\EMG$-sampler of $\mu$.

Consider any $K \in \Nats^n$. It is easy to see that for any ${x \in \Supp \mu^{K} \cap \Bool^{\leq l(K)}}$, ${f_G^K(x)=\rho^K(f(x))}$ (for $K \not\in I$ this is vacuously true). Also, for any ${x \in \Bool^{>l(K)}}$, $f_G^K(x)=0$. Denote 

\[p^K:=\mu^{K}(\Bool^{\leq l(K)})\]

We get

$$\E_{\mu^{K}}[\Abs{f_G^K(x)-f(x)}]=p^K \E_{\mu^{K}}[\Abs{f_G^K(x)-f(x)} \mid \Abs{x} \leq l(K)] + (1 - p^K)\E_{\mu^{K}}[\Abs{f_G^K(x)-f(x)} \mid \Abs{x} > l(K)]$$

$$\E_{\mu^{K}}[\Abs{f_G^K(x)-f(x)}]=p^K \E_{\mu^{K}}[\Abs{\rho^K(f(x))-f(x)} \mid \Abs{x} \leq l(K)] + (1 - p^K)\E_{\mu^{K}}[\Abs{f(x)} \mid \Abs{x} > l(K)]$$

$$\E_{\mu^{K}}[\Abs{f_G^K(x)-f(x)}] \leq p^K \zeta(K) + (1 - p^K)\sup \Abs{f}$$

The right hand side is obviously in $\Fall$.
\end{proof}

We now introduce the notions of samplability of samplability over a given \enquote{base space} $Y$.

\begin{definition}

Consider a word ensemble $\mu$, an encoded set $Y$ and a family ${\{\pi^K: \Supp \mu^{K} \Markov Y\}_{K \in \Nats^n}}$. $\mu$ is called \emph{$\EMG$-samplable (resp. $\EG$-samplable) relative to $\pi$} when there is an $\MGrow$-scheme (resp. $\Gamma$-scheme) $\sigma$ of signature $Y \rightarrow \Words$ s.t. ${\E_{y \sim \pi_*^K\mu^{K}}[\Dtv(\mu \mid (\pi^K)^{-1}(y),\sigma_y^K)] \in \Fall}$.

In this case, $\sigma$ is called an \emph{$\EMG$-sampler (resp. $\EG$-sampler) of $\mu$ relative to $\pi$}.

\end{definition}

\begin{samepage}
\begin{definition}
\label{def:gen_rel}

Consider a distributional estimation problem $(\mu,f)$, an encoded set $Y$ and a family $\{\pi^K: \Supp \mu^{K} \Markov Y\}_{K \in \Nats^n}$. $(\mu,f)$ is called \emph{$\EMG$-samplable (resp. $\EG$-samplable) relative to $\pi$} when there is an $\MGrow$-scheme (resp. $\Gamma$-scheme) $G$ of signature $Y \rightarrow \Words \times \Rats$ s.t.

\begin{enumerate}[(i)]

\item $G_0$ is an $\EMG$-sampler (resp. $\EG$-sampler) of $\mu$ relative to $\pi$.

\item For any $K \in \Nats^n$, $y \in Y$, Denote $X_{G,y}^K:=\Supp G_{0y}^K$. For any ${x \in \Words}$, denote 

$$f_G^K(x,y):=\begin{cases}\E_{z \sim\UM_G^K}[G^K(y,z)_1 \mid G^K(y,z)_0 = x] \text{ if } x \in X_{G,y}^K \\ 0 \text{ if } x \not\in X_{G,y}^K \end{cases}$$

We require that the function ${\varepsilon(K):=\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_G^K(x,y)-f(x)}]}$ is in $\Fall$.

\end{enumerate}

When $\sup{\Abs{G_1}} < \infty$, $G$ is called an \emph{$\EMG$-sampler (resp. $\EG$-sampler) of $(\mu,f)$ relative to $\pi$}.

\end{definition}
\end{samepage}

Note that relative samplability reduces to absolute (ordinary) samplability when $Y=\bm{1}$.

The following propositions are basic properties of samplable ensembles and problems which often come in handy.

\begin{samepage}
\begin{proposition}
\label{prp:smp}

Consider a word ensemble $\mu$, an encoded set $Y$, a family ${\{\pi^K: \Supp \mu^{K} \Markov Y\}_{K \in \Nats^n}}$ and ${h: (\Supp \mu) \times Y \rightarrow \Reals}$ bounded. Suppose $\sigma$ is an $\EMG$-sampler of $\mu$ relative to $\pi$. Then

\begin{equation}
\label{eqn:prp__smp}
\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[h(x,y)] \equiv \E_{(y,z) \sim \pi_*^K\mu^{K} \times \UM_\sigma^K}[h(\sigma^K(y,z),y)] \pmod \Fall
\end{equation}

\end{proposition}
\end{samepage}

\begin{proof}

If we sample $(x,y)$ from $\mu^{K} \ltimes \pi^K$ and then sample $x'$ from ${\mu^{K} \mid (\pi^K)^{-1}(y)}$, $(x',y)$ will obey the distribution $\mu^{K} \ltimes \pi^K$. Denote $\mu_y^K:=\mu^{K} \mid (\pi^K)^{-1}(y)$. We get

$$\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[h(x,y)] = \E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\E_{x' \sim \mu_y^K}[h(x',y)]]$$

$$\E_{\mu^{K} \ltimes \pi^K}[h(x,y)] - \E_{\pi_*^K\mu^{K} \times \UM_\sigma^K}[h(\sigma^K(y,z),y)] = \E_{\mu^{K} \ltimes \pi^K}[\E_{\mu_y^K}[h(x',y)]] - \E_{\pi_*^K\mu^{K} \times \UM_\sigma^K}[h(\sigma^K(y,z),y)]$$

$$\E_{\mu^{K} \ltimes \pi^K}[h(x,y)] - \E_{\pi_*^K\mu^{K} \times \UM_\sigma^K}[h(\sigma^K(y,z),y)] = \E_{\mu^{K} \ltimes \pi^K}[\E_{\mu_y^K}[h(x',y)]-\E_{\UM_\sigma^K}[h(\sigma^K(y,z),y)]]$$

$$\E_{\mu^{K} \ltimes \pi^K}[h(x,y)] - \E_{\pi_*^K\mu^{K} \times \UM_\sigma^K}[h(\sigma^K(y,z),y)] = \E_{\mu^{K} \ltimes \pi^K}[\E_{\mu_y^K}[h(x',y)]-\E_{\sigma_y^K}[h(x',y)]]$$

$$\Abs{\E_{\mu^{K} \ltimes \pi^K}[h(x,y)] - \E_{\pi_*^K\mu^{K} \times \UM_\sigma^K}[h(\sigma^K(y,z),y)]} \leq \E_{\mu^{K} \ltimes \pi^K}[\Abs{\E_{\mu_y^K}[h(x',y)]-\E_{\sigma_y^K}[h(x',y)]}]$$

$$\Abs{\E_{\mu^{K} \ltimes \pi^K}[h(x,y)] - \E_{\pi_*^K\mu^{K} \times \UM_\sigma^K}[h(\sigma^K(y,z),y)]} \leq (\sup h - \inf h) \E_{\mu^{K} \ltimes \pi^K}[\Dtv(\mu_y^K,\sigma_y^K)]$$

Using the defining property of $\sigma$, we get the desired result.
\end{proof}

\begin{proposition}
\label{prp:gen}

Consider a distributional estimation problem $(\mu,f)$, an encoded set $Y$, a family ${\{\pi^K: \Supp \mu^{K} \Markov Y\}_{K \in \Nats^n}}$ and ${h: (\Supp \mu) \times Y \rightarrow \Reals}$ bounded. Denote $\mu_\pi^K:=\mu^{K} \ltimes \pi^K$. Suppose $G$ is an $\EMG$-sampler of $(\mu,f)$ relative to $\pi$. Then

\begin{equation}
\E_{\mu_\pi^K}[h(x,y)f(x)] \equiv \E_{\pi_*^K\mu^{K} \times \UM_G^K}[h(G^K(y,z)_0,y)G^K(y,z)_1] \pmod \Fall
\end{equation}

\end{proposition}

\begin{proof}

$$\E_{\mu_\pi^K}[h(x,y)f(x)]-\E_{\mu_\pi^K}[h(x,y)f_G^K(x,y)]=\E_{\mu_\pi^K}[h(x,y)(f(x)-f_G^K(x,y))]$$

$$\Abs{\E_{\mu_\pi^K}[h(x,y)f(x)]-\E_{\mu_\pi^K}[h(x,y)f_G^K(x,y)]} \leq \E_{\mu_\pi^K}[\Abs{h(x,y)} \cdot \Abs{f(x)-f_G^K(x,y)}]$$

$$\Abs{\E_{\mu_\pi^K}[h(x,y)f(x)]-\E_{\mu_\pi^K}[h(x,y)f_G^K(x,y)]} \leq (\sup \Abs{h}) \E_{\mu_\pi^K}[\Abs{f(x)-f_G^K(x,y)}]$$

By property (ii) of Definition~\ref{def:gen_rel}

$$\E_{\mu_\pi^K}[h(x,y)f(x)] \equiv \E_{\mu_\pi^K}[h(x,y)f_G^K(x,y)] \pmod \Fall$$

Using property (i) of Definition~\ref{def:gen_rel} we can apply Proposition~\ref{prp:smp} to the second term and get

$$\E_{\mu_\pi^K}[h(x,y)f(x)] \equiv \E_{\pi_*^K\mu^{K} \times \UM_G^K}[h(G^K(y,z)_0,y) f_G^K(G^K(y,z)_0,y)] \pmod \Fall$$

$$\E_{\mu_\pi^K}[h(x,y)f(x)] \equiv \E_{\pi_*^K\mu^{K} \times \UM_G^K}[h(G^K(y,z)_0,y) \E_{z' \sim\UM_G^K}[G^K(y,z')_1 \mid G^K(y,z')_0 = G^K(y,z)_0]] \pmod \Fall$$

$$\E_{\mu_\pi^K}[h(x,y)f(x)] \equiv \E_{\pi_*^K\mu^{K} \times \UM_G^K}[h(G^K(y,z)_0,y) G^K(y,z)_1] \pmod \Fall$$
\end{proof}

\subsection{Independent Variables}
\label{subsec:indep_var}

Independent random variables $F_1, F_2$ satisfy 

\begin{equation}
\label{eqn:ev_mult}
\E[F_1 F_2] = \E[F_1] \E[F_2]
\end{equation}

To formulate an analogous property for optimal predictors, we need a notion of independence for distributional decision problems which doesn't make the identity tautologous. Consider distributional decision problems $(\mu, f_1)$, $(\mu, f_2)$. Informally, $f_1$ is \enquote{independent} of $f_2$ when learning the value of $f_2(x)$ provides no efficiently accessible information about $f_1(x)$. In the present work, we won't try to formalise this in full generality. Instead, we will construct a specific scenario in which the independence assumption is justifiable.

We start with an informal description. Suppose that $f_1(x)$ depends only on part $\pi(x)$ of the information in $x$ i.e. $f_1(x) = g(\pi(x))$. Suppose further that given $y=\pi(x)$ it is possible to efficiently produce samples $x'$ of $\mu \mid \pi^{-1}(y)$ for which $f_2(x')$ is known. Then, the knowledge of $f_2(x)$ doesn't provide new information about $g(\pi(x))$ since equivalent information can be efficiently produced without this knowledge.
 Moreover, if we can only efficiently produce samples $x'$ of $\mu \mid \pi^{-1}(y)$ together with $\tilde{f}_2(x')$ an \emph{unbiased estimate} of $f_2(x')$, we still expect the analogue of \ref{eqn:ev_mult} to hold since the expected value of $\tilde{f}_2(x') - f_2(x')$ vanishes for any given $x'$ so it is uncorrelated with $f_1(x)$.
 
The following theorem formalises this setting.

\begin{samepage}
\begin{theorem}
\label{thm:mult}

Consider $\mu$ a word ensemble, $f_1, f_2: \Supp \mu \rightarrow \Reals$ bounded, $(\nu,g)$ a distributional estimation problem and $\pi: \Words \Scheme \Words$. Assume the following conditions:

\begin{enumerate}[(i)]

\item\label{con:thm__mult__dist} $\pi_*^{K}(\mu^{K}) \equiv \nu^{K} \pmod \Fall$

\item\label{con:thm__mult__fun} Denote ${\bar{g}: \Words \rightarrow \Reals}$ the extension of $g$ by $0$.  We require ${\E_{(x,z) \sim \mu^{K} \times \Un_\pi^{K}}[\Abs{f_1(x)-\bar{g}(\pi^{K}(x,z))}] \in \Fall}$

\item\label{con:thm__mult__smp} $(\mu, f_2)$ is $\EMG$-samplable relative to $\pi$.

\end{enumerate}

Suppose $P_1$ is an $\ESG$-optimal predictor for $(g,\nu)$ and $P_2$ is an $\ESG$-optimal predictor for $(\mu,f_2)$. Denote $P_\pi := P_1 \circ \pi$. Construct ${P: \Words \Scheme \Rats}$ s.t. $\R_P=\R_{P_\pi}+\R_{P_2}$ and for any $z_1 \in \WordsLen{\R_{P_\pi}(K)}$ and $z_2 \in \WordsLen{\R_{P_2}(K)}$

\begin{equation}
P^{K}(x,z_1 z_2)=P_\pi^{K}(x,z_1) P_2^{K}(x,z_2)
\end{equation}

Then, $P$ is an $\ESG$-optimal predictor for $(\mu,f_1 f_2)$.

\end{theorem}
\end{samepage}

In order to prove Theorem~\ref{thm:mult} we will need the following proposition.

\begin{samepage}
\begin{proposition}
\label{prp:mixed_ort}

Consider $(\mu,f)$ a distributional estimation problem, $P$ an $\ESG$-optimal predictor for $(\mu,f)$ and $S: \Words \times \Rats \MScheme \Rats$ bounded. Then

\begin{equation}
\label{eqn:prp__mixed_ort}
\E_{\mu^{K} \times \Un_P^{K} \times \UM_S^{K}}[(P^{K}(x,y) - f(x))S^{K}(x,P^{K}(x,y),z,w)] \equiv 0 \pmod \Fall
\end{equation}

\end{proposition}
\end{samepage}

\begin{proof}

For any $K \in \Nats^n$, choose 

$$w^{K} \in \Argmax{w \in \Supp \M_S^{K}} \Abs{\E_{\mu^{K} \times \Un_P^{K} \times \Un^{\R_S^{K}(w)}}[(P^{K}(x,y) - f(x))S^{K}(x,P^{K}(x,y),z,w)]}$$

Construct $\bar{S}: \Words \times \Rats \Scheme \Rats$ s.t. 

\begin{align*}
\R_{\bar{S}}(K)&=\R_S^{K}(w^{K}) \\
\bar{S}^{K}(x,t,z)&=S^{K}(x,t,z,w^{K})
\end{align*}

$P$ is an $\ESG$-optimal predictor for $(\mu,f)$, therefore

$$\E_{\mu^{K} \times \Un_P^{K} \times \Un_{\bar{S}}^{K}}[(P^{K}(x,y) - f(x))\bar{S}^{K}(x,P^{K}(x,y),z)] \equiv 0 \pmod \Fall$$

$$\E_{\mu^{K} \times \Un_P^{K} \times \Un^{\R_S^{K}(w)}}[(P^{K}(x,y) - f(x))S^{K}(x,P^{K}(x,y),z,w^{K})] \equiv 0 \pmod \Fall$$

By construction of $w^{K}$, the absolute value of the left hand side is no less than the absolute value of the left hand side in \ref{eqn:prp__mixed_ort}.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:mult}]

Consider $K \in \Nats^n$, $x \in \Supp \mu^{K}$, $z_1 \in \WordsLen{\R_{P_1}(K)}$, ${z_2 \in \WordsLen{\R_{P_2}(K)}}$ and ${z_3 \in \WordsLen{\R_\pi(K)}}$.

\[P^{K}(x,z_1 z_3 z_2)-f_1(x)f_2(x)=P_\pi^{K}(x, z_1 z_3) P_2^{K}(x,z_2) - f_1(x) f_2(x)\]

Adding and subtracting $P_\pi^{K}(x, z_1 z_3) f_2(x)$ from the right hand side and grouping variables, we get

\[P^{K}(x,z_1 z_3 z_2)-f_1(x)f_2(x)=P_\pi^{K}(x, z_1 z_3)(P_2^{K}(x,z_2)-f_2(x))+(P_\pi^{K}(x, z_1 z_3)-f_1(x))f_2(x)\]

For any bounded $S: \Words \times \Rats \Scheme \Rats$ we get

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E[(P_2^{K}-f_2) P_\pi^{K} S^{K}]} + \Abs{\E[(P_\pi^{K}-f_1)f_2 S^{K}]}$$

$P_2$ is an $\ESG$-optimal predictor for $(\mu,f_2)$ therefore the first term on the right hand side is in $\Fall$.

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E[(P_\pi^{K}-f_1)f_2 S^{K}]} \pmod \Fall$$

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E[(P_\pi^{K}-f_1)f_2 S^{K}] - \E[(P_\pi^{K}-\bar{g} \circ \pi^{K})f_2 S^{K}] + \E[(P_\pi^{K}-\bar{g} \circ \pi^{K})f_2 S^{K}]} \pmod \Fall$$

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E[(P_\pi^{K}-f_1)f_2 S^{K}] - \E[(P_\pi^{K}-\bar{g} \circ \pi^{K})f_2 S^{K}]} + \Abs{\E[(P_\pi^{K}-\bar{g} \circ \pi^{K})f_2 S^{K}]} \pmod \Fall$$

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E[(\bar{g} \circ \pi^{K}-f_1)f_2 S^{K}]} + \Abs{\E[(P_\pi^{K}-\bar{g} \circ \pi^{K})f_2 S^{K}]} \pmod \Fall$$

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq (\sup \Abs{f_2}) (\sup \Abs{S}) \E[\Abs{\bar{g} \circ \pi^{K} - f_1}] + \Abs{\E[(P_\pi^{K}-\bar{g} \circ \pi^{K})f_2 S^{K}]} \pmod \Fall$$

Condition~\ref{con:thm__mult__fun} implies the first term on the right hand side is in $\Fall$.

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E[(P_\pi^{K}-\bar{g} \circ \pi^{K})f_2 S^{K}]} \pmod \Fall$$

Denote $\Un_{\text{tot}}^{K}:= \Un_{P_1}^{K} \times \Un_{P_2}^{K} \times \Un_S^{K}$. We change variables inside the expected value on the right hand side by $y:=\pi^{K}(x,z_3)$. Observing that $(x,y)$ obeys the distribution $\mu^{K} \ltimes \pi^{K}$ we get

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E_{\mu^{K} \ltimes \pi^{K} \times \Un_{\text{tot}}^{K}}[(P_1^{K}(y,z_1)-\bar{g}(y))f_2(x) S^{K}(x,P_1^{K}(y,z_1)P_2^{K}(x,z_2), z_4)]} \pmod \Fall$$

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E_{\mu^{K} \ltimes \pi^{K}}[\E_{\Un_{\text{tot}}^{K}}[(P_1^{K}(y,z_1)-\bar{g}(y))S^{K}(x,P_1^{K}(y,z_1)P_2^{K}(x,z_2), z_4)]f_2(x)]} \pmod \Fall$$

Let $G$ be an $\EMG$-sampler of $(\mu,f_2)$ relative to $\pi$. Applying Proposition~\ref{prp:gen} to the right hand side we get

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E_{\pi_*^K\mu^{K} \times \UM_G^{K}}[\E[(P_1^{K}(y)-\bar{g}(y))S^{K}(G^{K}(y)_0,P_1^{K}(y)P_2^{K}(G^{K}(y)_0))]G^{K}(y)_1]} \pmod \Fall$$

Using condition~\ref{con:thm__mult__dist} we conclude that

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E_{\nu^k \times \UM_G^{K}}[\E[(P_1^{K}(y)-g(y))S^{K}(G^{K}(y)_0,P_1^{K}(y)P_2^{K}(G^{K}(y)_0))]G^{K}(y)_1]} \pmod \Fall$$

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \leq \Abs{\E_{\nu^k \times \Un_{\text{tot}}^{K} \times \UM_G^{K}}[(P_1^{K}(y)-g(y))S^{K}(G^{K}(y)_0,P_1^{K}(y)P_2^{K}(G^{K}(y)_0))G^{K}(y)_1]} \pmod \Fall$$

By Proposition~\ref{prp:mixed_ort}, this implies

$$\Abs{\E[(P^{K}-f_1 f_2)S^{K}]} \equiv 0 \pmod \Fall$$
\end{proof}

The following corollary demonstrates one natural scenario in which the conditions of Theorem~\ref{thm:mult} hold.

\begin{samepage}
\begin{corollary}
\label{crl:dir_prod}

Consider $(\mu_1,f_1)$, $(\mu_2,f_2)$ distributional estimation problems. Suppose $P_1$ is an $\ESG$-optimal predictor for $(\mu_1,f_1)$, $P_2$ is an $\ESG$-optimal predictor for $(\mu_2,f_2)$, $\sigma$ is an $\EMG$-sampler for $\mu_1$ and $G$ is an $\EMG$-sampler for $(\mu_2,f_2)$. Define ${\mu^{K}:=\En_*^2(\mu_1^k \times \mu_2^k)}$. Define ${f: \Supp \mu \rightarrow \Reals}$ by ${f(\Chev{x_1,x_2}):=f_1(x_1)f_2(x_2)}$. Then, there is $P$, an $\ESG$-optimal predictor for $(\mu,f)$, s.t. $\R_P=\R_{P_1}+\R_{P_2}$ and for any $K \in \Nats^n$, $x_1 \in \Supp \sigma_\bullet^{K}$, $x_2 \in \Words$, $z_1 \in \WordsLen{\R_{P_1}(K)}$ and $z_2 \in \WordsLen{\R_{P_2}(K)}$

\begin{equation}
P^{K}(\Chev{x_1,x_2}, z_1 z_2)=P_1^{K}(x_1,z_1) P_2^{K}(x_2,z_2)
\end{equation}

\end{corollary}
\end{samepage}

In order to prove Corollary~\ref{crl:dir_prod}, we'll need the following propositions

\begin{samepage}
\begin{proposition}
\label{prp:thm__mult__cond1}

Consider $\mu_1$, $\mu_2$ word ensembles and $\sigma_1$, $\sigma_2$ which are $\EMG$-samplers for $\mu_1$ and $\mu_2$ respectively. Define ${\mu^k:=\En_*^2(\mu_1^k \times \mu_2^k)}$. Suppose $\pi: \Words \Scheme \Words$ is s.t. for any $K \in \Nats^n$, $x_1 \in \Supp \sigma_{1\bullet}^{K}$, ${x_2 \in \Supp \sigma_{2\bullet}^{K}}$ and $z \in \Bool^{\R_\pi(K)}$, $\pi^{K}(\Chev{x_1,x_2},z)=x_1$. Then $\pi_*^K\mu^{K} \equiv \mu_1^{K} \pmod \Fall$

\end{proposition}
\end{samepage}

\begin{proof}

$\sigma_{1\bullet}^{K} \equiv \mu_1^{K} \pmod \Fall$ and $\sigma_{2\bullet}^{K} \equiv \mu_2^{K} \pmod \Fall$.  By Proposition~\ref{prp:prob_cong_dir},

\[\sigma_{1\bullet}^{K} \times \sigma_{2\bullet}^{K} \equiv \mu_1^{K} \times \mu_2^{K} \pmod \Fall\]

Denote $\mu_\sigma^{K}:=\En_*^2(\sigma_{1\bullet}^{K} \times \sigma_{2\bullet}^{K})$. We get ${\mu_\sigma^{K} \equiv \mu^{K} \pmod \Fall}$ and therefore ${\pi_*^K\mu_\sigma^{K} \equiv \pi_*^K\mu^{K} \pmod \Fall}$ (by Proposition~\ref{prp:prob_cong_push}). Obviously $\pi_*^K\mu_\sigma^{K}=\sigma_{1\bullet}^{K}$. We conclude that ${\pi_*^K\mu^{K} \equiv \sigma_{1\bullet}^{K} \pmod \Fall}$ and therefore ${\pi_*^K\mu^{K} \equiv \mu_1 \pmod \Fall}$ (by Proposition~\ref{prp:prob_cong_eq}).
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:thm__mult__cond2}

Consider $\mu_1$, $\mu_2$ word ensembles and $\sigma_1$, $\sigma_2$ which are $\EMG$-samplers for $\mu_1$ and $\mu_2$ respectively. Suppose $\pi: \Words \Scheme \Words$ is s.t. for any $K \in \Nats^n$, $x_1 \in \Supp \sigma_{1\bullet}^{K}$, ${x_2 \in \Supp \sigma_{2\bullet}^{K}}$ and $z \in \Bool^{\R_\pi(K)}$, $\pi^{K}(\Chev{x_1,x_2},z)=x_1$. Then, for any $g: \Supp \mu_1 \rightarrow \Reals$ bounded and ${\bar{g}: \Words \rightarrow \Reals}$ its extension by ${0}$, we have 

$$\E_{(x_1,x_2,z) \sim\mu_1^{K} \times \mu_2^{K} \times \Un_\pi^{K}}[\Abs{g(x_1)-\bar{g}(\pi^{K}(\Chev{x_1,x_2},z))}] \in \Fall$$

\end{proposition}
\end{samepage}

\begin{proof}

Denote $M:= \sup g - \inf g$.

$$\E[\Abs{g(x_1)-\bar{g}(\pi^{K}(\Chev{x_1,x_2}))}] \leq M \Prb_{ \mu_1^{K} \times \mu_2^{K}}[(x_1,x_2) \not\in \Supp \sigma_{1\bullet}^{K} \times \Supp \sigma_{2\bullet}^{K}]$$

$$\E[\Abs{g(x_1)-\bar{g}(\pi^{K}(\Chev{x_1,x_2}))}] \leq M\Prb_{ \sigma_{1\bullet}^{K} \times \sigma_{2\bullet}^{K}}[(x_1,x_2) \not\in \Supp \sigma_{1\bullet}^{K} \times \Supp \sigma_{2\bullet}^{K}] \pmod \Fall$$

$$\E[\Abs{g(x_1)-\bar{g}(\pi^{K}(\Chev{x_1,x_2}))}] \equiv 0 \pmod \Fall$$
\end{proof}
\begin{samepage}
\begin{proposition}
\label{prp:smp_base_change}

Consider word ensembles $\mu_1$ and $\mu_2$ with $\EMG$-samplers $\sigma_1$ and $\sigma_2$ respectively. Define ${\mu^k:=\En_*^2(\mu_1^k \times \mu_2^k)}$. Suppose ${\pi: \Words \Scheme \Words}$ is s.t. for any $K \in \Nats^n$, ${x_1 \in \Supp \sigma_{1\bullet}^{K}}$, ${x_2 \in \Words}$ and $z \in \Bool^{\R_\pi(K)}$, ${\pi^{K}(\Chev{x_1,x_2},z)=x_1}$. Consider ${\sigma: \Words \MScheme \Words}$ s.t. $\UM_\sigma^{K}=\UM_{\sigma_2}^{K}$ and for any $x \in \Supp \sigma_{1\bullet}^{K}$, ${\sigma^{K}(x,z,w)=\Chev{x,\sigma_2^{K}(z,w)}}$. Then, $\sigma$ is an $\EMG$-sampler of $\mu$ relative to $\pi$. In particular, since such an $\sigma$ can always be constructed, $\mu$ is $\EMG$-samplable relative to $\pi$.

\end{proposition}
\end{samepage}

\begin{proof}

\[\mu^{K} \equiv \En_*^2(\sigma_{1\bullet}^K \times \sigma_{2\bullet}^K)\pmod \Fall\]

\[\pi_*^K\mu^{K} \equiv \pi_*^K\En_*^2(\sigma_{1\bullet}^K \times \sigma_{2\bullet}^K) \pmod \Fall\] 

\[\pi_*^K\mu^{K} \equiv  \sigma_{1\bullet}^K \pmod \Fall\]

Denote $\mu_x^K:=\mu \mid (\pi^K)^{-1}(x)$.

\[\E_{x \sim \pi_*^K\mu^{K}}[\Dtv(\mu_x^K,\sigma_x^K)] \equiv \E_{x \sim \sigma_{1\bullet}^K}[\Dtv(\mu_x^K,\sigma_x^K)] \pmod \Fall\]

For any $x \in \Supp \sigma_{1\bullet}^{K}$, $\mu_x^K = \En_*^2(\delta_x \times \mu_2^{K})$ and $\sigma_x^K=\En_*^2(\delta_x \times \sigma_{2\bullet}^K)$.

\[\E_{x \sim \pi_*^K\mu^{K}}[\Dtv(\mu_x^K,\sigma_x^K)] \equiv \E_{x \sim \sigma_{1\bullet}^K}[\Dtv(\En_*^2(\delta_x \times \mu_2^{K}),\En_*^2(\delta_x \times \sigma_{2\bullet}^K))] \pmod \Fall\]

\[\E_{x \sim \pi_*^K\mu^{K}}[\Dtv(\mu_x^K,\sigma_x^K)] \equiv \E_{x \sim \sigma_{1\bullet}^K}[\Dtv(\mu_2^{K},\sigma_{2\bullet}^K)] \pmod \Fall\]

\[\E_{x \sim \pi_*^K\mu^{K}}[\Dtv(\mu_x^K,\sigma_x^K)] \equiv \Dtv(\mu_2^{K},\sigma_{2\bullet}^K) \pmod \Fall\]

\[\E_{x \sim \pi_*^K\mu^{K}}[\Dtv(\mu_x^K,\sigma_x^K)] \equiv 0 \pmod \Fall\]
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:thm__mult__cond3}

Consider $\mu_1$ a word ensemble with $\EMG$-sampler $\sigma$ and $(\mu_2, f)$ a distribution estimation problem with $\EMG$-sampler $G$. Define the distributional estimation problem $(\mu,\bar{f})$ by 

\begin{align*}
\mu^k:=\En_*^2(\mu_1^k \times \mu_2^k)\\
\bar{f}(\Chev{x_1,x_2})=f(x_2)
\end{align*}

Suppose $\pi: \Words \Scheme \Words$ is s.t. for any $K \in \Nats^n$, ${x_1 \in \Supp \sigma_\bullet^{K}}$, ${x_2 \in \Words}$ and $z \in \Bool^{\R_\pi(K)}$, $\pi^{K}(\Chev{x_1,x_2},z)=x_1$. Then, $(\mu,\bar{f})$ is $\EMG$-samplable relative to $\pi$.

\end{proposition}
\end{samepage}

\begin{proof}

Construct $\bar{G}: \Words \MScheme \Words \times \Rats$ s.t. $\UM_{\bar{G}}^K=\UM_G^K$ and for any ${x \in \Supp \sigma_\bullet^K}$

\[\bar{G}^K(x,y,z)=(\Chev{x,G^K(y,z,w)_0},G^K(y,z,w)_1)\] 

By Proposition~\ref{prp:smp_base_change}, $\bar{G}_0$ is an $\EMG$-sampler of $\mu$ relative to $\pi$.

\[\mu^{K} \equiv \En_*^2(\sigma_\bullet^K \times G_{0\bullet}^K)\pmod \Fall\]

\[\mu^{K} \ltimes \pi^K \equiv \En_*^2(\sigma_\bullet^K \times G_{0\bullet}^K) \ltimes \pi^K \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv \E_{(x,y) \sim \En_*^2(\sigma_\bullet^K \times G_{0\bullet}^K) \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv \E_{(x_1,x_2) \sim \sigma_\bullet^K \times G_{0\bullet}^K}[\Abs{f_{\bar{G}}^K(\Chev{x_1,x_2},x_1)-\bar{f}(\Chev{x_1,x_2})}] \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv \E_{(x_1,x_2) \sim \sigma_\bullet^K \times G_{0\bullet}^K}[\Abs{\E_{\UM_{\bar{G}}^K}[\bar{G}_1^K(x_1) \mid \bar{G}^K(x_1)_0 = \Chev{x_1,x_2}]-f(x_2)}] \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv \E_{(x_1,x_2) \sim \sigma_\bullet^K \times G_{0\bullet}^K}[\Abs{\E_{\UM_G^K}[G_1^K \mid \Chev{x_1,G^K} = \Chev{x_1,x_2}]-f(x_2)}] \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv \E_{(x_1,x_2) \sim \sigma_\bullet^K \times G_{0\bullet}^K}[\Abs{\E_{\UM_G^K}[G_1^K \mid G^K = x_2]-f(x_2)}] \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv \E_{x_2 \sim G_{0\bullet}^K}[\Abs{f_G^K(x_2)-f(x_2)}] \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv \E_{x_2 \sim \mu_2^{K}}[\Abs{f_G^K(x_2)-f(x_2)}] \pmod \Fall\]

\[\E_{(x,y) \sim \mu^{K} \ltimes \pi^K}[\Abs{f_{\bar{G}}^K(x,y)-\bar{f}(x)}] \equiv 0 \pmod \Fall\]
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:thm__mult__op}

Consider word ensemble $\mu_1$  with $\EMG$-sampler $\sigma$ and $(\mu_2,f)$ a distributional estimation problem. Define the distributional estimation problem $(\mu,\bar{f})$ by 

\begin{align*}
\mu^k:=\En_*^2(\mu_1^k \times \mu_2^k)\\
\bar{f}(\Chev{x_1,x_2})=f(x_2)
\end{align*}

Suppose $P$ is an $\ESG$-optimal predictor for $(\mu_2,f)$. Let ${\bar{P}: \Words \Scheme \Rats}$ be s.t. $\R_{\bar{P}}=\R_P$ and for any $K \in \Nats^n$, $x_1 \in \Supp \sigma_\bullet^K$, $x_2 \in \Supp \mu_2^K$ and $z \in \Bool^{\R_P(K)}$, $\bar{P}^K(\Chev{x_1,x_2},z)=P^K(x_2,z)$. Then, $\bar{P}$ is an $\ESG$-optimal predictor for $(\mu,\bar{f})$.

\end{proposition}
\end{samepage}

\begin{proof}

Consider any $S: \Words \times \Rats \Scheme \Rats$ bounded. Denote $\Un_{PS}^K:=\Un_P^K \times \Un_S^K$, ${\mu_{PS}^K:=\mu^{K} \times \Un_{PS}^K}$.

\[\E_{\mu_{PS}^K}[(\bar{P}^{K}(x) - \bar{f}(x))S^{K}(x,\bar{P}^{K}(x))]=\E_{\mu_1^{K} \times \mu_2^{K} \times \Un_{PS}^K}[(\bar{P}^K(\Chev{x_1,x_2}) - \bar{f}(\Chev{x_1,x_2}))S^K(\Chev{x_1,x_2},\bar{P}^K(\Chev{x_1,x_2}))]\]

\[\E_{\mu_{PS}^K}[(\bar{P}^{K}(x) - \bar{f}(x))S^{K}(x,\bar{P}^{K}(x))]=\E_{\mu_1^{K} \times \mu_2^{K} \times \Un_{PS}^K}[(\bar{P}^K(\Chev{x_1,x_2}) - f(x_2))S^K(\Chev{x_1,x_2},\bar{P}^K(\Chev{x_1,x_2}))]\]

\[\E_{\mu_{PS}^K}[(\bar{P}^{K}(x) - \bar{f}(x))S^{K}(x,\bar{P}^{K}(x))]=\E_{\mu_1^{K}}[\E_{\mu_2^{K} \times \Un_{PS}^K}[(\bar{P}^K(\Chev{x_1,x_2}) - f(x_2))S^K(\Chev{x_1,x_2},\bar{P}^K(\Chev{x_1,x_2}))]]\]

Applying Proposition~\ref{prp:smp} (with $Y = \bm{1}$) to the right hand side, we get

\[\E_{\mu_{PS}^K}[(\bar{P}^{K} - \bar{f})S^{K}] \equiv \E_{\UM_\sigma^K}[\E_{\mu_2^{K} \times \Un_{PS}^K}[(\bar{P}^K(\Chev{\sigma^K,x_2}) - f(x_2))S^K(\Chev{\sigma^K,x_2},\bar{P}^K(\Chev{\sigma^K,x_2}))]] \pmod \Fall\]

\[\E_{\mu_{PS}^K}[(\bar{P}^{K} - \bar{f})S^{K}] \equiv \E_{\UM_\sigma^K}[\E_{\mu_2^{K} \times \Un_{PS}^K}[(P^K(x_2) - f(x_2))S^K(\Chev{\sigma^K,x_2},P^K(x_2))]] \pmod \Fall\]

\[\E_{\mu_{PS}^K}[(\bar{P}^{K} - \bar{f})S^{K}] \equiv \E_{\mu_2^{K} \times \Un_{PS}^K \times \UM_\sigma^K}[(P^K(x_2) - f(x_2))S^K(\Chev{\sigma^K,x_2},P^K(x_2))] \pmod \Fall\]

Using the fact $P$ is an $\ESG$-optimal predictor for $(\mu_2,f)$, we conclude

\[\E_{\mu_{PS}^K}[(\bar{P}^{K} - \bar{f})S^{K}] \equiv 0 \pmod \Fall\]
%
\end{proof}

\begin{proof}[Proof of Corollary \ref{crl:dir_prod}]

Define $\bar{f}_1, \bar{f}_2: \Supp \mu \rightarrow \Reals$ by $\bar{f}_1(\Chev{x_1,x_2})=f_1(x_1)$, $\bar{f}_2(\Chev{x_1,x_2})=f_2(x_2)$. 

Construct $\pi: \Words \Scheme \Words$ s.t. $\R_\pi \equiv 0$ and for any ${x_1 \in \Supp \sigma_\bullet^K}$ and $x_2 \in \Words$ 

\[\pi^K(\Chev{x_1,x_2})=x_1\]

This is possible because the runtime of $\sigma^K$ is bounded by a polynomial in $K$ so the length of $\sigma^K$'s output is also bounded by a polynomial in $K$, implying $\pi^K$ only has to read a polynomial size prefix of its input in order to output $x_1$.

Construct $\bar{P}: \Words \Scheme \Rats$ s.t. $\R_{\bar{P}}=\R_{P_2}$ and for any $x_1 \in \Supp \sigma_\bullet^K$, $x_2 \in \Words$ and $z \in \Bool^{\R_{P_2}(K)}$, $\bar{P}^K(\Chev{x_1,x_2},z)=P_2^K(x_2,z)$. This is possible for the same reason as above: $\bar{P}$ skips the polynomial size prefix corresponding to $x_1$ and then executes a simulation of running $P_2$ on $x_2$, even if $x_2$ is too long to read in full. By Proposition~\ref{prp:thm__mult__op}, $\bar{P}$ is an $\ESG$-optimal predictor for $(\mu,\bar{f}_2)$. 

We apply Theorem~\ref{thm:mult} where $\bar{f}_1$, $\bar{f}_2$ play the roles of $f_1$, $f_2$ and $(\mu_1, f_1)$ plays the role of $(\nu,g)$: condition~\ref{con:thm__mult__dist} holds due to Proposition~\ref{prp:thm__mult__cond1}, condition~\ref{con:thm__mult__fun} holds due to Proposition~\ref{prp:thm__mult__cond2} and condition~\ref{con:thm__mult__smp} holds due to Proposition~\ref{prp:thm__mult__cond3}. This gives us $P$, an optimal predictor for $(\mu, f)$ s.t. ${\R_P=\R_{P_1}+\R_{P_2}}$ and for any ${z_1 \in \Bool^{\R_{P_1}(K)}}$ and $z_2 \in \Bool^{\R_{P_1}(K)}$ 

\[P^K(x, z_1 z_2) = P_1^K(\pi^K(x),z_1) \bar{P}^K(x,z_2)\]

In particular, for any ${x_1 \in \Supp \sigma_\bullet^K}$ and $x_2 \in \Words$

\[P^K(\Chev{x_1,x_2}, z_1 z_2)=P_1^K(x_1,z_2)P_2^K(x_2,z_2)\]
%
\end{proof}

\section{Reductions and Completeness}
\label{sec:reductions}

\subsection{Reductions}
\label{sub:reductions}

In this subsection we study notions of Karp reduction between distributional estimation problems such that the pull-back of an optimal predictor is an optimal predictor. It is also interesting to study Cook reductions but we avoid it in the present work.

First, we demonstrate that the notion of Karp reduction used in average-case complexity theory is insufficiently strong for our purpose. 

Consider the setting of Corollary~\ref{crl:one_way}. Denote $\mu^k:=\Un^{2k}$ and define ${\chi: \Supp \mu \rightarrow \Bool}$ s.t. for any $x,y \in \Bool^k$, $\chi(xy)$ = $x \cdot y$. Construct $\pi_f: \Words \Scheme \Words$ s.t. for any $x,y \in \Bool^k$, ${\pi_f^k(xy) = \Chev{f(x),y}}$. $\pi_f$ can be regarded as a Karp reduction of $(\mu, \chi)$ to $(\mu_{(f)},\chi_f)$ since for any ${z \in \Supp \mu^k}$ we have $\chi_f(\pi_f^k(z))=\chi(z)$ and $(\pi_f)_*\mu=\mu_{(f)}$\footnote{This is a much stronger condition than what is needed for a reduction to preserve average-case complexity. See \cite{Bogdanov_2006} for details.}. However, the pull-back of $P$ is \emph{not} an $\Fall_{\text{neg}}(\Gamma)$-optimal predictor for $(\mu,\chi)$ since its error is $\E_{z \sim \mu^k}[(\frac{1}{2}-\chi(z))^2]=\frac{1}{4}$ whereas we can construct $Q: \Words \Scheme \Rats$ s.t. for any $z \in \Supp \mu^k$, $Q^k(z)=\chi(z)$ and therefore $\E_{z \sim \mu^k}[(Q^k(z)-\chi(z))^2]=0$.

We will describe several types of reductions that preserve optimal predictors. After that, we will characterize reductions that can be constructed by composing those types.

\subsubsection{Pseudo-invertibility}

\begin{samepage}
\begin{definition}
\label{def:psp_reduce}

Consider $(\mu,f)$, $(\nu,g)$ distributional estimation problems and ${\pi: \Words \Scheme \Words}$. $\pi$ is called a \emph{precise strict pseudo-invertible $\EG$-reduction of $(\mu,f)$ to $(\nu,g)$} when

\begin{enumerate}[(i)]

\item\label{con:def__psp_reduce__dist} $\pi_*^K\mu^{K} \equiv \nu^{K} \pmod \Fall$

\item\label{con:def__psp_reduce__fun} Denote ${\bar{g}: \Words \rightarrow \Reals}$ the extension of $g$ by 0. We require $\E_{(x,z) \sim \mu^{K} \times \Un_\pi^{K}}[\Abs{f(x)-\bar{g}(\pi^{K}(x,z))}] \in \Fall$

\item\label{con:def__psp_reduce__smp} $\mu$ is $\EMG$-samplable relative to $\pi$.

\end{enumerate}

\end{definition}
\end{samepage}

Note that condition~\ref{con:def__psp_reduce__smp} is violated in the one-way function example above.

Precise strict pseudo-invertible $\EG$-reductions preserve $\ESG$-optimal predictors as a simple corollary of Theorem~\ref{thm:mult}:

\begin{samepage}
\begin{corollary}
\label{crl:psp_reduce_sharp}

Consider $(\mu,f)$, $(\nu,g)$ distributional estimation problems and $\pi$ a precise strict pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$. Suppose $P$ is an $\ESG$-optimal predictor for $(\nu, g)$. Then, $P \circ \pi$ is an $\ESG$-optimal predictor for $(\mu, f)$.

\end{corollary}
\end{samepage}

\begin{proof}

Follows directly from Theorem~\ref{thm:mult} for $f_1 = f$, ${f_2 \equiv 1}$, $P_2 \equiv 1$. This relies on the trivial observation that $(\mu, 1)$ is samplable relative to $\pi$ iff $\mu$ is samplable relative to $\pi$.
%
\end{proof}

$\EG$-optimal predictors are also preserved.

\begin{samepage}
\begin{theorem}
\label{thm:psp_reduce}

Consider $(\mu,f)$, $(\nu,g)$ distributional estimation problems and $\pi$ a precise strict pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$. Suppose $P$ is an $\EG$-optimal predictor for $(\nu, g)$. Then, $P \circ \pi$ is an $\EG$-optimal predictor for $(\mu, f)$.

\end{theorem}
\end{samepage}

\begin{samepage}
\begin{proposition}
\label{prp:mixed_opt}

Consider ${(\mu,f)}$ a distributional estimation problem and ${P}$ an ${\EG}$-optimal predictor for ${(\mu,f)}$. Then, for any $Q: \Words \MScheme \Rats$ bounded

\begin{equation}
\label{eqn:prp__mixed_ort}
\E_{(x,y) \sim \mu^{K} \times \Un_P^K}[(P^K(x,y) - f(x))^2] \leq \E_{(x,y) \sim \mu^{K} \times \UM_Q^K}[(Q^K(x,y)-f(x))^2] \pmod \Fall
\end{equation}

\end{proposition}
\end{samepage}

\begin{proof}

For any ${K \in \Nats^n}$, choose 

\[w^K \in \Argmax{w \in \Supp \M_Q^K} \E_{(x,z) \sim \mu^{K} \times \Un^{\R_Q^K(w)}}[(Q^K(x,z,w)-f(x))^2]\]

Construct ${\bar{Q}: \Words \Scheme \Rats}$ s.t.

\begin{align*}
\R_{\bar{Q}}(K) &= \R_Q^K(w^K) \\
\bar{Q}^K(x,z) &= \bar{Q}^K(x,z,w)
\end{align*}

Equation \ref{eqn:op} for ${\bar{Q}}$ implies \ref{eqn:prp__mixed_ort}.
%
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:sq_diff_cong}

Consider $\{F^K\}_{K \in \Nats^n}$, $\{G_1^K\}_{K \in \Nats^n}$, $\{G_1^K\}_{K \in \Nats^n}$ uniformly bounded families of random variables and suppose ${\E[\Abs{G_1^K - G_2^K}] \in \Fall}$. Then

\begin{equation}
%\label{eqn:tbd}
\E[(F^K + G_1^K)^2] \equiv \E[(F^K + G_2^K)^2] \pmod \Fall
\end{equation}

\end{proposition}
\end{samepage}

\begin{proof}

\[\E[(F^K + G_1^K)^2] - \E[(F^K + G_2^K)^2] = \E[(2 F^K + G_1^K + G_2^K)(G_1^K - G_2^K)]\]

\[\Abs{\E[(F^K + G_1^K)^2] - \E[(F^K + G_2^K)^2]} \leq (2 \sup F + \sup G_1 + \sup G_2) \E[\Abs{G_1^K - G_2^K}]\]
%
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:psp_reduce}]

Let ${\sigma}$ be an ${\EMG}$-sampler of ${\mu}$ relative to ${\pi}$. Consider any ${Q: \Words \Scheme \Rats}$ bounded. Applying Proposition~\ref{prp:mixed_opt} for ${P}$ and ${Q \circ \sigma}$, we get

\[\E_{\nu^{K} \times \Un_P^K}[(P^K-g)^2] \leq \E_{\nu^{K} \times \Un_Q^K \times \UM_\sigma^K}[((Q \circ \sigma)^K - g)^2] \pmod \Fall\]

Using condition~\ref{con:def__psp_reduce__dist} of Definition~\ref{def:psp_reduce}

\[\E_{\pi_*^K\mu^{K} \times \Un_P^K}[(P^K-\bar{g})^2] \leq \E_{\pi_*^K\mu^{K} \times \Un_Q^K \times \UM_\sigma^K}[((Q \circ \sigma)^K - \bar{g})^2] \pmod \Fall\]

\[\E_{\pi_*^K\mu^{K} \times \Un_P^K}[(P^K-\bar{g})^2] \leq \E_{\pi_*^K\mu^{K} \times \UM_\sigma^K}[\E_{\Un_Q^K}[((Q \circ \sigma)^K - \bar{g})^2]] \pmod \Fall\]

The right hand side has the form of the right hand side in \ref{eqn:prp__smp} enabling us to apply Proposition~\ref{prp:smp} and get

\[\E_{\pi_*^K\mu^{K} \times \Un_P^K}[(P^K-\bar{g})^2] \leq \E_{\mu^{K} \times \Un_\pi^K}[\E_{\Un_Q^K}[(Q^K - \bar{g} \circ \pi^K)^2]] \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_\pi^K \times \Un_P^K}[((P \circ \pi)^K-\bar{g} \circ \pi^K)^2] \leq \E_{\mu^{K} \times \Un_\pi^K \times \Un_Q^K}[(Q^K - \bar{g} \circ \pi^K)^2] \pmod \Fall\]

By Proposition~\ref{prp:sq_diff_cong} and condition~\ref{con:def__psp_reduce__fun} of Definition~\ref{def:psp_reduce}

\[\E_{\mu^{K} \times \Un_\pi^K \times \Un_P^K}[((P \circ \pi)^K-f)^2] \leq \E_{\mu^{K} \times \Un_Q^K}[(Q^K - f)^2] \pmod \Fall\]
%
\end{proof}

We now consider a more general type of reduction which only preserves the function on average (the only difference is in condition~\ref{con:def__sp_reduce__fun}):

\begin{samepage}
\begin{definition}
\label{def:sp_reduce}

Consider $(\mu,f)$, $(\nu,g)$ distributional estimation problems and ${\pi: \Words \Scheme \Words}$. $\pi$ is called a \emph{strict pseudo-invertible $\EG$-reduction of $(\mu,f)$ to $(\nu,g)$} when

\begin{enumerate}[(i)]

\item\label{con:def__sp_reduce__dist} $\pi_*^K\mu^{K} \equiv \nu^{K} \pmod \Fall$

\item\label{con:def__sp_reduce__fun} Denote ${\bar{g}: \Words \rightarrow \Reals}$ the extension of $g$ by 0. We require $\E_{(x,z) \sim \mu^{K}}[\Abs{f(x)-\E_{\Un_\pi^{K}}[g(\pi^{K}(x,z))]}] \in \Fall$

\item\label{con:def__sp_reduce__smp} $\mu$ is $\EMG$-samplable relative to $\pi$.

\end{enumerate}

\end{definition}
\end{samepage}

\begin{samepage}
\begin{theorem}
\label{thm:sp_reduce_sharp}

Suppose $\gamma \in \Gamma_{\textnormal{poly}}^n$ is s.t. $\gamma^{-\frac{1}{2}} \in \Fall$. Consider $(\mu,f)$, $(\nu,g)$ distributional estimation problems, $\pi$ a strict pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$ and $P_g$ an $\ESG$-optimal predictor for $(\nu, g)$. Assume $\gamma (\R_P + \R_\pi) \in \GrowR$. Construct ${P_f}$ s.t. for any ${\{z_i \in \BoolR{\pi}\}_{i \in [\gamma(K)]}}$ and ${\{w_i \in \BoolR{P_g}\}_{i \in [\gamma(K)]}}$

\begin{align}
\label{eqn:thm__sp_reduce__rpf}\R_{P_f}(K) &= \gamma(K) (\R_{P_g}(K) + \R_\pi(K)) \\
\label{eqn:thm__sp_reduce__pf}P_f^K(x, \prod_{i \in [\gamma(K)]} w_i z_i) &= \frac{1}{\gamma(K)}\sum_{i \in [\gamma(K)]} P_g^K(\pi^K(x,z_i),w_i)
\end{align}

Then, $P_f$ is an $\ESG$-optimal predictor for $(\mu, f)$.

\end{theorem}
\end{samepage}

\begin{samepage}
\begin{proposition}
\label{prp:ev_equiv_mean}

Consider $\gamma \in \Gamma_{\textnormal{poly}}^n$, $\mu$ a word ensemble and $\bar{g}: \Words \rightarrow \Reals$ bounded. Then,

\begin{equation}
%\label{eqn:tbd}
\E_{(x,z_0, z_1 \ldots z_{\gamma(K)-1}) \sim \mu^{K} \times \prod_{i \in [\gamma(K)]} \Un_\pi^K}[\Abs{\E_{z \sim \Un_\pi^{K}}[\bar{g}(\pi^{K}(x,z))]-\frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \bar{g}(\pi^K(x,z_i))}] \leq \frac{\sup \Abs{\bar{g}}}{\gamma(K)^{\frac{1}{2}}}
\end{equation}

\end{proposition}
\end{samepage}

\begin{proof}

Denote $\Un_\gamma^K:=\prod_{i \in [\gamma(K)]} \Un_\pi^K$.

\[\E[\Abs{\E[\bar{g}(\pi^{K}(x,z))]-\frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \bar{g}(\pi^K(x,z_i))}] \leq \E_{\mu^{K}}[\E_{\Un_\gamma^K}[(\E_{\Un_\pi^K}[\bar{g}(\pi^{K}(x,z))]-\frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \bar{g}(\pi^K(x,z_i)))^2]^{\frac{1}{2}}]\]

\[\E[\Abs{\E[\bar{g}(\pi^{K}(x,z))]-\frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \bar{g}(\pi^K(x,z_i))}] \leq \frac{1}{\gamma(K)^{\frac{1}{2}}} \E_{\mu^{K}}[\Var_{\Un_\pi^K}[\bar{g}(\pi^{K}(x,z))]^{\frac{1}{2}}]\]
%
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:sp_reduce_sharp}]

Consider any $S: \Words \times \Rats \Scheme \Rats$ bounded. Denote ${\Un_{PS}^K:=\Un_{P_f}^K \times \Un_S^K}$. Using condition~\ref{con:def__sp_reduce__fun} of Definition~\ref{def:sp_reduce}

\[\E_{\mu^{K} \times \Un_{PS}^K}[(P_f^K(x) - f(x))S(x,P_f^K(x))] \equiv \E_{\mu^{K} \times \Un_{PS}^K}[(P_f^K(x) - \E_{\Un_\pi^{K}}[g(\pi^{K}(x))])S(x,P_f^K(x))] \pmod \Fall\]

Using the construction of $P_f$, the assumption on $\gamma$ and Proposition~\ref{prp:ev_equiv_mean}, we get

\[\E[(P_f^K - f)S] \equiv \E_{\mu^{K} \times \Un_{PS}^K}[(\frac{1}{\gamma(K)}\sum_{i \in [\gamma(K)]} P_g^K(\pi^K(x,z_i),w_i) - \frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \bar{g}(\pi^K(x,z_i)))S(x,P_f^K(x))] \pmod \Fall\]

\[\E[(P_f^K - f)S] \equiv \frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \E_{\mu^{K} \times \Un_{PS}^K}[(P_g^K(\pi^K(x,z_i),w_i) - \bar{g}(\pi^K(x,z_i)))S(x,P_f^K(x))] \pmod \Fall\]

Let $\sigma$ be an $\EMG$-sampler of $\mu$ relative to ${\pi}$. Denote 

\begin{align*}
\mu_\pi^K &:= \pi_*^K\mu^{K} \\
\Un_i^K&:=\prod_{j \in [\gamma(K)]} \Un_{P_g}^K \times \prod_{j \in [\gamma(K)] \setminus i} \Un_{\pi}^K \times \Un_S^K \times \UM_\sigma^K 
\end{align*}

Applying Proposition~\ref{prp:smp} we get
%Denoting ${y_i:=\pi^K(x,z_i)}$

\[\E[(P_f^K - f)S] \equiv \frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \E_{\mu_\pi^K \times \Un_i^K}[(P_g^K(y) - \bar{g}(y))S(\sigma^K(y),\frac{1}{\gamma(K)}(P_g^K(y)+\sum_{j \in [\gamma(K)] \setminus i} P_g^K(\pi^K(\sigma^K(y),z_j))))] \pmod \Fall\]

Using condition~\ref{con:def__sp_reduce__dist} of Definition~\ref{def:sp_reduce}, we get

\[\E[(P_f^K - f)S] \equiv \frac{1}{\gamma(K)} \sum_{i \in [\gamma(K)]} \E_{\nu^{K} \times \Un_i^K}[(P_g^K - g)S(\sigma^K,\frac{1}{\gamma(K)}(P_g^K+\sum_{j \in [\gamma(K)] \setminus i} P_g^K(\pi^K(\sigma^K,z_j))))] \pmod \Fall\]

$P_g$ is a $\ESG$-optimal predictor for $(\nu,g)$, therefore

\[\E[(P_f^K - f)S] \equiv 0 \pmod \Fall\]
%
\end{proof}

\begin{samepage}
\begin{theorem}
\label{thm:sp_reduce}

Suppose $\gamma \in \Gamma_{\textnormal{poly}}^n$ is s.t. $\gamma^{-\frac{1}{2}} \in \Fall$. Consider $(\mu,f)$, $(\nu,g)$ distributional estimation problems, $\pi$ a strict pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$ and $P_g$ an $\EG$-optimal predictor for $(\nu, g)$. Assume $\R_P + \gamma \R_\pi \in \GrowR$. Construct ${P_f}$  s.t. for any ${\{z_i \in \BoolR{\pi}\}_{i \in [\gamma(K)]}}$ and ${w \in \BoolR{P_g}}$

\begin{align}
\label{eqn:thm__sp_reduce__rpf}\R_{P_f}(K) &= \R_{P_g}(K) + \gamma(K) \R_\pi(K) \\
\label{eqn:thm__sp_reduce__pf}P_f^K(x, w \prod_{i \in [\gamma(K)]} z_i) &= \frac{1}{\gamma(K)}\sum_{i \in [\gamma(K)]} P_g^K(\pi^K(x,z_i),w)
\end{align}


Then, $P_f$ is an $\EG$-optimal predictor for ${(\mu,g)}$.

\end{theorem}
\end{samepage}

\begin{samepage}
\begin{proposition}
\label{prp:ev_diff_sq}

Consider ${F}$ a bounded random variable and ${s,t \in \Reals}$. Then

\begin{equation}
%\label{eqn:tbd}
\E[(F - s)^2 - (F - t)^2] = (\E[F] - s)^2 - (\E[F] - t)^2
\end{equation}

\end{proposition}
\end{samepage}

\begin{proof}

\[\E[(F - s)^2 - (F - t)^2] = \E[(2F - s - t)(t-s)]\]

\[\E[(F - s)^2 - (F - t)^2] = (2\E[F] - s - t)(t-s)\]

\[\E[(F - s)^2 - (F - t)^2] = (\E[F] - s)^2 - (\E[F] - t)^2\]
%
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:sp_reduce}]

Let ${\sigma}$ be an ${\EMG}$-sampler of ${\mu}$ relative to ${\pi}$. Consider any ${Q_f: \Words \Scheme \Rats}$ bounded. Construct ${Q_g: \Words \MScheme \Rats}$ s.t. for any ${z_\sigma \in \UM_\sigma^K}$, ${z_Q \in \BoolR{Q_f}}$, ${z_\pi \in \Bool^{\gamma(K) \R_\pi(K)}}$ and ${z_g \in \BoolR{P_g}}$

\begin{align*}
\M_{Q_g}^K &= c_*^4(\M_\sigma^K \times \M_{Q_f}^K \times \M_{\pi}^K \times \M_{P_g}^K) \\
\R_{Q_g}^K(\Chev{z_{\sigma1}, \A_{Q_f}(K),\A_{\pi}(K),\A_{P_g}(K)}) &= \R_\sigma^K(z_{\sigma1}) + \R_{Q_f}(K) + \gamma(K)\R_{\pi}(K) + \R_{P_g}(K) \\
Q_g^K(x,z_{\sigma0} z_{Q} z_{\pi} z_{g}, \Chev{z_{\sigma1}, \A_{Q_f}(K),\A_{\pi}(K),\A_{P_g}(K)}) &= Q_f^K(\sigma^K(x,z_\sigma),z_{Q})-P_f^K(\sigma^K(x,z_\sigma),z_g z_\pi)+P_g^K(x,z_g)
\end{align*}

Applying Proposition~\ref{prp:mixed_opt} for ${P_g}$ and ${Q_g}$, we get

\[\E_{\nu^{K} \times \Un_{P_g}^K}[(P^K - g)^2] \leq \E_{\nu^{K} \times \UM_{Q_g}^K}[(Q_g^K - g)^2] \pmod \Fall\]

Using condition~\ref{con:def__sp_reduce__dist} of Definition~\ref{def:sp_reduce}

\[\E_{\pi_*^K\mu^{K} \times \Un_{P_g}^K}[(P_g^K-\bar{g})^2] \leq \E_{\pi_*^K\mu^{K} \times \UM_{Q_g}^K}[(Q_g^K - \bar{g})^2] \pmod \Fall\]

\[\E_{\pi_*^K\mu^{K} \times \Un_{P_g}^K}[(P_g^K-\bar{g})^2] \leq \E_{\pi_*^K\mu^{K} \times \UM_{Q_g}^K}[((Q_f \circ \sigma)^K - (P_f \circ \sigma)^K + P_g^K - \bar{g})^2] \pmod \Fall\]

The right hand side has the form of the right hand side in \ref{eqn:prp__smp} enabling us to apply Proposition~\ref{prp:smp} and get

\[\E_{\mu^{K} \times \Un_\pi^K \times \Un_{P_g}^K}[((P_g \circ \pi)^K-\bar{g} \circ \pi^K)^2] \leq \E_{\mu^{K} \times \Un_\pi^K \times \Un_{Q_f}^K \times \Un_{P_f}^K}[(Q_f ^K - P_f^K+(P_g \circ \pi)^K - \bar{g} \circ \pi^K)^2] \pmod \Fall\]

We can consider the expressions within the expected values on both sides as random variables w.r.t. $\Un_\pi^K$ while fixing the other components of the distribution. This allows us applying Proposition~\ref{prp:ev_diff_sq} to the difference between the right hand side and the left hand side (with the terms that don't depend on $\Un_\pi^K$ playing the role of the constants), which results in moving the expected value over $\Un_\pi^K$ inside the squares:

\[\E_{\mu^{K} \times \Un_{P_g}^K}[\E_{\Un_\pi^K}[(P_g \circ \pi)^K-\bar{g} \circ \pi^K]^2] \leq \E_{\mu^{K} \times \Un_{Q_f}^K \times \Un_{P_f}^K}[(Q_f ^K - P_f^K+\E_{\Un_\pi^K}[(P_g \circ \pi)^K - \bar{g} \circ \pi^K])^2] \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_{P_g}^K}[(\E_{\Un_\pi^K}[(P_g \circ \pi)^K]-\E_{\Un_\pi^K}[\bar{g} \circ \pi^K])^2] \leq \E_{\mu^{K} \times \Un_{Q_f}^K \times \Un_{P_f}^K}[(Q_f ^K - P_f^K+\E_{\Un_\pi^K}[(P_g \circ \pi)^K] - \E_{\Un_\pi^K}[\bar{g} \circ \pi^K])^2] \pmod \Fall\]

We now apply Proposition~\ref{prp:sq_diff_cong} via condition \ref{con:def__sp_reduce__fun} of Definition~\ref{def:sp_reduce}

\[\E_{\mu^{K} \times \Un_{P_g}^K}[(\E_{\Un_\pi^K}[(P_g \circ \pi)^K]-f)^2] \leq \E_{\mu^{K} \times \Un_{Q_f}^K \times \Un_{P_f}^K}[(Q_f ^K - P_f^K+\E_{\Un_\pi^K}[(P_g \circ \pi)^K] - f)^2] \pmod \Fall\]

Denote $y_i:=\pi^K(x,z_i)$ where the ${z_i}$ are sampled independently from ${\Un_\pi^K}$. Applying Proposition~\ref{prp:sq_diff_cong} via Proposition~\ref{prp:ev_equiv_mean} and the assumption on $\gamma$, we get

\[\E_{\mu^{K} \times \Un_{P_f}^K}[(\frac{1}{\gamma(K)}\sum_{i \in [\gamma(K)]}P_g^K(y_i)-f)^2] \leq \E_{\mu^{K} \times \Un_{Q_f}^K \times \Un_{P_f}^K}[(Q_f ^K - P_f^K+\frac{1}{\gamma(K)}\sum_{i \in [\gamma(K)]}P_g^K(y_i) - f)^2] \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_{P_f}^K}[(P_f^K-f)^2] \leq \E_{\mu^{K} \times \Un_{Q_f}^K \times \Un_{P_f}^K}[(Q_f ^K - P_f^K+P_f^K - f)^2] \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_{P_f}^K}[(P_f^K-f)^2] \leq \E_{\mu^{K} \times \Un_{Q_f}^K}[(Q_f ^K - f)^2] \pmod \Fall\]
%
\end{proof}

\subsubsection{Dominance}

Next, we consider a scenario in which the identity mapping can be regarded as a valid reduction between distributional estimation problems that have the same function but different word ensembles.

\begin{samepage}
\begin{definition}
%\label{def:tbd}

Consider ${\mu}$, ${\nu}$ word ensembles. ${\mu}$ is said to be \emph{${\EG}$-dominated by ${\nu}$} when there is ${W: \Words \Scheme \Rats^{\geq 0}}$ bounded s.t.

\begin{equation}
%\label{eqn:tbd}
\sum_{x \in \Words} \Abs{\nu^{K}(x)\E_{\Un_W^K}[W^K(x)]-\mu^{K}(x)} \in \Fall
\end{equation}

In this case, ${W}$ is called a \emph{Radon-Nikodym ${\EG}$-derivative of ${\mu}$ w.r.t. ${\nu}$}.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{proposition}
\label{prp:dom_reduce_sharp}

Consider ${\mu}$, ${\nu}$ word ensembles, ${f: \Supp \mu \cup \Supp \nu \rightarrow \Reals}$ bounded and ${P}$ an ${\ESG}$-optimal predictor for ${(\nu,f)}$. Suppose ${\mu}$ is ${\EG}$-dominated by ${\nu}$. Then, ${P}$ is an ${\ESG}$-optimal predictor for ${(\mu,f)}$.

\end{proposition}
\end{samepage}

\begin{proof}

Let ${W}$ be a Radon-Nikodym ${\EG}$-derivative of ${\mu}$ w.r.t. ${\nu}$. Consider any ${S: \Words \times \Rats \Scheme \Rats}$ bounded.

\[\E_{\nu^{K} \times \Un_P^K \times \Un_W^K \times \Un_S^K}[(P^K(x)-f(x))W^K(x)S^K(x,P^K(x))] \equiv 0 \pmod \Fall\]

\[\sum_{x \in \Words} \nu^{K}(x) \E_{\Un_W^K}[W^K(x)] \E_{\Un_P^K \times \Un_S^K}[(P^K(x)-f(x))S^K(x,P^K(x))] \equiv 0 \pmod \Fall\]

\[\sum_{x \in \Words} (\nu^{K}(x) \E_{\Un_W^K}[W^K(x)] - \mu^{K}(x) + \mu^{K}(x)) \E_{\Un_P^K \times \Un_S^K}[(P^K(x)-f(x))S^K(x,P^K(x))] \equiv 0 \pmod \Fall\]

\[\sum_{x \in \Words} (\nu^{K}(x) \E_{\Un_W^K}[W^K(x)] - \mu^{K}(x)) \E_{\Un_P^K \times \Un_S^K}[(P^K-f)S^K] + \sum_{x \in \Words} \ \mu^{K}(x) \E_{\Un_P^K \times \Un_S^K}[(P^K-f)S] \equiv 0 \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_P^K \times \Un_S^K}[(P^K-f)S] \equiv \sum_{x \in \Words} (\nu^{K}(x) \E_{\Un_W^K}[W^K(x)] - \mu^{K}(x)) \E_{\Un_P^K \times \Un_S^K}[(P^K-f)S^K] \pmod \Fall\]

\[\Abs{\E_{\mu^{K} \times \Un_P^K \times \Un_S^K}[(P^K-f)S]} \leq (\sup \Abs{P} + \sup \Abs{f}) \sup \Abs{S} \sum_{x \in \Words} \Abs{\nu^{K}(x) \E_{\Un_W^K}[W^K(x)] - \mu^{K}(x)} \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_P^K \times \Un_S^K}[(P^K-f)S] \equiv 0 \pmod \Fall\]
%
\end{proof}

The corresponding statement for ${\EG}$-optimal predictors may be regarded as a generalization of Corollary~\ref{crl:weight}.

\begin{samepage}
\begin{proposition}
\label{prp:dom_reduce}

Assume ${\Fall}$ is ${\GrowA}$-ample. Consider ${\mu}$, ${\nu}$ word ensembles, ${f: \Supp \mu \cup \Supp \nu \rightarrow \Reals}$ bounded and ${P}$ an ${\EG}$-optimal predictor for ${(\nu,f)}$. Suppose ${\mu}$ is ${\EG}$-dominated by ${\nu}$. Then, ${P}$ is an ${\EG}$-optimal predictor for ${(\mu,f)}$.

\end{proposition}
\end{samepage}

\begin{proof}

Let ${W}$ be a Radon-Nikodym ${\EG}$-derivative of ${\mu}$ w.r.t. ${\nu}$. Consider any ${Q: \Words \Scheme \Rats}$ bounded. According to Proposition~\ref{prp:weight}

\[\E_{\nu^{K} \times \Un_W^K \times \Un_P^K}[W^K(x)(P^K(x)-f(x))^2] \leq \E_{\nu^{K} \times \Un_W^K \times \Un_Q^K}[W^K(x)(Q^K(x)-f(x))^2] \pmod \Fall\]

\[\sum_{x \in \Words} \nu^{K}(x) \E_{\Un_W^K}[W^K(x)] \E_{\Un_P^K}[(P^K(x)-f(x))^2] \leq \sum_{x \in \Words} \nu^{K}(x) \E_{\Un_W^K}[W^K(x)] \E_{\Un_Q^K}[(Q^K(x)-f(x))^2] \pmod \Fall\]

Using the assumption on ${W}$

\[\sum_{x \in \Words} \mu^{K}(x) \E_{\Un_P^K}[(P^K(x)-f(x))^2] \leq \sum_{x \in \Words} \mu^{K}(x) \E_{\Un_Q^K}[(Q^K(x)-f(x))^2] \pmod \Fall\]

\[\E_{\mu^{K} \times \Un_P^K}[(P^K(x)-f(x))^2] \leq \E_{\mu^{K} \times \Un_Q^K}[(Q^K(x)-f(x))^2] \pmod \Fall\]
%
\end{proof}

\subsubsection{Pullbacks}

Finally, we consider another scenario in which the identity mapping is a valid reduction. This scenario is a simple re-indexing of the word ensemble. For the remainder of subsection~\ref{sub:reductions}, we fix some ${m \in \Nats}$.

\begin{samepage}
\begin{definition}
%\label{def:tbd}

We denote ${\Gamma_{\text{poly}}^{mn}:=\{\gamma: \Nats^m \rightarrow \Nats^n \mid \forall i \in [n]: \gamma_i \in \Gamma_{\text{poly}}^m \}}$.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}
%\label{def:tbd}

Consider ${\Gamma_*}$ a growth space of rank ${n}$ and ${\alpha \in \Gamma_{\text{poly}}^{mn}}$. We introduce the notation

\begin{equation}
\label{eqn:tbd}
\Gamma_*\alpha:=\{\gamma_\alpha: \Nats^m \rightarrow \Reals^{\geq 0} \textnormal{ bounded} \mid \exists \gamma \in \Gamma_*: \gamma_\alpha \leq \gamma \circ \alpha\}
\end{equation}

Obviously ${\Gamma_*\alpha}$ is a growth space of rank ${m}$.

We also denote ${\Gamma \alpha := (\GrowR \alpha, \GrowA \alpha)}$.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}
%\label{def:tbd}

Consider ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$. We introduce the notation

\begin{equation}
\label{eqn:tbd}
\Fall \alpha:=\{\varepsilon_\alpha: \Nats^m \rightarrow \Reals^{\geq 0} \textnormal{ bounded} \mid \exists \varepsilon \in \Fall: \varepsilon_\alpha \leq \varepsilon \circ \alpha\}
\end{equation}

\end{definition}
\end{samepage}

\begin{samepage}
\begin{proposition}
\label{prp:tbd}

For any ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$, ${\Fall \alpha}$ is a fall space

\end{proposition}
\end{samepage}

\begin{proof}

Conditions \ref{con:def__fall__add} and \ref{con:def__fall__ineq} are obvious. To verify condition \ref{con:def__fall__pol}, consider a polynomial ${h: \Nats^n \rightarrow \Nats}$ s.t. ${2^{-h} \in \Fall}$. Without loss of generality, we can assume all the coefficients of ${h}$ are non-negative and in particular, it is non-decreasing in all arguments. Consider ${p: \Nats^m \rightarrow \Nats^n}$ a polynomial map s.t. for any ${i \in [n]}$, ${\alpha_i \leq p_i}$. Clearly, ${2^{-h \circ p} \in \Fall \alpha}$.
%
\end{proof}

\begin{samepage}
\begin{definition}
%\label{def:tbd}

Consider ${\mu}$ a word ensemble of rank ${n}$ and ${\alpha: \Nats^m \rightarrow \Nats^n}$. The \emph{pullback of ${\mu}$ by ${\alpha}$}, denoted ${\mu^\alpha}$, is the word ensemble of rank ${m}$ given by ${(\mu^\alpha)^k:=\mu^{\alpha(k)}}$.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}
%\label{def:tbd}

Consider ${X}$, ${Y}$ encoded sets, ${S: X \Scheme Y}$ and ${\alpha: \Nats^m \Alg \Nats^n}$ s.t. ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$ as a function and ${\T_\alpha \in \Gamma_{\textnormal{poly}}^m}$. We define ${S^\alpha: X \xrightarrow{\Gamma \alpha} Y}$ by requiring that for any ${L \in \Nats^m}$, ${\R_{S^\alpha}(L)=\R_S(\alpha(L))}$ and ${(S^\alpha)^L(x,y)=S^{\alpha(L)}(x,y)}$.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{proposition}
\label{prp:rev_sch_idx}

Consider  ${X}$, ${Y}$ encoded sets, ${\alpha: \Nats^m \Alg \Nats^n}$ and ${\beta: \Nats^n \Alg \Nats^m}$. Assume that ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$ and ${\beta \in \Gamma_{\textnormal{poly}}^{nm}}$ as functions, ${\T_\alpha \in \Gamma_{\textnormal{poly}}^m}$, ${\T_\beta \in \Gamma_{\textnormal{poly}}^n}$ and ${\beta(\alpha(L))=L}$. Then, for any  ${S: X \xrightarrow{\Gamma \alpha} Y}$ there is ${\tilde{S}: X \Scheme Y}$ s.t. for all ${K \in \Nats^n}$ that satisfy ${\alpha(\beta(K))=K}$, ${x \in X}$ and ${y,z \in \Words}$

\begin{align}
\A_{\tilde{S}}(K)&=\A_S(\beta(K)) \\
\R_{\tilde{S}}^K(z)&=\R_S^{\beta(K)}(z) \\
\tilde{S}^K(x,y,z)&=S^{\beta(K)}(x,y,z)
\end{align}

\end{proposition}
\end{samepage}

\begin{proof}

There is ${\gamma_{\mathfrak{R}} \in \GrowR}$ s.t. ${\R_S(L) \leq \gamma_{\mathfrak{R}}(\alpha(L))}$ and ${\gamma_{\mathfrak{A}} \in \GrowA}$ s.t. ${\Abs{\A_S(L)} \leq \gamma_{\mathfrak{A}}(\alpha(L))}$. In particular, if ${K \in \Nats^n}$ is s.t. ${\alpha(\beta(K))=K}$ then ${\R_S(\beta(K)) \leq \gamma_{\mathfrak{R}}(K)}$ and ${\Abs{\A_S(\beta(K))} \leq \gamma_{\mathfrak{A}}(K)}$.
%
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:idx_reduce_sharp}

Consider $(\mu,f)$ a distributional estimation problem of rank ${n}$, ${P}$ an ${\ESG}$-optimal predictor for ${(\mu,f)}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and ${\beta: \Nats^n \Alg \Nats^m}$. Assume that ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$ and ${\beta \in \Gamma_{\textnormal{poly}}^{nm}}$ as functions, ${\T_\alpha \in \Gamma_{\textnormal{poly}}^m}$, ${\T_\beta \in \Gamma_{\textnormal{poly}}^n}$ and ${\beta(\alpha(L))=L}$. Then, ${P^\alpha}$ is an ${\Fall \alpha^\sharp(\Gamma \alpha)}$-optimal predictor for ${(\mu^\alpha,f)}$.

\end{proposition}
\end{samepage}

\begin{proof}

Consider any ${S: \Words \times \Rats \xrightarrow{\Gamma \alpha} \Rats}$ bounded. Construct ${\tilde{S}: \Words \times \Rats \Scheme \Rats}$ by applying Proposition~\ref{prp:rev_sch_idx} to ${S}$. There is ${\varepsilon \in \Fall}$ s.t. for any ${K \in \Nats^n}$

\[\Abs{\E_{\mu^{K} \times \Un_P^K \times \Un_{\tilde{S}}^K}[(P^K(x,y) - f(x))\tilde{S}^K(x,P^K(x,y),z)]}=\varepsilon(K)\]

Substituting ${\alpha(L)}$ for ${K}$, we get

\[\Abs{\E_{\mu^{\alpha(L)} \times \Un_P^{\alpha(L)} \times \Un_{\tilde{S}}^{\alpha(L)}}[(P^{\alpha(L)}(x,y) - f(x))\tilde{S}^{\alpha(L)}(x,P^{\alpha(L)}(x,y),z)]}=\varepsilon(\alpha(L))\]

\[\Abs{\E_{(\mu^\alpha)^{L} \times \Un_{P^\alpha}^{L} \times \Un_{\tilde{S}}^{\alpha(L)}}[((P^\alpha)^{L}(x,y) - f(x))\tilde{S}^{\alpha(L)}(x,(P^\alpha)^{L}(x,y),z)]}=\varepsilon(\alpha(L))\]

We have ${\alpha(\beta(\alpha(L))=\alpha(L)}$, therefore

\[\Abs{\E_{(\mu^\alpha)^{L} \times \Un_{P^\alpha}^{L} \times \Un_{S}^{\beta(\alpha(L))}}[((P^\alpha)^{L}(x,y) - f(x))S^{\beta(\alpha(L))}(x,(P^\alpha)^{L}(x,y),z)]}=\varepsilon(\alpha(L))\]

\[\Abs{\E_{(\mu^\alpha)^{L} \times \Un_{P^\alpha}^{L} \times \Un_{S}^{L}}[((P^\alpha)^{L}(x,y) - f(x))S^{L}(x,(P^\alpha)^{L}(x,y),z)]}=\varepsilon(\alpha(L))\]
%
\end{proof}

\begin{samepage}
\begin{proposition}
\label{prp:idx_reduce}

Consider $(\mu,f)$ a distributional estimation problem of rank ${n}$, ${P}$ an ${\EG}$-optimal predictor for ${(\mu,f)}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and ${\beta: \Nats^n \Alg \Nats^m}$. Assume that ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$ and ${\beta \in \Gamma_{\textnormal{poly}}^{nm}}$ as functions, ${\T_\alpha \in \Gamma_{\textnormal{poly}}^m}$, ${\T_\beta \in \Gamma_{\textnormal{poly}}^n}$ and ${\beta(\alpha(L))=L}$. Then, ${P^\alpha}$ is an ${\Fall \alpha(\Gamma \alpha)}$-optimal predictor for ${(\mu^\alpha,f)}$.

\end{proposition}
\end{samepage}

\begin{proof}

Consider any ${Q: \Words \xrightarrow{\Gamma \alpha} \Rats}$ bounded. Construct ${\tilde{Q}: \Words \Scheme \Rats}$ by applying Proposition~\ref{prp:rev_sch_idx} to ${Q}$. There is ${\varepsilon \in \Fall}$ s.t.

\[\E_{\mu^{K} \times \Un_P^K}[(P^K(x,y)-f(x))^2] \leq \E_{\mu^{K} \times \Un_{\tilde{Q}}^K}[(\tilde{Q}^K(x,y)-f(x))^2] + \varepsilon(K)\]

Substituting ${\alpha(L)}$ for ${K}$, we get

\[\E_{\mu^{\alpha(L)} \times \Un_P^{\alpha(L)}}[(P^{\alpha(L)}(x,y)-f(x))^2] \leq \E_{\mu^{\alpha(L)} \times \Un_{\tilde{Q}}^{\alpha(K)}}[(\tilde{Q}^{\alpha(L)}(x,y)-f(x))^2] + \varepsilon({\alpha(L)})\]

\[\E_{(\mu^\alpha)^{L} \times \Un_{P^\alpha}^L}[((P^\alpha)^L(x,y)-f(x))^2] \leq \E_{(\mu^\alpha)^{L} \times \Un_{\tilde{Q}}^{\alpha(L)}}[(\tilde{Q}^{\alpha(L)}(x,y)-f(x))^2] + \varepsilon(\alpha(L))\]

We have ${\alpha(\beta(\alpha(L))=\alpha(L)}$, therefore

\[\E_{(\mu^\alpha)^{L} \times \Un_{P^\alpha}^L}[((P^\alpha)^L(x,y)-f(x))^2] \leq \E_{(\mu^\alpha)^{L} \times \Un_Q^{\beta(\alpha(L))}}[(Q^{\beta(\alpha(L))}(x,y)-f(x))^2] + \varepsilon(\alpha(L))\]

\[\E_{(\mu^\alpha)^{L} \times \Un_{P^\alpha}^L}[((P^\alpha)^L(x,y)-f(x))^2] \leq \E_{(\mu^\alpha)^{L} \times \Un_Q^L}[(Q^L(x,y)-f(x))^2] + \varepsilon(\alpha(L))\]
%
\end{proof}

\subsubsection{Lax Pseudo-invertible Reductions}

We now consider compositions of reductions of different types.

%BLAH

\begin{samepage}
\begin{definition}
\label{def:pp_reduce}

Consider $(\mu,f)$ a distributional estimation problem of rank ${m}$, $(\nu,g)$ distributional estimation problem of rank ${n}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and ${\pi: \Words \Scheme \Words}$. $\pi$ is called a \emph{precise pseudo-invertible $\EG$-reduction of $(\mu,f)$ to $(\nu,g)$ over ${\alpha}$} when there is ${\beta: \Nats^n \Alg \Nats^m}$ s.t.

\begin{enumerate}[(i)]

\item\label{con:def__pp_reduce__pol_deg} As functions, ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$ and ${\beta \in \Gamma_{\textnormal{poly}}^{nm}}$.

\item\label{con:def__pp_reduce__eff_deg} ${\T_\alpha \in \Gamma_{\textnormal{poly}}^m}$, ${\T_\beta \in \Gamma_{\textnormal{poly}}^n}$

\item\label{con:def__pp_reduce__inv_deg} $\forall L \in \Nats^m: \beta(\alpha(L))=L$

\item\label{con:def__pp_reduce__dist} ${\pi_*\mu}$ is ${\Fall \alpha(\Gamma \alpha)}$-dominated by ${\nu^\alpha}$.

\item\label{con:def__pp_reduce__fun} Denote ${\bar{g}: \Words \rightarrow \Reals}$ the extension of $g$ by 0. We require $\E_{(x,z) \sim \mu^{K} \times \Un_\pi^{K}}[\Abs{f(x)-\bar{g}(\pi^{K}(x,z))}] \in \Fall$

\item\label{con:def__pp_reduce__smp} $\mu$ is $\EMG$-samplable relative to $\pi$.

\end{enumerate}

\end{definition}
\end{samepage}

\begin{samepage}
\begin{corollary}
\label{crl:pp_reduce_sharp}

Consider $(\mu,f)$ a distributional estimation problem of rank ${m}$, $(\nu,g)$ distributional estimation problem of rank ${n}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and $\pi$ a precise pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$ over ${\alpha}$. Suppose $P$ is an $\ESG$-optimal predictor for $(\nu, g)$. Then, $P^\alpha \circ \pi$ is an $\Fall \alpha^\sharp (\Gamma \alpha)$-optimal predictor for $(\mu, f)$.

\end{corollary}
\end{samepage}

\begin{proof}

By Proposition~\ref{prp:idx_reduce_sharp} and conditions \ref{con:def__pp_reduce__pol_deg}, \ref{con:def__pp_reduce__eff_deg} and \ref{con:def__pp_reduce__inv_deg} of Definition~\ref{def:pp_reduce}, ${P^\alpha}$ is an ${\Fall \alpha^\sharp(\Gamma \alpha)}$-optimal predictor for ${(\nu^\alpha, g)}$. By Proposition~\ref{prp:dom_reduce_sharp} and condition \ref{con:def__pp_reduce__dist} of Definition~\ref{def:pp_reduce}, ${P^\alpha}$ is also an ${\Fall \alpha^\sharp(\Gamma \alpha)}$-optimal predictor for ${(\pi_* \mu, g)}$. By Corollary~\ref{crl:psp_reduce_sharp} and conditions \ref{con:def__pp_reduce__fun} and \ref{con:def__pp_reduce__smp} of Definition~\ref{def:pp_reduce}, ${P^\alpha \circ \pi}$ is an ${\Fall \alpha^\sharp(\Gamma \alpha)}$-optimal predictor for ${(\mu, f)}$.
%
\end{proof}

\begin{samepage}
\begin{corollary}
%\label{crl:tbd}

Assume ${\Fall}$ is ${\GrowA}$-ample. Consider $(\mu,f)$ a distributional estimation problem of rank ${m}$, $(\nu,g)$ distributional estimation problem of rank ${n}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and $\pi$ a precise pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$ over ${\alpha}$. Suppose $P$ is an $\EG$-optimal predictor for $(\nu, g)$. Then, $P^\alpha \circ \pi$ is an $\Fall \alpha (\Gamma \alpha)$-optimal predictor for $(\mu, f)$.

\end{corollary}
\end{samepage}

\begin{proof}

Completely analogous to proof of Corollary~\ref{crl:pp_reduce_sharp}.
%
\end{proof}

\begin{samepage}
\begin{definition}
\label{def:p_reduce}

Consider $(\mu,f)$ a distributional estimation problem of rank ${m}$, $(\nu,g)$ distributional estimation problem of rank ${n}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and ${\pi: \Words \Scheme \Words}$. $\pi$ is called a \emph{pseudo-invertible $\EG$-reduction of $(\mu,f)$ to $(\nu,g)$ over ${\alpha}$} when there is ${\beta: \Nats^n \Alg \Nats^m}$ s.t.

\begin{enumerate}[(i)]

\item\label{con:def__p_reduce__pol_deg} As functions, ${\alpha \in \Gamma_{\textnormal{poly}}^{mn}}$ and ${\beta \in \Gamma_{\textnormal{poly}}^{nm}}$.

\item\label{con:def__p_reduce__eff_deg} ${\T_\alpha \in \Gamma_{\textnormal{poly}}^m}$, ${\T_\beta \in \Gamma_{\textnormal{poly}}^n}$

\item\label{con:def__p_reduce__inv_deg} $\forall L \in \Nats^m: \beta(\alpha(L))=L$

\item\label{con:def__p_reduce__dist} ${\pi_*\mu}$ is ${\Fall \alpha(\Gamma \alpha)}$-dominated by ${\nu^\alpha}$.

\item\label{con:def__p_reduce__fun} Denote ${\bar{g}: \Words \rightarrow \Reals}$ the extension of $g$ by 0. We require $\E_{(x,z) \sim \mu^{K}}[\Abs{f(x)-\E_{\Un_\pi^{K}}[g(\pi^{K}(x,z))]}] \in \Fall$

\item\label{con:def__p_reduce__smp} $\mu$ is $\EMG$-samplable relative to $\pi$.

\end{enumerate}

\end{definition}
\end{samepage}

The following corollaries are completely analogous to Corollary~\ref{crl:pp_reduce_sharp} and therefore given without proof. We also drop the explicit constructions of the optimal predictors which are obviously modeled on Theorem~\ref{thm:sp_reduce_sharp} and Theorem~\ref{thm:sp_reduce}.

\begin{samepage}
\begin{corollary}
%\label{crl:tbd}

Consider $(\mu,f)$ a distributional estimation problem of rank ${m}$, $(\nu,g)$ distributional estimation problem of rank ${n}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and $\pi$ a pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$ over ${\alpha}$. Suppose there exists an $\ESG$-optimal predictor for $(\nu, g)$. Then, there exists an $\Fall \alpha^\sharp (\Gamma \alpha)$-optimal predictor for $(\mu, f)$???

% Conditions on \gamma

\end{corollary}
\end{samepage}

\begin{samepage}
\begin{corollary}
%\label{crl:tbd}

Assume ${\Fall}$ is ${\GrowA}$-ample. Consider $(\mu,f)$ a distributional estimation problem of rank ${m}$, $(\nu,g)$ distributional estimation problem of rank ${n}$, ${\alpha: \Nats^m \Alg \Nats^n}$ and $\pi$ a pseudo-invertible $\EG$-reduction of $(\mu, f)$ to $(\nu, g)$ over ${\alpha}$. Suppose there exists an $\EG$-optimal predictor for $(\nu, g)$. Then, there exists an $\Fall \alpha (\Gamma \alpha)$-optimal predictor for $(\mu, f)$???

% Conditions on \gamma

\end{corollary}
\end{samepage}

Note that the last results involved passing from fall space ${\Fall}$ and growth spaces ${\Gamma}$ to fall space ${\Fall \alpha}$ and growth spaces ${\Gamma \alpha}$, however in many natural examples ${m = n}$, ${\Fall \alpha \subseteq \Fall}$ and ${\Gamma \alpha = \Gamma}$ so an ${\Fall \alpha^\sharp(\Gamma \alpha)}$-optimal predictor (resp. ${\Fall \alpha(\Gamma \alpha)}$-optimal predictor) is in particular an ${\ESG}$-optimal predictor (resp. ${\EG}$-optimal predictor).

\subsection{Completeness}

In this subsection we show that in any class of problems which has a complete problem in the sense of worst-case complexity, there is a distributional estimation problem which is complete w.r.t. precise pseudo-invertible ${\EG}$-reductions for this class paired with ${\EG}$-samplable word ensembles. In particular, ${\ESG}$-optimal predictors (resp. ${\EG}$-optimal predictors) exist for all problems in the latter class iff one exists for the complete problem.

\begin{samepage}
\begin{definition}
%\label{def:tbd}

A \emph{word family of rank ${n}$} is a family ${M}$ of sets ${\{M^K \subseteq \Words\}_{K \in \Nats^n}}$.

We will use the notation ${\Supp M := \bigcup_{K \in \Nats^n} M^K}$.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}
%\label{def:tbd}

A \emph{worst-case estimation problem of rank ${n}$} is a pair ${(M,f)}$ where ${M}$ is a word family of rank ${n}$ and ${f: \Supp M \rightarrow \Reals}$ is bounded.

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}
%\label{def:tbd}

% worst-case reductions...

\end{definition}
\end{samepage}

% Don't forget to fix the last two corollaries in 3.1!

TBD

\section{Existence and Uniqueness}
\label{sec:e_and_u}

% Including stability of conditional expectations
 
% The 0th parameter serves to define asymptotic behavior and can be roughly thought of as determining the size of the input. The other parameters serve to control the resources available to the predictor. To illustrate the significance of the other parameters using the informal\footnote{This example cannot be formalized in the framework as presented here since the set of prime numbers is in $\textsc{P}$. We can probably tackle it by introducing restrictions on spatial resources, but this out of the current scope.} example from Section 1, the question \enquote{what is the probability 7614829 is prime?} depends on the amount of available resources. For example, we can use additional resources to test for divisibility by additional smaller primes (or in some more clever way) until eventually we are able to test primality and assign a probability in $\{0,1\}$.

TBD

\section{Reflective Systems and Game Theory}
\label{sec:reflective}

% Consider dropping

TBD

\section{Discussion}
\label{sec:discussion}

TBD

\appendix

\section{Appendix}

We review the definitions of hard-core predicate and one-way function and state the Goldreich-Levin theorem.

We will use the notation $\Gamma_{\text{det}}:=(\Gamma_0^1,\Gamma_0^1)$, $\Gamma_{\text{rand}}:=(\Gamma_{\text{poly}}^1,\Gamma_0^1)$, ${\Gamma_{\text{circ}}:=(\Gamma_0^1,\Gamma_{\text{poly}}^1)}$.

\begin{samepage}
\begin{definition}

Given $\mu$ a word ensemble\footnote{The standard definition of a hard-core predicate corresponds to the case $\mu^k=\Un^k$. Here we allow for slightly greater generality.}, $f: \Supp \mu \rightarrow \Words$ and ${B: \Words \xrightarrow{\Gamma_{\text{det}}} \Bool}$, $B$ is a called a \emph{hard-core predicate} of $(\mu,f)$ when for any $S: \Words \xrightarrow{\Gamma_{\textnormal{rand}}} \Bool$

\begin{equation}
\Prb_{(x,y) \sim \mu^k \times \Un_S^k}[S^k(f(x),y)=B^k(x)] \leq \frac{1}{2} \pmod {\Fall_{\text{neg}}}
\end{equation}

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}

Given $\mu$ a word ensemble, $f: \Supp \mu \rightarrow \Words$ and ${B: \Words \xrightarrow{\Gamma_{\text{det}}} \Bool}$, $B$ is a called a \emph{non-uniformly hard-core predicate} of $(\mu,f)$ when for any ${S: \Words \xrightarrow{\Gamma_{\textnormal{circ}}} \Bool}$ 

\begin{equation}
\Prb_{x \sim \mu^k}[S^k(f(x),y)=B^k(x)] \leq \frac{1}{2} \pmod {\Fall_{\text{neg}}}
\end{equation}

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}

$f: \Words \Alg \Words$ is called an \emph{one-way function}
when

\begin{enumerate}[(i)]

\item There is $p: \Nats \rightarrow \Nats$ polynomial s.t. $\forall x \in \Words: \T_f(x) \leq p(\Abs{x})$.

\item For any $S: \Words \xrightarrow{\Gamma_{\text{rand}}} \Words$

\begin{equation}
\Prb_{(x,y) \sim \Un^k \times \Un_S^k}[f(S^k(f(x),y))=x] \in \Fall_{\text{neg}}
\end{equation}

\end{enumerate}

\end{definition}
\end{samepage}

\begin{samepage}
\begin{definition}

$f: \Words \Alg \Words$ is called a \emph{non-uniformly hard to invert} one-way function
when

\begin{enumerate}[(i)]

\item There is $p: \Nats \rightarrow \Nats$ polynomial s.t. $\forall x \in \Words: \T_f(x) \leq p(\Abs{x})$.

\item For any $S: \Words \xrightarrow{\Gamma_{\text{circ}}} \Words$

\begin{equation}
\Prb_{x \sim \Un^k}[f(S^k(f(x)))=x] \in \Fall_{\text{neg}}
\end{equation}

\end{enumerate}

\end{definition}
\end{samepage}

It is easy to see that any non-uniformly hard-core predicate is in particular a hard-core predicate and any non-uniformly hard to invert one-way function is in particular a one-way function.

The following appears in \cite{Goldreich_2008} as Theorem 7.7. Here we state it in the notation of the present work.

\begin{theorem}[Goldreich-Levin]
\label{thm:goldreich_levin}

Consider a one-way function ${f: \Words \Alg \Words}$. Let $\mu^k:=\Un^{2k}$, $f_{\textnormal{GL}}: \Supp \mu \rightarrow \Words$ and ${B: \Words \xrightarrow{\Gamma_{\textnormal{det}}} \Bool}$ be s.t. for any $x,y \in \WordsLen{k}$, $f_{\textnormal{GL}}(xy)=\Chev{f(x),y}$ and ${B^k(xy)=x \cdot y}$. Then, $B$ is a hard-core predicate of $(\mu, f_{\textnormal{GL}})$.

\end{theorem}

There is also a non-uniform version of the theorem which is not stated in \cite{Goldreich_2008}, but its proof is a straightforward adaptation.

\begin{theorem}
\label{thm:goldreich_levin_circ}

In the setting of Theorem~\ref{thm:goldreich_levin}, assume $f$ is non-uniformly hard to invert. Then $B$ is a non-uniformly hard-core predicate of $(\mu, f_{\textnormal{GL}})$.
\end{theorem}

\section*{Acknowledgments}

TBD

\bibliographystyle{unsrt}
\bibliography{Optimal_Predictors}

\end{document}
