%&latex
\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}
\usepackage{bm}

%\numberwithin{equation}{section}

\theoremstyle{definition}
\newtheorem{definition}{Definition}%[section]
\newtheorem{example}{Example}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%[section]
%\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
%\newtheorem{conjecture}{Conjecture}[section]

%\theoremstyle{remark}
%\newtheorem{note}{Note}[section]

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\A}[1]{\lvert #1 \rvert}
\newcommand{\N}[1]{\lVert #1 \rVert}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\B}{\operatorname{B}}

\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}

\newcommand{\PM}{\mathcal{P}}
\newcommand{\Lp}{{\operatorname{Lip}}}

\DeclareMathOperator{\Sp}{supp}

\newcommand{\DKR}{\operatorname{d}_{\textnormal{KR}}}

\newcommand{\Ob}{\mathcal{O}}

\begin{document}

%+Title
\title{Forecasting using incomplete models}
\author{Vadim Kosoy}
\date{}%\date{\today}
\maketitle
%-Title

%+Abstract
\begin{abstract}
We consider the task of forecasting an infinite sequence of future observation based on some number of past observations, where the probability measure generating the observations is \enquote{suspected} to satisfy one or more of a set of \emph{incomplete} models i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the \emph{realizable} setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the \emph{unrealizable} setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the  Kantorovich-Rubinstein metric is weaker than convergence in total variation.
\end{abstract}
%-Abstract

%+Contents
%\tableofcontents
%-Contents

\section{Introduction}

Forecasting future observations based on past observations is one of the fundamental problems in machine learning, and more broadly is one of the fundamental components of rational reasoning in general. This problem received much attention, using different methods (see e.g. \cite{Cesa-Bianchi_2006} or Chapter 21 in \cite{Shalev-Shwartz_2014}). Most of those methods assume fixing a class $\mathcal{H}$ of models or \enquote{hypotheses}, each of which defines a probability measure on sequences of observations (and also conditional probability measures), and produce a forecast $F^\mathcal{H}$ which satisfies at least one of two kinds of guarantees:

\begin{itemize}
\item 
In the \emph{realizable} setting, the guarantee is that if the observations are sampled from some $\mu \in \mathcal{H}$, then $F^\mathcal{H}$ converges to \enquote{ideal} forecast in some sense.
\item
In the \emph{unrealizable} setting, the guarantee is that for \emph{any} sequence of observations, $F^\mathcal{H}$ is asymptotically as good as the forecast produced by any $\mu \in \mathcal{H}$.
\end{itemize}

The realizable setting is often unrealistic, in particular because it requires that the environment under observation is simpler than the observer itself. Indeed, even though we avoid analyzing computational complexity in this work, it should be noted that the computational (e.g. time) complexity of a forecaster is always greater than the complexities of all $\mu \in \mathcal{H}$. On the other hand, the unrealizable setting usually only provides guarantees for short-term forecasts (since otherwise the training data is insufficient). The latter is in contrast to e.g. Bayesian inference where the time to learn the model depends on its prior probability, but once \enquote{learned} (i.e. once $F^\mathcal{H}$ converged to a given total variation distance from the true probability measure), arbitrarily long-term forecasts become reliable.

The spirit of our approach is that the environment might be very complex, but at the same time it might posses some simple features, and it is these features that the forecast must capture. For example, if we consider a sequence of observations $\{o_n \in \{0,1\}\}_{n \in \Nats}$ s.t. $o_{2k+1}=o_{2k}$, then whatever is the behavior of the even observations $o_{2k}$, the property $o_{2k+1}=o_{2k}$ should asymptotically be assigned high probability by the forecast (this idea was discussed in \cite{Hutter_2009} as \enquote{open problem 4j}).

Formally, we introduce the notion of an \emph{incomplete model}, which is a convex set $M$ in the space $\PM(X)$ of probability measures on the space of sequences $X$. Such an incomplete model may be regarded as a hybrid of probabilistic and \emph{Knightian} uncertainty. We then consider a countable\footnote{This work only deals with the \emph{nonparametric} setting in which $\mathcal{H}$ is discrete.} set $\mathcal{H}$ of incomplete models. For any $M \in \mathcal{H}$ and $\mu \in M$, our forecasts will converge to $M$ in an appropriate sense with $\mu$-probability 1. This convergence theorem can be regarded as an analogue for incomplete models of Bayesian merging of opinions (see \cite{Blackwell_1962}), and is our main result. Our setting can be considered to be in between realizable and unrealizable: it is \enquote{partially realizable} since we require the environment to conform to some $M \in \mathcal{H}$, it is \enquote{partially unrealizable} since $\mu \in M$ can be chosen adversarially (we can even allow non-oblivious choice, i.e. dependence on the forecast itself).

The forecasting method we demonstrate is based on the principles introduced in \cite{Garrabrant_2016}. The forecast may be regarded as the pricing of a certain combinatorial prediction market, where each incomplete model $M \in \mathcal{H}$ corresponds to a trader. This trader buys \enquote{stocks} (continuous functions $f$ on $X$) s.t. assuming the environment is sampled from some $\mu \in M$, the expected value $\E_\mu[f]$ of the stock is greater than its current cost $\E_{F}[f]$. The market pricing is then defined by the requirement that the aggregate of all traders doesn't make a net profit (the existence of such a pricing is established using the Kakutani-Glicksberg-Fan fixed point theorem). The fact that the aggregate of all traders cannot make a net profit implies that each individual trader can only make a bounded profit, which implies the main result in turn.

The structure of the paper is as follows. Section~\ref{sec:notation} fixes notation and conventions. Section~\ref{sec:learning} defines the setting and states the main theorem. Section~\ref{sec:markets} lays out the formalism of \enquote{combinatorial prediction markets,} and the central concept of a \emph{dominant market}. Section~\ref{sec:profit} introduces the concept of a \emph{profitable trading metastrategy}, which is a technique for proving convergence theorems about dominant markets. Section~\ref{sec:traders} connects the market formalism with incomplete models and proves the main theorem.

\section{Notation and Preliminaries}
\label{sec:notation}

TBD

\section{Learning Incomplete Models}
\label{sec:learning}

Let $\Sqn{\Ob_n}$ be a sequence of compact Polish spaces. $\Ob_n$ represents the space of possible observations at time $n$. Denote $Y_n := \prod_{m < n} \Ob_n$  and $X:=\prod_{n \in \Nats} \Ob_n$. $\pi_n: X \rightarrow Y_n$ is the projection mapping. $Y_n$ will be regarded as a topological space using the product topology and as a measurable space with the $\sigma$-algebra of \emph{universally measurable} sets. $\PM(X)$ will denote the space of probability measures on $X$, which will be regarded as a topological space using the weak topology and as a measurable space using the $\sigma$-algebra of \emph{Borel} sets. Given $\mu \in \PM(X)$, we denote $\Sp \mu \subseteq X$ the support of $\mu$.

\begin{definition}

A \emph{forecaster} $F$ is a family of measurable mappings

\[\Sqn{F_n: Y_n \rightarrow \PM(X)}\]

s.t. $\Sp {F_n(y)} \subseteq \pi_n^{-1}(y)$.

\end{definition}

Given a forecaster $F$ and $y \in Y_n$, $F_n(y)$ represents the forecast corresponding to observation history $y$. The condition $\Sp {F_n(y)} \subseteq \pi_n^{-1}(y)$ reflects the obvious requirement of consistency with past observations.

In order to formulate a claim about forecast convergence, we will need a metric on $\PM(X)$. Consider $\rho: X \times X \rightarrow \Reals$ a metrization of $X$. Let $\Lp(X;\rho)$ be the Banach space of $\rho$-Lipschitz functions on $X$, equipped with the norm

\begin{equation}
\N{f}_\rho:=\max_{x}{\A{f(x)}} + \sup_{x \ne y} \frac{\A{f(x)-f(y)}}{\rho(x,y)}
\end{equation}

$\PM(X)$ can be regarded as a compact subset of the dual space $\Lp(X;\rho)'$, yielding the following metrization of $\PM(X)$:

\begin{equation}
\DKR^\rho(\mu,\nu):=\sup_{\N{f}_\rho \leq 1}{(\E_\mu[f] - \E_\nu[f])}
\end{equation}

We call $\DKR^\rho$ the \emph{Kantorovich-Rubinstein metric}\footnote{This is slightly different from the conventional definition but strongly equivalent. Other names used in the literature for the strongly equivalent metric are \enquote{1st Wassertein metric} and \enquote{earth mover's distance.}}.

Fix any $x \in X$. It is easy to see that

\[\lim_{n \rightarrow \infty} \max_{x' \in \pi_n^{-1}(x)} \rho(x', x) = 0\]

Denote $\delta_x \in \PM(X)$ the unique probability measure s.t. $\delta_x(\{x\})=1$. It follows that for any sequence $\Sqn{\mu_n \in \PM(X)}$ s.t. $\Sp{\mu_n} \subseteq \pi_n^{-1}(x)$

\[\lim_{n \rightarrow \infty} \DKR^\rho(\mu_n, \delta_x) = 0\]

Therefore, formulating a non-vacuous convergence theorem requires \enquote{renormalizing} $\DKR$ for each $n \in \Nats$. To this end, we consider a \emph{sequence} of metrizations of X: $\Sqn{\rho_n: X \times X \rightarrow \Reals}$. We denote $\DKR^n:=\DKR^{\rho_n}$.

Another ingredient we will need is a notion of regular conditional probability for incomplete models. For this purpose we will assume that for all $n \in \Nats$, $\Ob_n$ is a compact subset of $\Reals^{d_n}$ for some $d_n \in \Nats$, since this will allow us to use the Lebesgue differentiation theorem. In particular, $Y_n$ is a subset of $\Reals^{\sum_{m < n} d_n}$, and we will regard it as a metric space using the Euclidean metric. For any $y \in Y_n$ and $r > 0$, $\B_r(y)$ will denote the open ball of radius $r$ with center at $y$.

\begin{definition}

Consider any $M \subseteq \PM(X)$. Given $y \in Y_n$, we define

\begin{equation}
M''_y:=\{\lim_{r \rightarrow 0}{(\mu \mid \pi_n^{-1}(\B_r(y)))} \mid \mu \in M\}
\end{equation}

Here, $\mu \mid \pi_n^{-1}(\B_r(y))$ is the conditional probability measure (defined whenever $\mu(\pi_n^{-1}(\B_r(y))) > 0$) and the limit need not always exist. That is, $M''_y$ is the set of such limits for those $\mu \in M$ for which the (weak) limit exists (in particular $y \in \Sp{\mu}$).

We further define

\begin{equation}
M'_n:=\{(y,\mu) \in Y_n \times \PM(X) \mid \mu \textnormal{ is in the convex hull of } M''_y\}
\end{equation}

\begin{equation}
M_y:=\{\mu \in \PM(X) \mid (y,\mu) \in \bar{M}'_n\}
\end{equation}

Here, $\bar{M}'_n$ denotes the closure of $M'_n$.

\end{definition}

Note that $M_y$ is always convex and closed.

We are now ready to formulate the main result.

\begin{theorem}

Fix any $\mathcal{H} \subseteq 2^{\PM(X)}$ countable. Then, there exists a forecaster $F^\mathcal{H}$ s.t. for any $M \in \mathcal{H}$, $\mu \in M$ and $\mu$-almost any $x \in X$

\begin{equation}
\lim_{n \rightarrow 0} \DKR^n(F^\mathcal{H}_n(\pi_n(x)), M_{\pi_n(x)}) = 0
\end{equation}

\end{theorem}

This result can also be extended to the non-oblivious setting...

TBD

\section{Dominant Markets}
\label{sec:markets}

Bar

\section{Profitable Trading Metastrategies}
\label{sec:profit}

Bar

\section{Traders for Incomplete Models}
\label{sec:traders}

TBD

\section{Discussion}

TBD

\section*{Acknowledgments}

TBD

\bibliographystyle{unsrt}
\bibliography{Dominant_Markets}

\end{document}


