%&latex
\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}
\usepackage{bm}

%\numberwithin{equation}{section}

\theoremstyle{definition}
\newtheorem{definition}{Definition}%[section]
\newtheorem{example}{Example}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}{Lemma}%[section]
\newtheorem{proposition}{Proposition}%[section]
\newtheorem{corollary}{Corollary}%[section]
%\newtheorem{conjecture}{Conjecture}[section]

%\theoremstyle{remark}
%\newtheorem{note}{Note}[section]

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\A}[1]{\lvert #1 \rvert}
\newcommand{\N}[1]{\lVert #1 \rVert}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\I}{\operatorname{id}}
\newcommand{\D}{\operatorname{diag}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\B}{\operatorname{B}}

\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}

\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\DeclareMathOperator{\Gr}{graph}

\newcommand{\PM}{\mathcal{P}}
\newcommand{\Lp}{{\operatorname{Lip}}}

\DeclareMathOperator{\Sp}{supp}

\newcommand{\DKR}{\operatorname{d}_{\textnormal{KR}}}

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\OO}{\Ob^\omega}
\newcommand{\PO}{\pi^\Ob}
\newcommand{\PMO}{\PM(\OO)}
\newcommand{\MC}{\mathcal{H}}
\newcommand{\Gm}{\mathcal{G}}
\newcommand{\GMO}{\Gm(\OO)}
\newcommand{\CO}{C(\OO)}
\DeclareMathOperator{\V}{V}
\DeclareMathOperator{\SV}{\Sigma V}

\begin{document}

%+Title
\title{Forecasting using incomplete models}
\author{Vadim Kosoy}
\date{}%\date{\today}
\maketitle
%-Title

%+Abstract
\begin{abstract}
We consider the task of forecasting an infinite sequence of future observation based on some number of past observations, where the probability measure generating the observations is \enquote{suspected} to satisfy one or more of a set of \emph{incomplete} models i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the \emph{realizable} setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the \emph{unrealizable} setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the  Kantorovich-Rubinstein metric is weaker than convergence in total variation.
\end{abstract}
%-Abstract

%+Contents
%\tableofcontents
%-Contents

\section{Introduction}

Forecasting future observations based on past observations is one of the fundamental problems in machine learning, and more broadly is one of the fundamental components of rational reasoning in general. This problem received much attention, using different methods (see e.g. \cite{Cesa-Bianchi_2006} or Chapter 21 in \cite{Shalev-Shwartz_2014}). Most of those methods assume fixing a class $\MC$ of models or \enquote{hypotheses}, each of which defines a probability measure on sequences of observations (and also conditional probability measures), and produce a forecast $F^\MC$ which satisfies at least one of two kinds of guarantees:

\begin{itemize}
\item 
In the \emph{realizable} setting, the guarantee is that if the observations are sampled from some $\mu \in \MC$, then $F^\MC$ converges to \enquote{ideal} forecast in some sense.
\item
In the \emph{unrealizable} setting, the guarantee is that for \emph{any} sequence of observations, $F^\MC$ is asymptotically as good as the forecast produced by any $\mu \in \MC$.
\end{itemize}

The realizable setting is often unrealistic, in particular because it requires that the environment under observation is simpler than the observer itself. Indeed, even though we avoid analyzing computational complexity in this work, it should be noted that the computational (e.g. time) complexity of a forecaster is always greater than the complexities of all $\mu \in \MC$. On the other hand, the unrealizable setting usually only provides guarantees for short-term forecasts (since otherwise the training data is insufficient). The latter is in contrast to e.g. Bayesian inference where the time to learn the model depends on its prior probability, but once \enquote{learned} (i.e. once $F^\MC$ converged to a given total variation distance from the true probability measure), arbitrarily long-term forecasts become reliable.

The spirit of our approach is that the environment might be very complex, but at the same time it might posses some simple features, and it is these features that the forecast must capture. For example, if we consider a sequence of observations $\{o_n \in \{0,1\}\}_{n \in \Nats}$ s.t. $o_{2k+1}=o_{2k}$, then whatever is the behavior of the even observations $o_{2k}$, the property $o_{2k+1}=o_{2k}$ should asymptotically be assigned high probability by the forecast (this idea was discussed in \cite{Hutter_2009} as \enquote{open problem 4j}).

Formally, we introduce the notion of an \emph{incomplete model}, which is a convex set $M$ in the space $\PM(X)$ of probability measures on the space of sequences $X$. Such an incomplete model may be regarded as a hybrid of probabilistic and \emph{Knightian} uncertainty. We then consider a countable\footnote{This work only deals with the \emph{nonparametric} setting in which $\MC$ is discrete.} set $\MC$ of incomplete models. For any $M \in \MC$ and $\mu \in M$, our forecasts will converge to $M$ in an appropriate sense with $\mu$-probability 1. This convergence theorem can be regarded as an analogue for incomplete models of Bayesian merging of opinions (see \cite{Blackwell_1962}), and is our main result. Our setting can be considered to be in between realizable and unrealizable: it is \enquote{partially realizable} since we require the environment to conform to some $M \in \MC$, it is \enquote{partially unrealizable} since $\mu \in M$ can be chosen adversarially (we can even allow non-oblivious choice, i.e. dependence on the forecast itself).

The forecasting method we demonstrate is based on the principles introduced in \cite{Garrabrant_2016}. The forecast may be regarded as the pricing of a certain combinatorial prediction market, where each incomplete model $M \in \MC$ corresponds to a trader. This trader buys \enquote{stocks} (continuous functions $f$ on $X$) s.t. assuming the environment is sampled from some $\mu \in M$, the expected value $\E_\mu[f]$ of the stock is greater than its current cost $\E_{F}[f]$. The market pricing is then defined by the requirement that the aggregate of all traders doesn't make a net profit (the existence of such a pricing is established using the Kakutani-Glicksberg-Fan fixed point theorem). The fact that the aggregate of all traders cannot make a net profit implies that each individual trader can only make a bounded profit, which implies the main result in turn.

The structure of the paper is as follows. Section~\ref{sec:learning} defines the setting and states the main theorem. Section~\ref{sec:garrabrant} lays out the formalism of \enquote{combinatorial prediction markets,} and the central concept of a \emph{dominant market}. Section~\ref{sec:prudent} introduces the concept of a \emph{profitable trading metastrategy}, which is a technique for proving convergence theorems about dominant markets. Section~\ref{sec:construction} connects the market formalism with incomplete models and proves the main theorem.

\section{Notation}
\label{sec:notation}

TBD

\section{Learning Incomplete Models}
\label{sec:learning}

Let $\Sqn{\Ob_n}$ be a sequence of compact Polish spaces. $\Ob_n$ represents the space of possible observations at time $n$. Denote $\Ob^n := \prod_{m < n} \Ob_n$  and $\Ob^\omega:=\prod_{n \in \Nats} \Ob_n$. $\PO_n: \Ob^\omega \rightarrow \Ob^n$ is the projection mapping. Given $y \in \Ob^n$, $y\OO := (\PO_n)^{-1}(y)$ is a closed subspace of $\OO$. $\Ob^n$ will be regarded as a topological space using the product topology and as a measurable space with the $\sigma$-algebra of \emph{universally measurable} sets. For any Polish space $X$, $\PM(X)$ will denote the space of probability measures on $X$, which will be regarded as a topological space using the weak topology and as a measurable space using the $\sigma$-algebra of \emph{Borel} sets. Given $\mu \in \PM(X)$, we denote $\Sp \mu \subseteq X$ the support of $\mu$.

\begin{definition}

A \emph{forecaster} $F$ is a family of measurable mappings

\[\Sqn{F_n: \Ob^n \rightarrow \PMO}\]

s.t. $\Sp {F_n(y)} \subseteq (\pi^\Ob_n)^{-1}(y)$.

\end{definition}

Given a forecaster $F$ and $y \in \Ob^n$, $F_n(y)$ represents the forecast corresponding to observation history $y$. The condition $\Sp {F_n(y)} \subseteq y\OO$ reflects the obvious requirement of consistency with past observations.

In order to formulate a claim about forecast convergence, we will need a metric on $\PMO$. Consider $\rho: \OO \times \OO \rightarrow \Reals$ a metrization of $\OO$. Let $\Lp(\OO;\rho)$ be the Banach space of $\rho$-Lipschitz functions on $\OO$, equipped with the norm

\begin{equation}
\N{f}_\rho:=\max_{x}{\A{f(x)}} + \sup_{x \ne y} \frac{\A{f(x)-f(y)}}{\rho(x,y)}
\end{equation}

$\PMO$ can be regarded as a compact subset of the dual space $\Lp(\OO;\rho)'$, yielding the following metrization of $\PMO$:

\begin{equation}
\DKR^\rho(\mu,\nu):=\sup_{\N{f}_\rho \leq 1}{(\E_\mu[f] - \E_\nu[f])}
\end{equation}

We call $\DKR^\rho$ the \emph{Kantorovich-Rubinstein metric}\footnote{This is slightly different from the conventional definition but strongly equivalent. Other names used in the literature for the strongly equivalent metric are \enquote{1st Wassertein metric} and \enquote{earth mover's distance.}}.

Fix any $x \in \OO$. It is easy to see that

\begin{equation}
\lim_{n \rightarrow \infty} \max_{x' \in \PO_n(x)\OO} \rho(x', x) = 0
\end{equation}

Denote $\delta_x \in \PMO$ the unique probability measure s.t. $\delta_x(\{x\})=1$. It follows that for any sequence $\Sqn{\mu_n \in \PMO}$ s.t. $\Sp{\mu_n} \subseteq \pi_n^{-1}(x)$

\begin{equation}
\lim_{n \rightarrow \infty} \DKR^\rho(\mu_n, \delta_x) = 0
\end{equation}

Therefore, formulating a non-vacuous convergence theorem requires \enquote{renormalizing} $\DKR$ for each $n \in \Nats$. To this end, we consider a \emph{sequence} of metrizations of $\OO$: $\Sqn{\rho_n: \OO \times \OO \rightarrow \Reals}$. We denote $\DKR^n:=\DKR^{\rho_n}$.

Another ingredient we will need is a notion of regular conditional probability for incomplete models. For this purpose we will assume that for all $n \in \Nats$, $\Ob_n$ is a compact subset of $\Reals^{d_n}$ for some $d_n \in \Nats$, since this will allow us to use the Lebesgue differentiation theorem. In particular, $\Ob^n$ is a subset of $\Reals^{\sum_{m < n} d_n}$, and we will regard it as a metric space using the Euclidean metric. For any $y \in Y_n$ and $r > 0$, $\B_r(y)$ will denote the open ball of radius $r$ with center at $y$.

\begin{definition}

Consider any $M \subseteq \PMO$. Given $y \in \Ob^n$, we define

\begin{equation}
M''_y:=\{\lim_{r \rightarrow 0}{(\mu \mid \pi_n^{-1}(\B_r(y)))} \mid \mu \in M\}
\end{equation}

Here, $\mu \mid \pi_n^{-1}(\B_r(y))$ is the conditional probability measure (defined whenever $\mu(\pi_n^{-1}(\B_r(y))) > 0$) and the limit need not always exist. That is, $M''_y$ is the set of such limits for those $\mu \in M$ for which the (weak) limit exists (in particular $y \in \Sp{\mu}$).

We further define

\begin{equation}
M'_n:=\{(y,\mu) \in Y_n \times \PMO \mid \mu \textnormal{ is in the convex hull of } M''_y\}
\end{equation}

\begin{equation}
M_y:=\{\mu \in \PMO \mid (y,\mu) \in \bar{M}'_n\}
\end{equation}

Here, $\bar{M}'_n$ denotes the closure of $M'_n$.

\end{definition}

Note that $M_y$ is always convex and closed.

We are now ready to formulate the main result.

\begin{theorem}
\label{thm:main}

Fix any $\MC \subseteq 2^{\PMO}$ countable. Then, there exists a forecaster $F^\MC$ s.t. for any $M \in \MC$, $\mu \in M$ and $\mu$-almost any $x \in \OO$

\begin{equation}
\lim_{n \rightarrow 0} \DKR^n(F^\MC_n(\PO_n(x)), M_{\PO_n(x)}) = 0
\end{equation}

\end{theorem}

\section{Dominant Forecasters}
\label{sec:garrabrant}

In this section we explain our generalization of the methods introduced in \cite{Garrabrant_2016} under the name \enquote{logical inductors.} The main differences between our formalism and that of \cite{Garrabrant_2016} are

\begin{itemize}
\item 
We are interested in sequence forecasting rather than formal logic.
\item
We consider probability measures on certain Polish spaces, rather than probability assignment functions on finite sets.
\item
In particular, no special treatment of expected values is required.
\item
The observations are stochastic rather than deterministic.
\end{itemize}

Our terminology also differs from \cite{Garrabrant_2016}: their \enquote{market} is our \enquote{forecaster}, their \enquote{trader} is our \enquote{gambler}. In any case, our exposition assumes no prior knowledge about logical inductors.

The idea is to consider a collection of gamblers making bets against the forecaster. If a gambler with finite budget cannot make an infinite profit, the gambler is said to be \enquote{dominated} by the forecaster. We then prove that for any countable collection of gamblers, there is a forecaster that dominates all of them.

Given a compact Polish space $X$, $C(X)$ will denote the Banach space of continuous functions with uniform norm. We will also use the shorthand notation $\Gm(X):=C(\PM(X) \times X)$. Equivalently, $\Gm(X)$ can be regarded to be the space of continuous functions from $\PM(X)$ to $C(X)$.

We regard $\GMO$ as the space of bets that can be made against a forecaster. Given $\gamma \in \GMO$, a forecast $\mu \in \PMO$ and observation sequence $x \in \PMO$, the value of the bet $\gamma$ is

\begin{equation}
\V{\gamma}(\nu,x):=\gamma(\nu,x) - \E_{x' \sim \nu}[\gamma(\nu,x')]
\end{equation}

Note that this defines a bounded operator $\V: \Gm(X) \rightarrow \Gm(X)$.

$C(X)$ and $\Gm(X)$ will also be regarded as a measurable spaces, using the $\sigma$-algebra of Borel sets. We remind that the $\sigma$-algebra on $\Ob^n$ is the algebra of \emph{universally measurable} sets.

\begin{definition}

A \emph{gambler} is a family of measurable mappings

\[\Sqn{G_n : \Ob^n \times \PMO^n \rightarrow \Gm(\OO)}\]

\end{definition}

A gambler is considered to observe a forecaster and bet against it. Given $y \in \Ob^n$ and $\bm{\mu} \in \PMO^n$, $G_n(y,\bm{\mu})=\gamma$ means that, if $y$ are the first $n$ observations and $(\bm{\mu},\nu)$ are the first $n + 1$ forecasts (where $\nu$ is the forecast made after observing all of $y$), the gambler will make bet $\gamma$.

We now introduce some notation regarding the interaction of gamblers and forecasters. 

Given a gambler $G$ and a forecaster $F$, we define the measurable mappings $G^F_n: \Ob^n \rightarrow \GMO$ by

\begin{equation}
\label{eqn:GF}
G^F_n(y):=G_n(y,F_0(y),F_1(y) \ldots F_{n-1}(y))
\end{equation}

We allowed some abuse of notation by making implicit the projection from $\Ob^n$ to $\Ob^m$ in the expression $F_m(y)$.

We define the measurable mappings $\overline{\V G}^F_n: \Ob^n \rightarrow \CO$ and $\SV G^F_n: \Ob^{n-1} \rightarrow \CO$ by

\begin{equation}
\overline{\V G}^F_n(y):=(\V G^F_n(y))(F_n(y))
\end{equation}

\begin{equation}
\SV G^F_n(y) := \sum_{m < n} \overline{\V G}^F_m(y)
\end{equation}

In the definition of $\SV G^F_0$, $\Ob^{-1}$ is defined to be equal to $\Ob^0$ (i.e. the one point space).

We are now ready to state the formally state the definition of \enquote{dominance} alluded to in the beginning of the section.

\begin{definition}
\label{def:dominance}

Consider a forecaster $F$ and a gambler $G$. $F$ is said to \emph{dominate} $G$ when for any $x \in \OO$, if condition~\ref{eqn:def_dominance__loss} holds then condition~\ref{eqn:def_dominance__gain} holds:

\begin{equation}
\label{eqn:def_dominance__loss}
\inf_{n \in \Nats} \min_{x' \in \PO_n(x)\OO} {\SV G^F_{n+1}(\PO_n(x),x')} > -\infty
\end{equation}

\begin{equation}
\label{eqn:def_dominance__gain}
\max_{n \in \Nats} \sup_{x' \in \PO_n(x)\OO} {\SV G^F_{n+1}(\PO_n(x),x')} < +\infty
\end{equation}

\end{definition}

The following is the main theorem of this section, which is our analogue of Theorem 3.6.1 from \cite{Garrabrant_2016}.

\begin{theorem}
\label{thm:exist_dominant}

Let $\mathfrak{G}$ be a countable set of gamblers. Then, there exists a forecaster $F^\mathfrak{G}$ s.t. for any $G \in \mathfrak{G}$, $F$ dominates $G$.

\end{theorem}

The rest of the section is devoted to proving Theorem~\ref{thm:exist_dominant}. The proof will consist of two steps. First we show that for any \emph{single} gambler, there is a dominant forecaster. Then, given a countable set $\mathfrak{G}$ of gamblers, we construct a single gambler s.t. dominating this gambler implies dominating all gamblers in the set.

The following lemma shows that for any bet, there is a forecast which makes the bet unwinnable.

\begin{lemma}
\label{lmm:?}

Consider $X$ a compact Polish space and $\gamma \in \Gm(X)$. Then, there exists $\mu \in \PM(X)$ s.t.

\begin{equation}
\Sp \mu \subseteq \Argmax{} \gamma(\mu)
\end{equation}

\end{lemma}

\begin{proof}

?

\end{proof}

TBD

We define the measurable mappings $\bar{G}^F_n: \Ob^n \rightarrow \CO$ by

\begin{equation}
\bar{G}^F_n(y) := G^F_n(y,F_n(y))
\end{equation}

\begin{proposition}
\label{prp:dominate_one}

Let $G$ be a gambler. Then, there exists a forecaster $F$ s.t. for all $n \in \Nats$ and $y \in \Ob_n$

\begin{equation}
\Sp F_n(y) \subseteq \Argmax{y\OO} \bar{G}^F_n
\end{equation}

\end{proposition}

???

TBD

\section{Prudent Gambling Strategies}
\label{sec:prudent}

TBD

\section{Gamblers for Incomplete Models}
\label{sec:construction}

TBD

\section{Discussion}

TBD

\section*{Acknowledgments}

TBD

\bibliographystyle{unsrt}
\bibliography{Dominant_Markets}

\end{document}


