%&latex
\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{cite}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{commath}
\usepackage{bm}

%\numberwithin{equation}{section}

\theoremstyle{definition}
\newtheorem{definition}{Definition}%[section]
\newtheorem{example}{Example}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}{Lemma}%[section]
\newtheorem{proposition}{Proposition}%[section]
\newtheorem{corollary}{Corollary}%[section]
%\newtheorem{conjecture}{Conjecture}[section]

%\theoremstyle{remark}
%\newtheorem{note}{Note}[section]

\newcommand{\Comment}[1]{}

\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Coms}{\mathbb{C}}

\newcommand{\A}[1]{\lvert #1 \rvert}
\newcommand{\N}[1]{\lVert #1 \rVert}

\newcommand{\Argmin}[1]{\underset{#1}{\operatorname{arg\,min}}\,}
\newcommand{\Argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\newcommand{\I}{\operatorname{id}}
\newcommand{\D}{\operatorname{diag}}

\newcommand{\Sq}[2]{\{#1\}_{#2 \in \Nats}}
\newcommand{\Sqn}[1]{\Sq{#1}{n}}

\newcommand{\B}{\operatorname{B}}

\DeclareMathOperator{\E}{E}
\newcommand{\EE}[2]{\operatorname{E}_{\substack{#1 \\ #2}}}
\newcommand{\EEE}[3]{\operatorname{E}_{\substack{#1 \\ #2 \\ #3}}}

\newcommand{\M}{\xrightarrow{\textnormal{k}}}
\DeclareMathOperator{\Gr}{graph}

\newcommand{\PM}{\mathcal{P}}
\newcommand{\Lp}{{\operatorname{Lip}}}

\DeclareMathOperator{\Sp}{supp}

\newcommand{\DKR}{\operatorname{d}_{\textnormal{KR}}}

\newcommand{\R}[1]{\N{#1}_{\text{R}}}

\newcommand{\Ob}{\mathcal{O}}
\newcommand{\OO}{\Ob^\omega}
\newcommand{\PO}{\pi^\Ob}
\newcommand{\PMO}{\PM(\OO)}
\newcommand{\MC}{\mathcal{H}}
\newcommand{\Gm}{\mathcal{B}}
\newcommand{\GMO}{\Gm(\OO)}
\newcommand{\CO}{C(\OO)}
\newcommand{\GC}{\mathfrak{G}}
\DeclareMathOperator{\V}{V}
\DeclareMathOperator{\SV}{\Sigma V}
\DeclareMathOperator{\SVM}{\Sigma V_{\min}}
\DeclareMathOperator{\Ab}{A}
\DeclareMathOperator{\Nr}{N}
\DeclareMathOperator{\Bd}{B}
\newcommand{\BM}{\bm{\mu}}

\begin{document}

%+Title
\title{Forecasting using incomplete models}
\author{Vadim Kosoy}
\date{}%\date{\today}
\maketitle
%-Title

%+Abstract
\begin{abstract}
We consider the task of forecasting an infinite sequence of future observation based on some number of past observations, where the probability measure generating the observations is \enquote{suspected} to satisfy one or more of a set of \emph{incomplete} models i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the \emph{realizable} setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the \emph{unrealizable} setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the  Kantorovich-Rubinstein metric is weaker than convergence in total variation.
\end{abstract}
%-Abstract

%+Contents
%\tableofcontents
%-Contents

\section{Introduction}

Forecasting future observations based on past observations is one of the fundamental problems in machine learning, and more broadly is one of the fundamental components of rational reasoning in general. This problem received much attention, using different methods (see e.g. \cite{Cesa-Bianchi_2006} or Chapter 21 in \cite{Shalev-Shwartz_2014}). Most of those methods assume fixing a class $\MC$ of models or \enquote{hypotheses}, each of which defines a probability measure on sequences of observations (and also conditional probability measures), and produce a forecast $F^\MC$ which satisfies at least one of two kinds of guarantees:

\begin{itemize}
\item 
In the \emph{realizable} setting, the guarantee is that if the observations are sampled from some $\mu \in \MC$, then $F^\MC$ converges to \enquote{ideal} forecast in some sense.
\item
In the \emph{unrealizable} setting, the guarantee is that for \emph{any} sequence of observations, $F^\MC$ is asymptotically as good as the forecast produced by any $\mu \in \MC$.
\end{itemize}

The realizable setting is often unrealistic, in particular because it requires that the environment under observation is simpler than the observer itself. Indeed, even though we avoid analyzing computational complexity in this work, it should be noted that the computational (e.g. time) complexity of a forecaster is always greater than the complexities of all $\mu \in \MC$. On the other hand, the unrealizable setting usually only provides guarantees for short-term forecasts (since otherwise the training data is insufficient). The latter is in contrast to e.g. Bayesian inference where the time to learn the model depends on its prior probability, but once \enquote{learned} (i.e. once $F^\MC$ converged to a given total variation distance from the true probability measure), arbitrarily long-term forecasts become reliable.

The spirit of our approach is that the environment might be very complex, but at the same time it might posses some simple features, and it is these features that the forecast must capture. For example, if we consider a sequence of observations $\{o_n \in \{0,1\}\}_{n \in \Nats}$ s.t. $o_{2k+1}=o_{2k}$, then whatever is the behavior of the even observations $o_{2k}$, the property $o_{2k+1}=o_{2k}$ should asymptotically be assigned high probability by the forecast (this idea was discussed in \cite{Hutter_2009} as \enquote{open problem 4j}).

Formally, we introduce the notion of an \emph{incomplete model}, which is a convex set $M$ in the space $\PM(X)$ of probability measures on the space of sequences $X$. Such an incomplete model may be regarded as a hybrid of probabilistic and \emph{Knightian} uncertainty. We then consider a countable\footnote{This work only deals with the \emph{nonparametric} setting in which $\MC$ is discrete.} set $\MC$ of incomplete models. For any $M \in \MC$ and $\mu \in M$, our forecasts will converge to $M$ in an appropriate sense with $\mu$-probability 1. This convergence theorem can be regarded as an analogue for incomplete models of Bayesian merging of opinions (see \cite{Blackwell_1962}), and is our main result. Our setting can be considered to be in between realizable and unrealizable: it is \enquote{partially realizable} since we require the environment to conform to some $M \in \MC$, it is \enquote{partially unrealizable} since $\mu \in M$ can be chosen adversarially (we can even allow non-oblivious choice, i.e. dependence on the forecast itself).

The forecasting method we demonstrate is based on the principles introduced in \cite{Garrabrant_2016}. The forecast may be regarded as the pricing of a certain combinatorial prediction market, where each incomplete model $M \in \MC$ corresponds to a trader. This trader buys \enquote{stocks} (continuous functions $f$ on $X$) s.t. assuming the environment is sampled from some $\mu \in M$, the expected value $\E_\mu[f]$ of the stock is greater than its current cost $\E_{F}[f]$. The market pricing is then defined by the requirement that the aggregate of all traders doesn't make a net profit (the existence of such a pricing is established using the Kakutani-Glicksberg-Fan fixed point theorem). The fact that the aggregate of all traders cannot make a net profit implies that each individual trader can only make a bounded profit, which implies the main result in turn.

The structure of the paper is as follows. Section~\ref{sec:learning} defines the setting and states the main theorem. Section~\ref{sec:garrabrant} lays out the formalism of \enquote{combinatorial prediction markets,} and the central concept of a \emph{dominant market}. Section~\ref{sec:prudent} introduces the concept of a \emph{profitable trading metastrategy}, which is a technique for proving convergence theorems about dominant markets. Section~\ref{sec:construction} connects the market formalism with incomplete models and proves the main theorem.

\section{Notation}
\label{sec:notation}

TBD

\section{Learning Incomplete Models}
\label{sec:learning}

Let $\Sqn{\Ob_n}$ be a sequence of compact Polish spaces. $\Ob_n$ represents the space of possible observations at time $n$. Denote $\Ob^n := \prod_{m < n} \Ob_n$  and $\Ob^\omega:=\prod_{n \in \Nats} \Ob_n$. $\PO_n: \Ob^\omega \rightarrow \Ob^n$ is the projection mapping and $x_{:n}:=\PO(x)$. Given $y \in \Ob^n$, $y\OO := (\PO_n)^{-1}(y)$ is a closed subspace of $\OO$. $\Ob^n$ will be regarded as a topological space using the product topology and as a measurable space with the $\sigma$-algebra of \emph{universally measurable} sets. For any measurable space $X$, $\PM(X)$ will denote the space of probability measures on $X$. When $X$ is a Polish space with the Borel $\sigma$-algebra, $\PM(X)$ will be regarded as a topological space using the weak topology and as a measurable space using the $\sigma$-algebra of Borel sets. Given $\mu \in \PM(X)$, we denote $\Sp \mu \subseteq X$ the support of $\mu$.

\begin{definition}

A \emph{forecaster} $F$ is a family of measurable mappings

\[\Sqn{F_n: \Ob^n \rightarrow \PMO}\]

s.t. $\Sp {F_n(y)} \subseteq y\OO$.

\end{definition}

Given a forecaster $F$ and $y \in \Ob^n$, $F_n(y)$ represents the forecast corresponding to observation history $y$. The condition $\Sp {F_n(y)} \subseteq y\OO$ reflects the obvious requirement of consistency with past observations.

In order to formulate a claim about forecast convergence, we will need a metric on $\PMO$. Consider $\rho: \OO \times \OO \rightarrow \Reals$ a metrization of $\OO$. Let $\Lp(\OO;\rho)$ be the Banach space of $\rho$-Lipschitz functions on $\OO$, equipped with the norm

\begin{equation}
\N{f}_\rho:=\max_{x}{\A{f(x)}} + \sup_{x \ne y} \frac{\A{f(x)-f(y)}}{\rho(x,y)}
\end{equation}

$\PMO$ can be regarded as a compact subset of the dual space $\Lp(\OO;\rho)'$, yielding the following metrization of $\PMO$:

\begin{equation}
\DKR^\rho(\mu,\nu):=\sup_{\N{f}_\rho \leq 1}{(\E_\mu[f] - \E_\nu[f])}
\end{equation}

We call $\DKR^\rho$ the \emph{Kantorovich-Rubinstein metric}\footnote{This is slightly different from the conventional definition but strongly equivalent. Other names used in the literature for the strongly equivalent metric are \enquote{1st Wassertein metric} and \enquote{earth mover's distance.}}.

Fix any $x \in \OO$. It is easy to see that

\begin{equation}
\lim_{n \rightarrow \infty} \max_{x' \in x_{:n}\OO} \rho(x', x) = 0
\end{equation}

Denote $\delta_x \in \PMO$ the unique probability measure s.t. $\delta_x(\{x\})=1$. It follows that for any sequence $\Sqn{\mu_n \in \PMO}$ s.t. $\Sp{\mu_n} \subseteq x_{:n}\OO$

\begin{equation}
\lim_{n \rightarrow \infty} \DKR^\rho(\mu_n, \delta_x) = 0
\end{equation}

Therefore, formulating a non-vacuous convergence theorem requires \enquote{renormalizing} $\DKR$ for each $n \in \Nats$. To this end, we consider a \emph{sequence} of metrizations of $\OO$: $\Sqn{\rho_n: \OO \times \OO \rightarrow \Reals}$. We denote $\DKR^n:=\DKR^{\rho_n}$.

Another ingredient we will need is a notion of regular conditional probability for incomplete models. For this purpose we will assume that for all $n \in \Nats$, $\Ob_n$ is a compact subset of $\Reals^{d_n}$ for some $d_n \in \Nats$, since this will allow us to use the Lebesgue differentiation theorem. In particular, $\Ob^n$ is a subset of $\Reals^{\sum_{m < n} d_n}$, and we will regard it as a metric space using the Euclidean metric. For any $y \in Y_n$ and $r > 0$, $\B_r(y)$ will denote the open ball of radius $r$ with center at $y$.

\begin{definition}

Consider any $M \subseteq \PMO$. Given $y \in \Ob^n$, we define

\begin{equation}
M_y:=\{\lim_{r \rightarrow 0}{(\mu \mid \pi_n^{-1}(\B_r(y)))} \mid \mu \in M\}
\end{equation}

Here, $\mu \mid \pi_n^{-1}(\B_r(y))$ is the conditional probability measure (defined whenever $\mu(\pi_n^{-1}(\B_r(y))) > 0$) and the limit need not always exist. That is, $M_y$ is the set of such limits for those $\mu \in M$ for which the (weak) limit exists (in particular $y \in \Sp{\mu}$).

We further define

\begin{equation}
M_n:=\{(y,\mu) \in Y_n \times \PMO \mid \mu \textnormal{ is in the convex hull of } M_y\}
\end{equation}

\begin{equation}
M \mid y:=\{\mu \in \PMO \mid (y,\mu) \in \bar{M}_n\}
\end{equation}

Here, $\bar{M}_n$ denotes the closure of $M_n$.

\end{definition}

Note that $M \mid y$ is always convex and closed.

We are now ready to formulate the main result.

\begin{theorem}
\label{thm:main}

Fix any $\MC \subseteq 2^{\PMO}$ countable. Then, there exists a forecaster $F^\MC$ s.t. for any $M \in \MC$, $\mu \in M$ and $\mu$-almost any $x \in \OO$

\begin{equation}
\lim_{n \rightarrow 0} \DKR^n(F^\MC_n(x_{:n}), M \mid x_{:n}) = 0
\end{equation}

\end{theorem}

\section{Dominant Forecasters}
\label{sec:garrabrant}

In this section we explain our generalization of the methods introduced in \cite{Garrabrant_2016} under the name \enquote{logical inductors.} The main differences between our formalism and that of \cite{Garrabrant_2016} are

\begin{itemize}
\item 
We are interested in sequence forecasting rather than formal logic.
\item
We consider probability measures on certain Polish spaces, rather than probability assignment functions on finite sets.
\item
In particular, no special treatment of expected values is required.
\item
The observations are stochastic rather than deterministic.
\end{itemize}

Our terminology also differs from \cite{Garrabrant_2016}: their \enquote{market} is our \enquote{forecaster}, their \enquote{trader} is our \enquote{gambler}. In any case, our exposition assumes no prior knowledge about logical inductors.

The idea is to consider a collection of gamblers making bets against the forecaster. If a gambler with finite budget cannot make an infinite profit, the gambler is said to be \enquote{dominated} by the forecaster. We then prove that for any countable collection of gamblers, there is a forecaster that dominates all of them.

Given a compact Polish space $X$, $C(X)$ will denote the Banach space of continuous functions with uniform norm. We will also use the shorthand notation $\Gm(X):=C(\PM(X) \times X)$. Equivalently, $\Gm(X)$ can be regarded to be the space of continuous functions from $\PM(X)$ to $C(X)$.

We regard $\GMO$ as the space of bets that can be made against a forecaster. Given $\beta \in \GMO$, a forecast $\mu \in \PMO$ and observation sequence $x \in \PMO$, the value of the bet $\beta$ is

\begin{equation}
\V{\beta}(\nu,x):=\beta(\nu,x) - \E_{x' \sim \nu}[\beta(\nu,x')]
\end{equation}

Note that this defines a bounded linear operator $\V: \Gm(X) \rightarrow \Gm(X)$.

$C(X)$ and $\Gm(X)$ will also be regarded as a measurable spaces, using the $\sigma$-algebra of Borel sets. We remind that the $\sigma$-algebra on $\Ob^n$ is the algebra of \emph{universally measurable} sets.

\begin{definition}

A \emph{gambler} is a family of measurable mappings

\[\Sqn{G_n : \Ob^n \times \PMO^n \rightarrow \Gm(\OO)}\]

\end{definition}

A gambler is considered to observe a forecaster and bet against it. Given $y \in \Ob^n$ and $\BM \in \PMO^n$, $G_n(y,\BM)=\beta$ means that, if $y$ are the first $n$ observations and $(\BM,\nu)$ are the first $n + 1$ forecasts (where $\nu$ is the forecast made after observing all of $y$), the gambler will make bet $\beta$.

We now introduce some notation regarding the interaction of gamblers and forecasters. 

Given a gambler $G$ and a forecaster $F$, we define the measurable mappings $G^F_n: \Ob^n \rightarrow \GMO$ by

\begin{equation}
\label{eqn:GF}
G^F_n(y):=G_n(y,F_0,F_1(y_{:1}) \ldots F_{n-1}(y_{:n-1}))
\end{equation}

Here, $y_{:m}$ denotes the projection of $y$ to $\Ob^m$.

We define the measurable mappings $\overline{\V G}^F_n: \Ob^n \rightarrow \CO$ and $\SV G^F_n: \Ob^{n-1} \rightarrow \CO$ by

\begin{equation}
\overline{\V G}^F_n(y):=(\V G^F_n(y))(F_n(y))
\end{equation}

\begin{equation}
\SV G^F_n(y) := \sum_{m < n} \overline{\V G}^F_m(y_{:m})
\end{equation}

In the definition of $\SV G^F_0$, $\Ob^{-1}$ is considered to be equal to $\Ob^0$ (i.e. the one point space).

We are now ready to state the formally state the definition of \enquote{dominance} alluded to in the beginning of the section.

\begin{definition}
\label{def:dominance}

Consider a forecaster $F$ and a gambler $G$. $F$ is said to \emph{dominate} $G$ when for any $x \in \OO$, if condition~\ref{eqn:def_dominance__loss} holds then condition~\ref{eqn:def_dominance__gain} holds:

\begin{equation}
\label{eqn:def_dominance__loss}
\inf_{n \in \Nats} \min_{x' \in x_{:n}\OO} {\SV G^F_{n+1}(x_{:n},x')} > -\infty
\end{equation}

\begin{equation}
\label{eqn:def_dominance__gain}
\sup_{n \in \Nats} \max_{x' \in x_{:n}\OO} {\SV G^F_{n+1}(x_{:n},x')} < +\infty
\end{equation}

\end{definition}

The following is the main theorem of this section, which is our analogue of Theorem 3.6.1 from \cite{Garrabrant_2016}.

\begin{theorem}
\label{thm:exist_dominant}

Let $\Sq{G^k}{k}$ be a family of gamblers. Then, there exists a forecaster $F^G$ s.t. for any $k \in \Nats$, $F$ dominates $G^k$.

\end{theorem}

The rest of the section is devoted to proving Theorem~\ref{thm:exist_dominant}. The proof will consist of two steps. First we show that for any \emph{single} gambler, there is a dominant forecaster. Then, given a countable set $\GC$ of gamblers, we construct a single gambler s.t. dominating this gambler implies dominating all gamblers in the set.

The following lemma shows that for any bet, there is a forecast which makes the bet unwinnable.

\begin{lemma}
\label{lmm:unwinnable}

Consider $X$ a compact Polish space and $\beta \in \Gm(X)$. Then, there exists $\mu \in \PM(X)$ s.t.

\begin{equation}
\Sp \mu \subseteq \Argmax{} \beta(\mu)
\end{equation}

\end{lemma}

\begin{proof}

Define ${K: \PM(X) \rightarrow 2^{\PM(X)}}$ as follows:

\[K(\mu):=\{\nu \in \PM(X) \mid \E_\nu[\beta(\mu)] = \max \beta(\mu)\}\]

For any ${\mu}$, ${K}(\mu)$ is obviously convex and non-empty. It is also easy to see that the graph of $K$ in $\PM(X) \times \PM(X)$ is closed. Applying the Kakutani-Glicksberg-Fan theorem, we conclude that $K$ has a fixed point, i.e. $\mu \in \PM(X)$ s.t. $\mu \in K(\mu)$.
\end{proof}

Next, we show that the probability measure of Lemma~\ref{lmm:unwinnable} can be made to depend measurably on past observations and the bet.

\begin{lemma}
\label{lmm:measurable_unwinnable}

Fix $Y_{1,2}$ compact Polish spaces and denote $X:=Y_1 \times Y_2$. Then, there exists a Borel measurable mapping $\alpha: Y_1 \times \Gm(X) \rightarrow \PM(X)$ s.t. for any $y \in Y_1$ and $\beta \in \Gm(X)$

\begin{equation}
\Sp \alpha(y,\beta) \subseteq \Argmax{y \times Y_2} \beta(\alpha(y,\beta))
\end{equation}

\end{lemma}

\begin{proof}

Define ${Z_1, Z_2, Z \subseteq Y_1 \times \Gm(X) \times \PM(X)}$ by

$${Z_1:=\{(y,\beta,\mu) \in Y_1 \times \Gm(X) \times \PM(X) \mid \Sp \mu \subseteq y \times Y_2\}}$$

$${Z_2:=\{(y,\beta,\mu) \in Y_1 \times \Gm(X) \times \PM(X) \mid \E_\mu[\beta(\mu)] = \max_{y_2 \in Y_2} \beta(\mu,y,y_2)\}}$$

$${Z:=Z_1 \cap Z_2 =\{(y,\beta,\mu) \in Y_1 \times \Gm(X) \times \PM(X) \mid \Sp \mu \subseteq \Argmax{y \times Y_2} \beta(\mu)\}}$$

We can view ${Z}$ as the graph of a \emph{multivalued} mapping from ${Y_1 \times \Gm(X)}$ to ${\PM(X)}$ (i.e a mapping from ${Y_1 \times \Gm(X)}$ to $2^{\PM(X)}$). We will now show that this multivalued mapping has a \emph{selection}, i.e. a single-valued Borel measurable mapping whose graph is a subset of $Z$. Obviously, the selection is the desired ${\alpha}$.

Fix $\rho$ a metrization of $Y_1$. $Z_1$ is the vanishing locus of the continuous function $\E_{(y_1, y_2) \sim \mu}[\rho(y_1,y)]$. $Z_2$ is the vanishing locus of the continuous function $\E_\mu[\beta(\mu)] - \max_{y_2 \in Y_2} \beta(\mu,y,y_2)$. Therefore $Z_{1,2}$ are closed and so is $Z$. In particular, the fiber ${Z_{y\beta}}$ of ${Z}$ over any ${(y,\beta) \in Y_1 \times \Gm(X)}$ is also closed. 

For any ${y \in Y_1}$, ${\beta \in \Gm(X)}$, define ${i_y: Y_2 \rightarrow X}$ by ${i_y(y_2):=(y,y_2)}$ and ${\beta_y \in \Gm(Y_{2})}$ by $\beta_y(\nu,y'):=\beta(i_{y*}\nu,y,y')$. Applying Lemma~\ref{lmm:unwinnable} to ${\beta_y}$ we get ${\nu \in \PM(Y_2)}$ s.t.

$$\Sp \nu \subseteq \Argmax{} \beta_y(\nu)$$

It follows that ${(y,\beta,i_{y*}\nu) \in Z}$ and hence ${Z_{y\beta}}$ is non-empty.

Consider any ${U \subseteq \PM(X)}$ open. Then, ${A_U:=(Y_{1} \times \Gm(X) \times U) \cap Z}$ is locally closed and in particular ${F_\sigma}$. Therefore, the image of ${A_U}$ under the projection to ${Y_{1} \times \Gm(X)}$ is also ${F_\sigma}$ and in particular Borel. 

Applying the Kuratowski-Rill-Nardzewski measurable selection theorem, we get the desired result.
\end{proof}

We define the measurable mappings $\bar{G}^F_n: \Ob^n \rightarrow \CO$ by

\begin{equation}
\bar{G}^F_n(y) := G^F_n(y,F_n(y))
\end{equation}

\begin{corollary}
\label{crl:dominate_one}

Let $G$ be a gambler. Then, there exists a forecaster $F$ s.t. for all $n \in \Nats$ and $y \in \Ob_n$

\begin{equation}
\Sp F_n(y) \subseteq \Argmax{y\OO} \bar{G}^F_n
\end{equation}

\end{corollary}

\begin{proof}

For every $n \in \Nats$, let $\alpha_n: \Ob_n \times \GMO \rightarrow \PMO$ be as in Lemma~\ref{lmm:measurable_unwinnable}. Observing that the definition of $G^F_n$ depends only on $F_m$ for $m < n$, we define $F$ recursively as

\[F_n(y):=\alpha_n(y,G^F_n(y))\]
\end{proof}

In particular, the forecaster $F$ of Corollary~\ref{crl:dominate_one} dominates $G$ since, as easy to see, $\overline{\V G}^F_n \leq 0$ and hence $\SV G^F_n \leq 0$.

We will now introduce a way to transform a gambler in a way that enforces a \enquote{finite spending budget.} Consider a gambler $G$ and fix $b > 0$ (the size of the \enquote{budget}). Define the measurable functions $\SV G_n: \Ob^{n-1} \times \PMO^n  \rightarrow \CO$ by

\begin{equation}
\SV G_n(y,\BM) := \sum_{m < n} (\V G_m(y_{:m},\BM_{:m}))(\BM_m)
\end{equation}

Here, $\BM_{:m}$ denotes the projection of $\BM \in \PMO^n$ to $\PMO^m$ that takes components $l < m$ and $\BM_m$ denotes the $m$-th component of $\BM$. Define the measurable functions $\SVM G_n: \Ob^{n-1} \times \PMO^n  \rightarrow \Reals$ by

\begin{equation}
\SVM G_n(y,\BM) := \min_{y\OO}{\SV G_n(y,\BM)}
\end{equation}

Define the measurable sets

\begin{equation}
\Ab_b G_n:=\{(y,\BM) \in \Ob^{n-1} \times \PMO^n \mid \SVM G_n(y,\BM) > -b\}
\end{equation}

Note that $\Ab_b G_n \times \Ob_n$ is a measurable subset of $\Ob^n \times \PMO^n$ and in particular can be regarded as a measurable space in itself. Define the measurable functions $\Nr_b G_n: \Ab_b G_n \times \Ob_n \rightarrow C(\PMO)$ by

\begin{equation}
\Nr_b G_n(y,\BM;\nu):=\max(1,\max_{y\OO} \frac{-(\V G_n(y,\BM))(\nu)}{\SV G_n(y_{:n-1},\BM)+b})^{-1}
\end{equation}

Finally, define the gambler $\B_b G$ as follows

\begin{equation}
\B_b G_n(y,\BM):=\begin{cases} 0 \text{ if } \exists m < n: (y_{:m},\BM_{:m+1}) \not\in \Ab_b G_{m+1} \\ \Nr_b G_n(y,\BM) \cdot G_n(y, \BM) \text{ otherwise} \end{cases}
\end{equation}

The next proposition shows that as long as $G$ stays \enquote{within budget $b$,} the operator $\B_b$ has no effect. 

\begin{proposition}
\label{prp:b_no_effect}

Consider any gambler $G$, $b > 0$, $n \in \Nats$, $y \in \Ob^{n - 1}$ and $\BM \in \PMO^n$. Assume that for all $m < n$, $(y_{:m},\BM_{:m+1}) \in \Ab_b G_{m+1}$. Then, for all $m < n$

\begin{equation}
\B_b G_m(y_{:m},\BM_{:m};\BM_m)=G_m(y_{:m},\BM_{:m};\BM_m)
\end{equation}

\end{proposition}

\begin{proof}

Consider any $m < n$. We have

$$\SVM G_{m+1}(y_{:m},\BM_{:m+1}) + b > 0$$

$$\forall x \in y_{:m}\OO: \SV G_{m+1}(y_{:m},\BM_{:m+1}; x) + b > 0$$

$$\forall x \in y_{:m}\OO: \SV G_{m}(y_{:m-1},\BM_{:m}; x) + (\V G_{m}(y_{:m},\BM_{:m}))(\BM_{m}, x) + b > 0$$

$$\forall x \in y_{:m}\OO: \SV G_{m}(y_{:m-1},\BM_{:m}; x) + b > -(\V G_{m}(y_{:m},\BM_{:m}))(\BM_{m}, x)$$

Using the assumption again, the left hand side is positive. It follows that

$$\forall x \in y_{:m}\OO: 1 > \frac{-(\V G_{m}(y_{:m},\BM_{:m}))(\BM_{m}, x)}{\SV G_{m}(y_{:m-1},\BM_{:m}; x) + b}$$

$$\Nr_b G_m(y_{:m},\BM_{:m};\BM_m) = 1$$

Since we are in the second case in the definition of ${\B_b G_m(y_{:m},\BM_{:m})}$, we get the desired result.
\end{proof}

Now, we show that $\B_b G$ indeed never \enquote{goes over budget.}

\begin{proposition}
\label{prp:b_stays_in_budget}

Consider any gambler $G$ and $b > 0$. Then, for any $n \in \Nats$, $y \in \Ob^{n-1}$ and $\BM \in \PMO^n$

\begin{equation}
\SVM \B_b G_n(y,\BM) \geq -b
\end{equation} 

\end{proposition}

\begin{proof}

Let ${m_0 \in \Nats}$ be the largest number s.t. ${m_0 \leq n}$ and 

$$\forall m \leq m_0: \SVM G_m(y_{:m-1},\BM_{:m}) > -b$$

For any ${m \leq m_0}$, we have, by Proposition~\ref{prp:b_no_effect}

$$\SVM \B_b G_m(y_{:m-1},\BM_{:m})=\SVM G_m(y_{:m-1},\BM_{:m}) > -b$$

(Note that the sum in the definition of ${\SVM \B_b G_m}$ only involves ${\B_b G_l}$ for ${l < m \leq m_0}$)

For ${m=m_0+1}$ and any $x \in y_{:m_0}\OO$, we have

$$\SV \B_b G_{m_0+1}(y_{:m_0},\BM_{:m_0+1};x) = \SV \B_b G_{m_0}(y_{:m_0-1},\BM_{:m_0};x) + (\V \B_b G_{m_0}(y_{:m_0},\BM_{:m_0}))(\BM_{m_0},x)$$

The first term only involves ${\B_b G_l}$ for ${l < m_0}$, allowing us to apply Proposition~\ref{prp:b_no_effect}. The second term is in the second case of the definition of $\B_b$.

\begin{align*}
\SV \B_b G_{m_0+1}(y_{:m_0},\BM_{:m_0+1};x) = &\SV G_{m_0}(y_{:m_0-1},\BM_{:m_0};x) +\\ &\Nr_b G_{m_0}(y_{:m_0},\BM_{:m_0};\BM_{m_0}) \cdot (\V G_{m_0}(y_{:m_0},\BM_{:m_0}))(\BM_{m_0},x)
\end{align*}

If ${x}$ is s.t. ${(\V G_{m_0}(y_{:m_0},\BM_{:m_0}))(\BM_{m_0},x) \geq 0}$, then

\[\SV \B_b G_{m_0+1}(y_{:m_0},\BM_{:m_0+1};x) \geq \SV G_{m_0}(y_{:m_0-1},\BM_{:m_0};x)\geq \SVM G_{m_0}(y_{:m_0-1},\BM_{:m_0}) > -b\]

If ${x}$ is s.t. ${(\V G_{m_0}(y_{:m_0},\BM_{:m_0}))(\BM_{m_0},x) < 0}$, then, by the definition of $\Nr_b$

\begin{align*}
\SV \B_b G_{m_0+1}(y_{:m_0},\BM_{:m_0+1};x) \geq &\SV G_{m_0}(y_{:m_0-1},\BM_{:m_0};x)+ \\
&\frac{\SV G_{m_0}(y_{:m_0-1},\BM_{:m_0};x) + b}{-(\V G_{m_0}(y_{:m_0},\BM_{:m_0}))(\BM_{m_0},x)} \cdot (\V G_{m_0}(y_{:m_0},\BM_{:m_0}))(\BM_{m_0},x)
\end{align*}

$$\SV \B_b G_{m_0+1}(y_{:m_0},\BM_{:m_0+1};x) \geq -b$$

Finally, consider ${m > m_0 + 1}$.

$$\SV \B_b G_{m}(y_{:m-1},\BM_{:m};x) = \SV \B_b G_{m-1}(y_{:m-2},\BM_{:m-1};x) + (\V \B_b G_{m-1}(y_{:m-1},\BM_{:m-1}))(\BM_{m-1},x)$$

The second term is in the first case of the definition of $\B_b$ and therefore vanishes.

$$\SV \B_b G_{m}(y_{:m-1},\BM_{:m};x) = \SV \B_b G_{m-1}(y_{:m-2},\BM_{:m-1};x)$$

By induction on ${m}$, we conclude:

$$\SVM \B_b G_{m}(y_{:m-1},\BM_{:m}) \geq \SVM \B_b G_{m-1}(y_{:m-2},\BM_{:m-1}) \geq -b$$
\end{proof}

We now show that for any gambler there is \enquote{limited budget} gambler s.t. any forecaster that dominates it also dominates the original gambler.

\begin{proposition}
\label{prp:frugal_gambler}

Let $G$ be a gambler and $\zeta: \Nats \rightarrow (0,1]$ be s.t.

\begin{equation}
b_\zeta := \sum_{b=0}^\infty \zeta(b) b < \infty
\end{equation}

Define the gambler $\B_\zeta G$ by

\begin{equation}
\B_\zeta G := \sum_{b = 1}^\infty \zeta(b) \B_b G
\end{equation}

Then

\begin{equation}
\label{eqn:prp_furgal_gambler__svm_b_zeta}
\SVM \B_\zeta G_n \geq -b_\zeta
\end{equation}

Moreover, if $F$ is a forecaster that dominates $\B_\zeta G$ then it also dominates $G$.

\end{proposition}

\begin{proof}

Equation~\ref{eqn:prp_furgal_gambler__svm_b_zeta} follows from Proposition~\ref{prp:b_stays_in_budget}. Now, consider any $F$ that dominates $\B_\zeta G$. For any $x \in \OO$, \ref{eqn:prp_furgal_gambler__svm_b_zeta} and Definition~\ref{def:dominance} imply

\[\sup_{n \in \Nats} \max_{x' \in x_{:n}\OO} {\SV \B_\zeta G^F_{n+1}(x_{:n},x')} < +\infty\]

Consider $x \in \OO$ s.t. condition~\ref{eqn:def_dominance__loss} holds. Denote

\[b_0: = \max( -\min_{n \in \Nats} \inf_{x' \in x_{:n}\OO} {\SV G^F_{n+1}(x_{:n},x')},0)\]

By Proposition~\ref{prp:b_no_effect}, for any $b > b_0$ we have $\B_bG^F = G^F$. We get

\[\sup_{n \in \Nats} \max_{x' \in x_{:n}\OO} (\sum_{b \leq b_0} \zeta(b) {\SV \B_b G^F_{n+1}(x_{:n},x')} + \sum_{b > b_0} \zeta(b) \SV G^F_{n+1}(x_{:n},x')) < +\infty\]

\[\sup_{n \in \Nats} \max_{x' \in x_{:n}\OO} (-\sum_{b \leq b_0} \zeta(b) b + \sum_{b > b_0} \zeta(b) \SV G^F_{n+1}(x_{:n},x')) < +\infty\]

\[\sup_{n \in \Nats} \max_{x' \in x_{:n}\OO} \SV G^F_{n+1}(x_{:n},x') < +\infty\]
\end{proof}

The last ingredient we need to prove Theorem~\ref{thm:exist_dominant} is the step from one gambler to a countable set of gamblers.

\begin{proposition}
\label{prp:combining_gamblers}

Let $\Sq{G^k}{k}$ be a family of gamblers, $\xi: \Nats \rightarrow (0,1]$ and $b_\xi > 0$ s.t.

\begin{equation}
\label{eqn:prp_combining_gamblers__budget}
\sum_{k = 0}^\infty \xi(k) \min(\SVM G^k_n,0) \geq -b_\xi
\end{equation}

Define the gambler $G^\xi$ by

\begin{equation}
G^\xi_n := \sum_{k = 0}^n \xi(k) G^k_n
\end{equation}

Consider a forecaster $F$ that dominates $G^\xi$. Then, $F$ dominates $G^k$ for all $k \in \Nats$.

\end{proposition}

\begin{proof}

Fix $k \in \Nats$ and $x \in \OO$. Equation~\ref{eqn:prp_combining_gamblers__budget} implies

\[\inf_{n \in \Nats} \min_{x' \in x_{:n}\OO} \SV G^{\xi F}_{n+1}(x_{:n},x') > -\infty\]

Therefore we have

\[\sup_{n \in \Nats} \max_{x' \in x_{:n}\OO} \SV G^{\xi F}_{n+1}(x_{:n},x') < +\infty\]

\[\sup_{n \geq k} \max_{x' \in x_{:n}\OO} (\xi(k) \SV G^{k F}_{n+1}(x_{:n},x') + \sum_{\substack{j \ne k\\ j \leq n}} \xi(j) \SV G^{j F}_{n+1}(x_{:n},x')) < +\infty\]

\[\sup_{n \geq k} \max_{x' \in x_{:n}\OO} (\xi(k) \SV G^{k F}_{n+1}(x_{:n},x') - b_\xi) < +\infty\]

\[\sup_{n \geq k} \max_{x' \in x_{:n}\OO} \SV G^{k F}_{n+1}(x_{:n},x') < +\infty\]

Since there are only finitely many values of $n$ less than $k$, it follows that

\[\sup_{n \in \Nats} \max_{x' \in x_{:n}\OO} \SV G^{k F}_{n+1}(x_{:n},x') < +\infty\]
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:exist_dominant}]

Define $\zeta(b) := \max(b,1)^{-3}$. By Proposition~\ref{prp:frugal_gambler}, we have

\[\SVM \B_{\zeta} G^k_n \geq -\frac{\pi^2}{6}\]

Define $\xi(k):=(k+1)^{-2}$.

We have

\[\sum_{k=0}^\infty \xi(k) \max(\SVM \B_{\zeta} G^k_n, 0) \geq -\frac{\pi^4}{36} \]

By Corollary~\ref{crl:dominate_one}, there is a forecaster $F^G$ that dominates $\B_\zeta G^\xi$. By Proposition~\ref{prp:combining_gamblers}, $F^G$ dominates $\B_\zeta G^k$ for all $k \in \Nats$. By Proposition~\ref{prp:frugal_gambler}, $F^G$ dominates $G^k$ for all $k \in \Nats$.
\end{proof}

\section{Prudent Gambling Strategies}
\label{sec:prudent}

In this section we construct a tool for proving asymptotic convergence results about dominant forecasters. We start by introducing a notion of \enquote{prudent gambling,} which requires that gambles are only made when the expected payoff is a certain fraction of the risk. First, we need some notation.

Given measurable spaces $X$ and $Y$, we will use the notation $f: X \M Y$ to denote a Markov kernel with source $X$ and target $Y$. Given $x \in X$, we will use the notation $f(x) \in \PM(Y)$. Given $\mu \in \PM(X)$, $\mu \ltimes f \in \PM(X \times Y)$ denotes the semidirect product of $\mu$ and $f$. $f_* \mu \in \PM(Y)$ denotes the pushforward of $\mu$ by $f$ (i.e. the pushforward of $\mu \ltimes f$ by the projection to $Y$). When $X,Y$ are Polish and $\pi: X \rightarrow Y$ is (Borel) measurable, $\mu \mid \pi: Y \M X$ is defined to be s.t. $\pi_* \mu \ltimes (\mu \mid \pi)$ is supported on the graph of $\pi$ and $(\mu \mid \pi)_* \pi_* \mu = \mu$ (i.e. $\mu \mid \pi$ is a regular conditional probability). By the disintegration theorem, $\mu \mid \pi$ exists and is defined up to coincidence $\pi_* \mu$-almost everywhere.

We remind that $\PO$ is the projection mapping from $\OO$ to $\Ob^n$.

\begin{definition}

A \emph{gambling strategy} is a uniformly bounded family of measurable mappings

\[\Sqn{S_n: \Ob^n \rightarrow \GMO}\]

\begin{definition}

Given a gambling strategy $S$ and $\mu^* \in \PMO$, $S$ is said to be \emph{$\mu^*$-prudent} when there is $\alpha > 0$ s.t. for any $n \in \Nats$, $\PO_* \mu^*$-almost any $y \in \Ob^n$ and any $\mu \in \PMO$

\begin{equation}
\E_{(\mu^* \mid \PO)(y)}[S_n(y; \mu)] - \E_\mu[S_n(y; \mu)] \geq \alpha (\max_{y\OO}{S_n(y; \mu)} - \min_{y\OO}{S_n(y; \mu)})
\end{equation}

\end{definition}

\end{definition}

TBD

\section{Gamblers for Incomplete Models}
\label{sec:construction}

TBD

\section{Discussion}

TBD

\section*{Acknowledgments}

TBD

\bibliographystyle{unsrt}
\bibliography{Dominant_Markets}

\end{document}


